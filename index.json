[{"content":"I just finished writing the first chapter of Mastering Spring MVC 4. That was a lot of fun. I enjoyed sharing the tips and tricks that I use when I bootstrap a Spring application.\nIn this first chapter, you will learn how to get started with spring MVC in a snap.\nKnow thy tools Have you heard of Spring Tool Suite? Do you know start.spring.io? Did you know you could actually curl start.spring.io?\nYou will learn all that and much more in this action-packed first chapter!\nThe power of Spring Boot Did you know spring boot actually does a lot of things for us?\n Initializing the DispatcherServlet of Spring MVC Setting up an encoding filter, which will enforce correct encoding of clients' requests Setting up a view resolver to tell Spring where to find our views Configuring static resources locations (css, js) Configuring supported locales and resource bundles Configuring a multipart resolver for file uploads to work Including tomcat or jetty to run our application on a web server Setting up error pages (404, etc)  The first chapter walks you through Spring Boot\u0026rsquo;s code to explain how it works and how to customize the default configurations.\nA big thank you to Phillip Webb, co-lead of Spring Boot for helping me and my editor sort out potential licensing issues when quoting Spring Boot\u0026rsquo;s code. If you\u0026rsquo;re interested, the code is under Apache license and allows reproduction under the conditions listed in paragraph 4.\nWell guys, good talking to you but I\u0026rsquo;d better get back to writing chapter two, which is about the MVC architecture and Spring MVC views and navigation.\n","permalink":"https://geowarin.com/book/chapter-1-setting-up-a-spring-web-application-in-no-time.html","summary":"I just finished writing the first chapter of Mastering Spring MVC 4. That was a lot of fun. I enjoyed sharing the tips and tricks that I use when I bootstrap a Spring application.\nIn this first chapter, you will learn how to get started with spring MVC in a snap.\nKnow thy tools Have you heard of Spring Tool Suite? Do you know start.spring.io? Did you know you could actually curl start.","title":"Chapter 1 - Setting up a Spring web application in no time"},{"content":"I was so busy writing the third chapter of Mastering Spring MVC 4, I did not take the time to write about chapter 2.\nIn this outstanding chapter you will learn the principles of the MVC Architecture. We will also be discussing good practices, the basis of Domain Driven Design and the trade-offs this kind of design imply on a Spring MVC architecture.\nA good read on the subject is available on Petri Kainulainen\u0026rsquo;s blog.\nYou will then set up Spring Boot to work with Thyemleaf, the benefits of this good templating engine and how to leverage it effectively in your MVC application.\nIn 5 minutes, I will walk you through the process of registering your application on twitter and we will be designing a little but beautiful tweet search engine using Spring Social Twitter:\nThis will give us the opportunity to discuss the navigation in a web application, java 8 streams system, the Spring Expression Language, material design, web jars and much more!\nAt the end of the chapter you will have laid the foundations of the web application that we will deploy on the cloud later and that will make you rich and famous!\nI hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-2-mastering-the-mvc-architecture.html","summary":"I was so busy writing the third chapter of Mastering Spring MVC 4, I did not take the time to write about chapter 2.\nIn this outstanding chapter you will learn the principles of the MVC Architecture. We will also be discussing good practices, the basis of Domain Driven Design and the trade-offs this kind of design imply on a Spring MVC architecture.\nA good read on the subject is available on Petri Kainulainen\u0026rsquo;s blog.","title":"Chapter 2 - Mastering the MVC Architecture"},{"content":"The third chapter of Mastering Spring MVC 4 is about the cornerstone of web application: forms. Oh boy, what a chapter.\nIn this epic piece of literature, we will create a complete profile form like this one:\nWe will discuss forms, validation, conversion and formatting.\nSee the birth date field up there? That\u0026rsquo;s a java.time.LocalDate. Do you wonder how to make it work with Spring MVC?\nOur application will be available in different languages and it will be easy to switch between them.\nWe will also be looking at a simple way to validate the form on the client side thanks to the available HTML 5 specification.\nThat\u0026rsquo;s not all, you might have noticed that we ask the user to fill out a list of tastes, things that interest him. That\u0026rsquo;s because we will allow searches on multiple keywords with matrix variables.\nSounds good? I hope it does and can\u0026rsquo;t wait for the moment when you will be able to get your hands on the book!\n","permalink":"https://geowarin.com/book/chapter-3-handling-forms-and-complex-url-mapping.html","summary":"The third chapter of Mastering Spring MVC 4 is about the cornerstone of web application: forms. Oh boy, what a chapter.\nIn this epic piece of literature, we will create a complete profile form like this one:\nWe will discuss forms, validation, conversion and formatting.\nSee the birth date field up there? That\u0026rsquo;s a java.time.LocalDate. Do you wonder how to make it work with Spring MVC?\nOur application will be available in different languages and it will be easy to switch between them.","title":"Chapter 3 - Handling forms and complex URL mapping"},{"content":"In the fourth chapter of Mastering Spring MVC 4 We will allow the user to upload his profile picture.\nThe file upload component implies handling errors at two levels: I/O exceptions at the controller level and multipart exception (a file too big, for instance) at the container level.\nIf your curious about this part take a look at this stackoverflow question.\nThis will give us the opportunity to discuss error handling in Spring MVC and Spring boot.\nCan\u0026rsquo;t wait to hear what you think about this chapter!\n","permalink":"https://geowarin.com/book/chapter-4-file-upload-error-handling.html","summary":"In the fourth chapter of Mastering Spring MVC 4 We will allow the user to upload his profile picture.\nThe file upload component implies handling errors at two levels: I/O exceptions at the controller level and multipart exception (a file too big, for instance) at the container level.\nIf your curious about this part take a look at this stackoverflow question.\nThis will give us the opportunity to discuss error handling in Spring MVC and Spring boot.","title":"Chapter 4 - File Upload and Error Handling"},{"content":"In this chapter of Mastering Spring MVC 4, we‚Äôll tackle main principles of a RESTful architecture. We‚Äôll learn how to convert data to JSON and handle different media types in the application.\nWe will use the JSR-310 (Java DateTime) spec from the get-go and see how to handle them properly in your MVC application.\nForever RESTful We will see how REST works in theory: HTTP codes and verbs, API versioning, HATEOAS, etc.\nThen, we will design an API that uses both JSON and XML to do a twitter search and manage users.\nTooling I will also show you some useful tools to debug REST API. My favorite one is a little command line tool called httpie.\nYou will study the best practice like exceptions handling and custom error pages.\nWe will also see how to set up Jackson serialization using mixins and Java 8 dates.\nDocumentation We will use swagger to document our API.\nI hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-5-crafting-a-restful-application.html","summary":"In this chapter of Mastering Spring MVC 4, we‚Äôll tackle main principles of a RESTful architecture. We‚Äôll learn how to convert data to JSON and handle different media types in the application.\nWe will use the JSR-310 (Java DateTime) spec from the get-go and see how to handle them properly in your MVC application.\nForever RESTful We will see how REST works in theory: HTTP codes and verbs, API versioning, HATEOAS, etc.","title":"Chapter 5 - Crafting a RESTful application"},{"content":"In this chapter of Mastering Spring MVC 4, we‚Äôll learn how to secure our web application and also how to cope with the security challenges of modern, distributed web applications.\nThis is a parts chapter:\n First, we will set up basic HTTP authentication in a minute Then, we will design a form-based authentication for the web pages, keeping the basic authentication for the REST API We will allow the users to signup via the Twitter OAuth API Then, we will leverage Spring Session to make sure our application can scale using a distributed session mechanism Last, we will configure Tomcat to use secured connection through SSL  Safe and sound At the end of this chapter you will be a security expert. We will protect our REST API with basic auth, which is secure as well as easy to interact with.\nWe will also design a gorgeous login page for our web application:\nGoing social My favorite part in this chapter is when we use Spring Social to allow users to log-in using their twitter account!\nInfinite scaling We will use Spring Session to put our users' sessions into Redis with little configuration.\nThis will allow us to add more servers to handle high traffic without worrying about sticky sessions.\nSSL In the end, we will see how to use SSL with tomcat.\nI hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-6-securing-your-application.html","summary":"In this chapter of Mastering Spring MVC 4, we‚Äôll learn how to secure our web application and also how to cope with the security challenges of modern, distributed web applications.\nThis is a parts chapter:\n First, we will set up basic HTTP authentication in a minute Then, we will design a form-based authentication for the web pages, keeping the basic authentication for the REST API We will allow the users to signup via the Twitter OAuth API Then, we will leverage Spring Session to make sure our application can scale using a distributed session mechanism Last, we will configure Tomcat to use secured connection through SSL  Safe and sound At the end of this chapter you will be a security expert.","title":"Chapter 6 - Securing your application"},{"content":"In this chapter, we will make sure our application never regresses thanks to a handful of unit tests.\nTo test or not to test? Chapter 5 is by far my favorite chapter of Mastering Spring MVC 4!\nIn this uplifting piece of literature, you will learn more about testing best practices: TDD, the pyramid of tests, unit tests and end-to-end tests.\nWe will see the difference behind mocking and stubbing and use mockito and the power of Spring\u0026rsquo;s IOC to study both options.\nWith spring-mvc-test we will create unit test for our web controllers as well as our REST controllers.\nAcceptance tests Using fluentlenium, we will create simple yet powerful end-to-end tests leveraging Selenium.\nI will tell you what the Page Object pattern is and how to use it with fluentlenium.\nYou will see how to configure Gradle to create a task for our acceptance tests and generate reports for this task.\nMaking it groovy At the end of the chapter I will show you how to make your tests even better and more readable with Spock.\nThen we will use Geb, a wonderful library used by the grails community to design acceptance tests on top of WebDriver.\nI hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-7-unit-and-acceptance-tests.html","summary":"In this chapter, we will make sure our application never regresses thanks to a handful of unit tests.\nTo test or not to test? Chapter 5 is by far my favorite chapter of Mastering Spring MVC 4!\nIn this uplifting piece of literature, you will learn more about testing best practices: TDD, the pyramid of tests, unit tests and end-to-end tests.\nWe will see the difference behind mocking and stubbing and use mockito and the power of Spring\u0026rsquo;s IOC to study both options.","title":"Chapter 7 - Leaving nothing to luck with unit and acceptance tests"},{"content":"In this chapter of Mastering Spring MVC 4, we will implement classic ways of optimizing a web application: cache control headers and Gzipping.\nWe will also use Spring\u0026rsquo;s cache abstraction and ETags.\nMore threads, please You will learn how to create asynchronous services with Spring Async.\nSpring Async is a nice part of Spring, if you want to dig deeper see [this article]({% post_url 2015-06-12-completable-futures-with-spring-async %}) where we use Java 8 CompletableFutures to create a multithreaded application.\nWebsockets To finish, we will enter the reactive stuff and learn how to use websockets with sockjs.\nI hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-8-optimizing-your-requests.html","summary":"In this chapter of Mastering Spring MVC 4, we will implement classic ways of optimizing a web application: cache control headers and Gzipping.\nWe will also use Spring\u0026rsquo;s cache abstraction and ETags.\nMore threads, please You will learn how to create asynchronous services with Spring Async.\nSpring Async is a nice part of Spring, if you want to dig deeper see [this article]({% post_url 2015-06-12-completable-futures-with-spring-async %}) where we use Java 8 CompletableFutures to create a multithreaded application.","title":"Chapter 8 - Optimizing your requests"},{"content":"In this chapter of Mastering Spring MVC 4, you will deploy your application on the cloud and invite the whole world to see!\nWe will see how to deploy our application on two popular PaaS: Cloud Foundry and Heroku.\nA big thank you to Wayne Lund at Pivotal who wrote the Pivotal Web Services part!\nKnow your options We will have a look at the different PaaS providers, then I will guide you through the steps of deploying your application on Cloud Foundry and Heroku.\nWe will use Redis to distribute our sessions as well as our application cache and prepare our application to handle thousands of requests without spending a single penny!\nI\u0026rsquo;m not bluffing I already deployed my application on Heroku!\nIt\u0026rsquo;s a free Heroku instance so you might have to wait 30 seconds for it to go out of sleep.\nAs always, I hope you will find the content of this chapter interesting, let me know your thoughts in the comments!\n","permalink":"https://geowarin.com/book/chapter-9-deploying-to-the-cloud.html","summary":"In this chapter of Mastering Spring MVC 4, you will deploy your application on the cloud and invite the whole world to see!\nWe will see how to deploy our application on two popular PaaS: Cloud Foundry and Heroku.\nA big thank you to Wayne Lund at Pivotal who wrote the Pivotal Web Services part!\nKnow your options We will have a look at the different PaaS providers, then I will guide you through the steps of deploying your application on Cloud Foundry and Heroku.","title":"Chapter 9 - Deploying to the cloud"},{"content":"At my current job, during interviews, we like to ask this seemingly innocent question: \u0026ldquo;What is a good unit test?\u0026rdquo;.\nIt turns out that it is a tricky questions. Let\u0026rsquo;s try to reflect on the practice of testing to make sure that we write tests as efficiently as possible.\nWhat is a Unit? Right from the start, we see that the definition of a unit is problematic.\nSome people will argue that a unit is a function, a class, or even a package. To me, this feels too restrictive.\nFor now, let\u0026rsquo;s define a unit as:\n A non-trivial amount of code that does a non-trivial thing\n We will see if we can refine this definition as we reflect on the practice of unit-testing.\nWhat is good about testing? Let\u0026rsquo;s try to give some properties of \u0026ldquo;good testing\u0026rdquo; practices.\nHere is what comes through my mind:\n Tests catch when the code breaks in unexpected ways They help me design my code while I am writing it They give me courage to change/refactor the code down the line They provide tight feedback loops  They catch errors This is the most obvious advantage of writing tests.\nOff-by-one errors, typos, or simply a misunderstanding of what the current code does might lead to a logical error.\nWe are human, we make mistakes.\nThey help me design my APIs Through experience, I have become a TDD practitioner. I find that I write code more efficiently when I\u0026rsquo;m writing a test first.\nFollowing TDD by the book would mean adhering to the Red / Green / Refactor mantra.\n üî¥ Red : You write a failing test that highlights what your code is supposed to do next üü¢ Green: You write the minimum amount of code to make that test pass üîµ Refactor: If necessary, you refactor your code (production or test)  =\u0026gt; Repeat until you code does what it is supposed to.\nWhile I find this approach helpful, I do not follow it dogmatically.\nWhat I take issue with is \u0026ldquo;minimal code to make tests fail or pass\u0026rdquo;.\nIf I think a portion of code is getting complicated, I would typically extract a function.\nThen, I\u0026rsquo;ll write a test that looks like this:\ntest(\u0026#39;it works\u0026#39;, () =\u0026gt; { const output = myFunction(some, input); expect(output).toEqual( // what\u0026#39;s the API?  ) }) I make it compile by writing an empty function (that probably returns null). I begin to write types for inputs and outputs.\nI start to think about how different inputs change the output.\nAnd then I start to think about edge cases.\nIf I come up with multiple things to keep in mind at the same time, I might write another test right away. Then, I try my best to write code that works from the get-go.\nOf course, by all means, write simpler test cases first. Then write more complicated ones so complexity becomes easier to tackle.\n Don\u0026rsquo;t be dogmatic, be productive.\n They give me courage to change the existing code We all have heard or lived with technical debt.\nCode ages and sometimes it does not age well. We want to be able to refactor it to reflect our current understanding of the domain.\nThis must be done as often as necessary and, therefore, be as painless as possible.\nThey give me immediate feedback Unit tests must be fast to run.\nThis is why some people oppose them to \u0026ldquo;integration tests\u0026rdquo; or \u0026ldquo;end-to-end tests\u0026rdquo;.\nI would argue that you should thrive to make all kinds of tests fast.\nThe difference between those three kinds of tests might not be obvious and again, I think it comes down to the definition of \u0026ldquo;unit\u0026rdquo;.\nLet me propose the following definitions:\n Integration tests execute code that is \u0026ldquo;off unit\u0026rdquo;.\n  End-to-end tests are about user interactions. Through the UI we can interact with our product and assert what the UI shows.\n I would argue that, more importantly than speed, these kinds of tests differ by the maintenance effort they require.\nMost people are familiar with the test pyramid, where the base is wider and composed of unit tests, and the tip is narrower and composed of fewer \u0026ldquo;high maintenance\u0026rdquo; tests.\nFollowing this practice you\u0026rsquo;ll have mostly fast, easy to maintain, tests and a few high-maintenance and potentially slower tests.\nIs there such a thing as too many tests? Yes.\n The objective of all good code is high cohesion and low coupling.\n This basically means maximizing how easy it is to change the code.\nIdeally, we want a minimal amount of code change to break a minimal amount of tests.\nTherefore, we should apply the same \u0026ldquo;clean code\u0026rdquo; principles to the tests as we apply to production code.\nWe should always Refactor/simplify/delete unnecessary tests.\nCan we predict the future? Now that we talked about cohesion and coupling, we might propose a better definition for a \u0026ldquo;unit\u0026rdquo;:\n A Unit is an arbitrary amount of related code that we expect to change as a whole\n All the nuance is in the \u0026ldquo;we expect\u0026rdquo;. With our current knowledge of the domain, we expect some part of the code to be expended in the future.\nWe might define some extensions points or make it easy to add behaviors by adding variables in an array or a configuration file.\nIf your predictions are wrong, you might have over-engineered your code, or on the contrary, failed to see abstractions that would have made your code easier to change.\nI think the latter is definitely better (YAGNI). If a portion of code is hard to change, we can refactor it until it\u0026rsquo;s easy to change and then, make the change.\nWhat to mock/fake/stub? With a better definition of a \u0026ldquo;unit\u0026rdquo;, we may want to explore what should be tested and what should not. And what the real difference between unit and integration tests is.\nThe usual candidates for mocking are:\n Database queries Network in general Code not directly under our responsibility  My rule of thumb is:\n Mock when it is convenient not to call the \u0026ldquo;off-unit\u0026rdquo; code.\n Some mocking tools are fragile (using reflection, code instrumentation). They make it easy to couple your mocks to implementation details and are prone to breaking.\n Remember that mocking couples the testing code to implementation details.\n Favor simplicity. Write your code such as dependencies are hidden behind small interfaces or functions.\nThen, they become simple to stub.\nMocking is a balancing act between:\n Maximizing speed Minimizing coupling of the test code to implementation details Convenience (tooling)  You might find some cases where hitting a real database, for instance, is not that \u0026ldquo;inconvenient\u0026rdquo;.\nIf your unit tests automatically launch a PostgreSQL database in a container in 0.5 seconds, it might be a pretty good tradeoff and reduce the overall amount tests you write as well as improve your confidence in your code.\nSee: Mocks Aren\u0026rsquo;t Stubs\nConclusion Code always has good reasons to change.\nRigid definitions are not helpful because they might make us forget the most important: tests are a tool that should make the code easier write and to change.\nKeep that goal in mind and you\u0026rsquo;ll write better code.\nWhat about you? Do you agree with my analysis? How do you test your code?\n","permalink":"https://geowarin.com/what-is-a-good-unit-test/","summary":"What constitutes good unit test might be a little more involved that you think. Let\u0026rsquo;s reflect on the practice to make sure that we test our applications as efficiently as possible.","title":"What is a good unit test?"},{"content":"I\u0026rsquo;ve begun coding in C# both as a gamedev hobbyist and at my new job.\nI have a java and javascript background, where we have tools to ensure a consistent code style between projects, like prettier.\nI was a bit surprised that most C# project do not seem to adhere to common, explicitly stated rules or, at least, did not seem to enforce any via tooling.\nRider, my favorite editor, does not have a clear-cut convention to apply on all projects. Instead, it tries to automatically detect the current project\u0026rsquo;s code style and adhere to it, which does not help.\nExisting code styles Microsoft Microsoft has some guidelines, but I found them a bit lacking.\nRoslyn has a strict code style. It is enforced via their code formatter.\nThey also have an .editorconfig file.\nRoslyn code style\u0026rsquo;s summary:  4 spaces indentation _camelCase for private fields readonly where applicable use var only when usage is obvious PascalCase for constant use braces for if/else blocks except when they all fit on a single line new lines before braces  These rules are common in most C# projects I\u0026rsquo;ve read.\nGoogle Google has a different code style, which tries to remove ambiguities from the official Microsoft guidelines.\nGoogle code style\u0026rsquo;s summary:  2 spaces indentation _camelCase for every \u0026ldquo;privatish\u0026rdquo; field (private, internal, etc.) PascalCase for everything public I prefix for interfaces use var only when usage is obvious Always use braces, even when optional NO new lines before braces  My personal preferences goes to google\u0026rsquo;s because I like braces for clarity.\nAs a java and javascript developer, new lines before braces trigger me a little üòÄ.\nAnd, perhaps more importantly, naming rules for public vs private stuff are not ambiguous and simple to follow.\nTooling via editor config Both Visual Studio and Rider support coding style via editorconfig:\n Rider Visual Studio  I stumbled upon a great medium article by Jonathan Harrison that gave me a simple base to work on.\nIn summary, it allows you to define styles:\ndotnet_naming_style.pascal_case_style.capitalization = pascal_case  dotnet_naming_style.lower_camel_case_style.required_prefix = _ dotnet_naming_style.lower_camel_case_style.capitalization = camel_case You can then use those styles in rules that have this form \u0026lt;kind\u0026gt;.\u0026lt;name\u0026gt;.\u0026lt;prop\u0026gt;, where the \u0026lt;name\u0026gt; is defined by you:\n# privatish fields and properties: _camelCase # define \u0026#39;private_fields\u0026#39; dotnet_naming_symbols.private_fields.applicable_kinds = field dotnet_naming_symbols.private_fields.applicable_accessibilities = private, protected, internal, protected_internal, private_protected # apply lower_camel_case_style to private fields dotnet_naming_rule.private_rule.symbols = private_fields dotnet_naming_rule.private_rule.style = lower_camel_case_style # Use PascalCase for public fields dotnet_naming_symbols.public_fields.applicable_kinds = field dotnet_naming_symbols.public_fields.applicable_accessibilities = public dotnet_naming_rule.pascal_case_for_public_fields.symbols = public_fields dotnet_naming_rule.pascal_case_for_public_fields.style = pascal_case_style dotnet_naming_rule.pascal_case_for_public_fields.severity = warning See the documentation.\nI created my personal editorconfig, trying to enforce google rules:\nhttps://gist.github.com/geowarin/03a8133c10bc4f103dda3167f7502feb\nIt\u0026rsquo;s probably not 100% correct, but I\u0026rsquo;ll try to update it as I go.\nConclusion Tooling is crucial to have a homogenous code style in your projects.\nI know that most C# devs will be horrified by my personal code style but this is irrelevant. What\u0026rsquo;s important is to have a code style and be consistent.\nI now have an .editorconfig file that I can drop in my projects. It is applied automatically when I format my code.\nThis is enough for my side projects, but there are other tools that might be interesting to look at in the future:\n dotnet format CSharpier  There also seems to be a way to enforce code style in the dotnet build.\nC# aficionados, I\u0026rsquo;m curious to hear your thoughts! Do you have a code style in your projects? Do you have tools to enforce it?\n","permalink":"https://geowarin.com/c-sharp-code-style/","summary":"Use editor config files to enforce your code style automatically!","title":"C# code style"},{"content":"If you are using React and Typescript, there is a lot that your editor can do to help you.\nHere are my favorite refactorings, I\u0026rsquo;m using IntelliJ for the screencast, but most of this will be available in VSCode as well.\nRename If I had to take a single refactoring to a desert island, it would be the \u0026ldquo;rename\u0026rdquo; refactoring.\nWith a typed language, you have no reasons not to use this.\n  Rename (Shift+F6)\n  IntelliJ has smart renames and understand getter/setter paradigms.\nCompared to Ctrl+R or other manual replace actions, your editor will make sure that you correctly select the relevant variables/methods, as well as being a lot faster.\nExtract component This refactoring is a real MVP. This enables a whole workflow for me: when I prototype, I lay down all the HTML until it looks right. Then I can extract subcomponents with a simple keybinding.\n  Extract component (no default keybinding)\n  I have bound this refactoring to Ctlr+Alt+Shift+M because it resembles the extract method refactoring (Ctlr+Alt+M).\nYou should definitely check out the Extract method refactoring, by the way üòÄ.\nExtract variable This will allow you to add meaningful names to your code in a heartbeat.\n  Extract variable (Ctrl+Alt+V)\n  I should have checked the const checkbox here, to have a const variable generated, instead of a let. ü§¶‚Äç\nExtract type The extract variable refactoring also works on types!\n  Extract type (Ctrl+Alt+V)\n  You can then use Alt+Enter to convert the type to an interface if you wish.\nMove I\u0026rsquo;m nearly done with my refactoring here, I just need my menu to be in its own file.\n  Move (F6)\n  And voil√†!\nConclusion Refactoring can dramatically increase your productivity by providing useful keybindings for complex code transformations.\nThey also give you full confidence that the resulting code will be 100% valid.\nFinally, while I showed you examples on a small scale, you can probably imagine how much time will be saved on larger scale refactorings!\nThanks to horsty for encouraging me to write this article. You should check out his blog (in French). He\u0026rsquo;s using the same blog template as I do, he is a man of taste. üòâ\n","permalink":"https://geowarin.com/typescript-refactorings/","summary":"Essential React and Typescript refactorings","title":"Typescript refactorings"},{"content":"Scripts Put this script in your path (e.g.,: ~/bin/blend-export):\n#!/usr/bin/env bash  DIR=$(dirname \u0026#34;$0\u0026#34;) blender \u0026#34;$1\u0026#34; --background -noaudio -P \u0026#34;$DIR/blender/to_gltf.py\u0026#34; This refers to a python script that you can put in ~/bin/blender/to_gltf.py:\nimport bpy import os filepath = os.path.basename(bpy.data.filepath) basepath = os.path.splitext(filepath) output_file_path = basepath[0] + \u0026#34;.gltf\u0026#34; bpy.ops.export_scene.gltf(filepath=output_file_path, export_format=\u0026#34;GLTF_EMBEDDED\u0026#34;) This is straight forward:\n The bash scripts open a blend file in blender, in the background, and directly executes a script The script uses the blender api to convert the current scene to gltf  Learn more here:\n Blender Command Line Arguments Blender API  Autocomplete A neat trick: you can use the fake-bpy-module python module to get auto-completions in your editor.\nOn arch linux, you can find this in the AUR\nThunar Finally, I like to have this action in my context menu. I can even convert a bunch of blend files in on click.\n  To do this, go in the Edit \u0026gt; Configure custom actions menu and add a new entry.\n  Basics\n Name: blend to gltf Command: for file in %F; do /home/geo/bin/blend-export \u0026quot;$file\u0026quot;; done  Appearance condition\n File Pattern: *.blend Appears if selections contains: other files  You can find this in my dotfiles.\n","permalink":"https://geowarin.com/convert-blend-files-to-gltf-using-a-script/","summary":"You can easily find blend files on the internet, wouldn\u0026rsquo;t it be nice to convert them to gltf files with one click?","title":"Convert blend files to gltf using a script"},{"content":"Mime mapping In my distro, there were no mimetypes declared for gltf/glb files.\nSo I added the following mapping in ~/.local/share/mime/packages/gltf.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;mime-info xmlns=\u0026#39;http://www.freedesktop.org/standards/shared-mime-info\u0026#39;\u0026gt; \u0026lt;mime-type type=\u0026#34;model/gltf+json\u0026#34;\u0026gt; \u0026lt;comment\u0026gt;GLTF model\u0026lt;/comment\u0026gt; \u0026lt;icon name=\u0026#34;model-stl\u0026#34;/\u0026gt; \u0026lt;glob pattern=\u0026#34;*.gltf\u0026#34;/\u0026gt; \u0026lt;/mime-type\u0026gt; \u0026lt;mime-type type=\u0026#34;model/gltf.binary\u0026#34;\u0026gt; \u0026lt;comment\u0026gt;GLTF binary model\u0026lt;/comment\u0026gt; \u0026lt;icon name=\u0026#34;model-stl\u0026#34;/\u0026gt; \u0026lt;glob pattern=\u0026#34;*.glb\u0026#34;/\u0026gt; \u0026lt;/mime-type\u0026gt; \u0026lt;/mime-info\u0026gt; Let\u0026rsquo;s update the mime database with update-mime-database ~/.local/share/mime.\nGltf viewer I found a nice program, written in rust, to preview gltf files: gltf-viewer\nLet\u0026rsquo;s add a desktop entry for this in /home/geo/.local/share/applications/gltf_viewer.desktop.\n[Desktop Entry] Name=GLTF Viewer GenericName=3D Model viewer Comment=3D Model viewer Exec=/home/geo/bin/gltf-viewer Terminal=false Type=Application Icon=acreloaded Categories=Graphics;3DGraphics;Viewer; MimeType=model/gltf+json;model/gltf.binary; NoDisplay=false This will associate gltf-viewer with our glb/gltf files. The program should launch when clicking gltf files in the file manager.\nYou might have to update-desktop-database ~/.local/share/applications for this to take effect.\nThumbnails gltf-viewer also has a feature to output a png image given a 3D model as input.\nLet\u0026rsquo;s write a little script to use that. I\u0026rsquo;ll put this in my path in ~/bin/gltf-thumbnailer.\n#!/bin/bash input=$1 output=$2 size=$3 /home/geo/bin/gltf-viewer -s \u0026#34;$output\u0026#34; -w \u0026#34;$size\u0026#34; -h \u0026#34;$size\u0026#34; \u0026#34;$input\u0026#34; Now the final piece of the puzzle: creating a thumbnail entry in /usr/share/thumbnailers/gltf.thumbnailer.\n[Thumbnailer Entry] TryExec=/home/geo/bin/gltf-thumbnailer Exec=/home/geo/bin/gltf-thumbnailer %i %o %s MimeType=model/gltf+json;model/gltf.binary; Here is an example of the parameters you can pass to your program:\n# %u %o %i %s # %u: url (ex: file:///home/geo/Models/gltf/bookB.gltf.glb) # %o: output thumbnail (ex: /tmp/tumbler-X0YBM90.png) # %i: input file (ex: /home/geo/Models/gltf/bookB.gltf.glb) # %s: size (ex: 128) If you mess up, you can always remove the thumbnail cache: rm -rf .cache/thumbnails/\n","permalink":"https://geowarin.com/thumbnails-in-linux/","summary":"Linux has a nice system to display thumbnails. Let\u0026rsquo;s try it to display GLTF files in thunar.","title":"Thumbnails in Linux"},{"content":"Assumptions:\n Your backend serves the API (REST, graphQL) You build your javascript with a separate bundler (parcel, webpack) Your frontend uses a push state (HTML 5 history) router You want hot module reloading (HMR) for the best developer experience  TLDR; The github repository shows four different solutions.\n1. CORS The most obvious solution is to set up the backend to allow Cross-Origin Resource Sharing (CORS) with the frontend, which runs on the development web server.\n@Bean @ConditionalOnProperty(name = [\u0026#34;com.geowarin.cors.allowedOrigin\u0026#34;]) fun corsFilter(corsProperties: CorsProperties): CorsWebFilter { val source = UrlBasedCorsConfigurationSource().apply { registerCorsConfiguration(\u0026#34;/api/**\u0026#34;, CorsConfiguration().apply { addAllowedOrigin(corsProperties.allowedOrigin!!) addAllowedHeader(\u0026#34;*\u0026#34;) addAllowedMethod(\u0026#34;*\u0026#34;) }) } return CorsWebFilter(source) } We run the frontend with its included web server on localhost:1234. The spring backend runs on localhost:8080.\nNavigating on localhost:1234, you will see that the frontend is able to call web services because the backend allows CORS from that origin.\nPros:\n Close to a production environment Simple enough  Cons:\n CORS?  2. Embed javascript into the backend Another solution is to embed the frontend resources directly inside the spring server, as static resources. The advantage is that we have only one web server and everything is on the same host.\nval acceptsHtmlOnly: RequestPredicate = RequestPredicate { request -\u0026gt; request.headers().accept().contains(MediaType.TEXT_HTML) \u0026amp;\u0026amp; !request.headers().accept().contains(MediaType.ALL) } @Configuration class RouterConfig { @Bean fun indexRoutes(props: EmbeddedProperties) = router { (GET(\u0026#34;*\u0026#34;) and acceptsHtmlOnly) { val indexHtml = DefaultResourceLoader().getResource(props.frontendDirectory) val indexHtml = frontendDirectory.createRelative(\u0026#34;index.html\u0026#34;) ServerResponse.ok().contentType(MediaType.TEXT_HTML).bodyValue(indexHtml) } } } @Configuration @EnableWebFlux class WebConfig(val props: EmbeddedProperties) : WebFluxConfigurer { override fun addResourceHandlers(registry: ResourceHandlerRegistry) { registry.addResourceHandler(\u0026#34;/**\u0026#34;) .addResourceLocations(props.frontendDirectory) .setCacheControl(props.cacheControl) } } Navigating on locahost:8080 you will see that the react application is able to call web services because they both run on the same host.\nThis configuration might have some occasional problems with HMR not fully reloaded so it\u0026rsquo;s not my favourite.\nHowever, by generating the result of the frontend build in src/main/resources/static we both have a very simple way to distribute the full web application, as well as a dev environment that is very similar to the production environment.\nPros:\n Simple to distribute No CORS  Cons:\n A bit of code to handle frontend routing on the backend Clunky hot reloading We cannot scale the backend and the frontend independently  3. Javascript proxy You might do the opposite, run an express web server which includes your bundler and proxies requests to localhost:8080.\nconst Bundler = require(\u0026#39;parcel\u0026#39;); const express = require(\u0026#39;express\u0026#39;); const proxy = require(\u0026#39;http-proxy-middleware\u0026#39;); const history = require(\u0026#39;connect-history-api-fallback\u0026#39;); const bundler = new Bundler(\u0026#39;src/index.html\u0026#39;); const app = express(); app.use(history()); app.use(proxy(\u0026#39;/api\u0026#39;, {target: \u0026#39;http://localhost:8080\u0026#39;, changeOrigin: true})); app.use(bundler.middleware()); app.listen(3000, \u0026#39;localhost\u0026#39;, (err) =\u0026gt; { if (err) { console.log(err); return; } console.log(\u0026#39;Listening at http://localhost:3000\u0026#39;); }); So going to localhost:3000 we can see that the frontend is able to make web requests as if it is running on the same host as the backend.\nPros:\n No CORS  Cons:\n Not a production solution (needs to be complemented with another solution)  4. Reverse proxy We can run a third web server that routes both to our frontend and backend. This is simple enough thanks to docker-compose.\nversion: \u0026#34;3\u0026#34; services: nginx: image: nginx:latest container_name: brginx volumes: - ./server.conf:/etc/nginx/conf.d/default.conf - ../frontend/dist:/usr/share/nginx/html ports: - 8081:8081 Here is the nginx configuration:\nserver { listen 8081; server_name localhost; location /api { proxy_pass http://host.docker.internal:8080; } location / { root /usr/share/nginx/html; set $fallback_file /index.html; if ($http_accept !~ text/html) { set $fallback_file /null; } try_files $uri $fallback_file; } } So navigating to the nginx server on localhost:8081, we can see that the backend and the frontend appear to be on the same host.\nPros:\n Close to a production environment Flexible Can scale with a load balancer  Cons:\n 3 processes  Conclusion Depending on how you wish to deploy your application, you might choose one of the approaches above or even mix them to reach developer nirvana.\nWhat about you? How do you develop your full stack application?\nI\u0026rsquo;d love to have your input!\nSources:\n Github repository  ","permalink":"https://geowarin.com/run-your-frontend-alongside-spring-boot/","summary":"Running a javascript application alongside your spring boot backend can be bit of a conundrum. Here are several ways to tackle this problem.","title":"Run your frontend alongside spring boot"},{"content":"This is a short note on how to share your intelliJ run configurations with git.\nFirst your .gitignore must whitelist the .idea/runConfigurations folder but not the rest of the .idea folder.\nYou probably don\u0026rsquo;t want to commit the entire folder because it can contain personal settings and plugin configurations.\n!.idea .idea/* !.idea/runConfigurations Here is a compound run configuration. It\u0026rsquo;s awesome. It launches multiple run configurations at once.\nThen you want to check the Share throught VCS checkbox on the top right corner, this will add an xml file to the .idea/runConfigurations file that you can commit and push.\n","permalink":"https://geowarin.com/share-intellij-run-configurations-with-git/","summary":"Isn\u0026rsquo;t it great when you checkout a project and have everything at hand to run it?","title":"Share IntelliJ run configurations with git"},{"content":"The problem Ackee is a neat self-hosted analytics solution for simple needs (e.g. a blog). On the repo, the authors give instructions to run the tool, a node application, via docker-compose.\nOn the other hand, the website whose analytics will be tracked needs to include a script that will look like this:\n\u0026lt;script async src=\u0026#34;https://unpkg.com/ackee-tracker@3.2.2/dist/ackee-tracker.min.js\u0026#34; data-ackee-server=\u0026#34;https://ackee.com\u0026#34; data-ackee-domain-id=\u0026#34;67bfa855-7569-4d29-a0a3-a2f4ceae2ea3\u0026#34; data-ackee-opts=\u0026#39;{ \u0026#34;ignoreLocalhost\u0026#34;: false }\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; The problem is that localhost:1313, where my blog runs locally, cannot send json requests to https:localhost or whichever host Ackee is running on, if they differ in name/port, etc.\nThis is a browser built-in protection known as CORS.\nTo allow the browser to make a certain type of requests (like json request) to another host, this host must explicitly allow it by responding with the following headers to a preflight request (with the OPTION http verb):\nAccess-Control-Allow-Origin \u0026quot;*\u0026quot; Access-Control-Allow-Methods \u0026quot;GET, POST, PATCH, OPTIONS\u0026quot; Access-Control-Allow-Headers \u0026quot;Content-Type\u0026quot; The example above allows whichever host (*) to send GET, POST, PATCH, and OPTIONS requests with the Content-Type header. More info can be found in the fetch specification\nThis is a good use-case for a reverse-proxy.\nWe\u0026rsquo;ll use nginx and a self-signed certificate for SSL.\nPlease note:\n You should avoid using wild-card in CORS headers in production SSL is not required to allow CORS You should use a properly signed certificate in production  However, I thought it would be a good occasion to learn something new. And I really wanted Ackee to work locally.\nThe solution Credits to Nickolas Kraus who wrote a very good article on how to run nginx with self-signed certificates.\nI just added docker-compose and CORS headers into the mix.\nGenerating the cerficates Here is a shell script that will create self-signed.crt, self-signed.key and dhparam.pem, your self-signed certificates. It will also add it to the macOS trust store.\nThis will work immediately with Chrome. Firefox has its own store and you will have to manually add the certificate upon the first connection.\nFor other platforms, see here.\n#!/usr/bin/env bash  mkdir -p nginx rm -f nginx/dhparam.pem nginx/self-signed.crt nginx/self-signed.key # create a ssl certificate sudo openssl req \\  -x509 -nodes -days 365 -newkey rsa:2048 \\  -subj \u0026#34;/CN=localhost\u0026#34; \\  -config nginx/openssl.cnf \\  -keyout nginx/self-signed.key \\  -out nginx/self-signed.crt # create a Diffie-Hellman key pair sudo openssl dhparam -out nginx/dhparam.pem 128 # add certificate to the trusted root store sudo security add-trusted-cert \\  -d -r trustRoot \\  -k /Library/Keychains/System.keychain nginx/self-signed.crt # to remove # sudo security delete-certificate -c \u0026#34;\u0026lt;name of existing certificate\u0026gt;\u0026#34; Nginx conf nginx.conf\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; proxy_http_version 1.1; # configure nginx server to redirect to HTTPS server { listen 80; server_name localhost; return 302 https://$server_name:443; } # configure nginx server with ssl server { listen 443 ssl http2; server_name localhost; include self-signed.conf; include ssl-params.conf; # route requests to the local development server location / { add_header Access-Control-Allow-Origin \u0026quot;*\u0026quot; always; add_header Access-Control-Allow-Methods \u0026quot;GET, POST, PATCH, OPTIONS\u0026quot; always; add_header Access-Control-Allow-Headers \u0026quot;Content-Type\u0026quot; always; add_header Strict-Transport-Security \u0026quot;max-age=31536000\u0026quot; always; add_header X-Frame-Options deny; proxy_pass http://ackee:3000/; } } include servers/*; } Note the proxy_pass line that will reference the site we reverse-proxy. With docker-compose the hostname will be the name of the container listed in docker-compose.yml.\nWe know that the node server runs on port 3000.\nYou can also see that nginx will automatically add the CORS headers we previously discussed.\nYou can see two includes:\nssl-params.conf\nssl_protocols TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers \u0026quot;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH\u0026quot;; ssl_ecdh_curve secp384r1; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 valid=300s; resolver_timeout 5s; add_header Strict-Transport-Security \u0026quot;max-age=63072000; includeSubdomains\u0026quot;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; ssl_dhparam dhparam.pem; self-signed.conf\nssl_certificate self-signed.crt; ssl_certificate_key self-signed.key; docker-compose docker-compose.yml\nversion: \u0026#34;3\u0026#34; services: nginx: image: nginx:latest container_name: pnginx volumes: - ./nginx/:/etc/nginx/ ports: - 80:80 - 443:443 depends_on: - ackee ackee: image: electerious/ackee container_name: ackee restart: always environment: - WAIT_HOSTS=mongo:27017 - ACKEE_MONGODB=mongodb://mongo:27017/ackee env_file: - .env depends_on: - mongo mongo: image: mongo container_name: mongo restart: always volumes: - ./data:/data/db Note the depends_on line which will make the Ackee container available inside the docker network on http://ackee.\nWith that, we have a node application running behind nginx with HTTPS and CORS enabled!\nResources:\n Github repository Original article Why should I use a Reverse Proxy if Node.js is Production-Ready? How-to: Adding trusted root certificates  ","permalink":"https://geowarin.com/reverse-proxy-on-docker-compose-with-cors-and-ssl/","summary":"How to set up a reverse-proxy with CORS and SSL, with docker-compose and a self-signed certificate.","title":"Reverse proxy on docker-compose with CORS and SSL"},{"content":"By default, when you try to debug project reactor calls, you get something like this:\nBut if you add this dependency :\ntestImplementation(\u0026#34;io.projectreactor:reactor-tools\u0026#34;) And click on this little funnel:\nAlso ensure that you have the reactor plugin and that its properly configured.\nYou get this:\nMuch better!\nSources:\n Intellij blog  ","permalink":"https://geowarin.com/debugging-webflux-with-intellij/","summary":"By default, it very difficult to read reactor stack traces. Make your life easier with this little tip!","title":"Debugging Webflux with IntelliJ"},{"content":"Being a CTO in a startup is much more than the technical side.\nHowever, the technical decisions you make early with software, especially in a timed-constraint environment like a startup, tend to stick with you for the rest of the project.\nIn this article, I will be listing a few of the technical decisions I made.\nSome of them felt right at all time, some of them I hope not to make again.\nThe stack Our project was a SaaS management application. We wanted our users to have a great experience, to distinguish ourselves from the competition, and improve their productivity.\nWe used Kotlin and Spring Boot on the backend, React and Typescript on the frontend.\nI\u0026rsquo;m really comfortable with those technologies and that\u0026rsquo;s why I chose them.\nEven if you\u0026rsquo;re only using parts of our stack, I believe that some of the things I learned will be useful to you.\nIt felt right (would do again in my next project) Kotlin Kotlin is a fantastic language. It takes inspirations from scala, groovy, ruby, keeps the best parts, and has a top notch IDE support.\nThere were a few rough edges at the start of the project (IDE performance regressions, erratic behavior on language updates) but for the past few months I can say without reserve that Kotlin has been nothing but a joy to work with.\nKotlin is Java, had it been designed in the past few years instead of having 20 years of legacy. Kotlin is Scala, had it been designed to be a productivity powerhouse instead of an academic language.\nIt is pragmatic, elegant, and with very few footguns. It would be really painful for me to start a new project with Java instead of Kotlin.\nPostgres Postgres is an awesome database. Don\u0026rsquo;t listen to the bells and whistle of NoSQL. I\u0026rsquo;m completely sure that 90% of the web applications built in 2018 will not require advanced partitioning and clustering and will fit nicely in a relational database.\nIf you\u0026rsquo;re building a social network, it\u0026rsquo;s different of course, but otherwise, just take the best relational database. Your team will probably be at-ease with SQL and you will have the peace of mind of a database schema.\nJOOQ and Flyway JooQ is a java library which allows you to write type-safe SQL queries in Java.\nFlyway is a very simple tool that handles your database migrations using simple SQL files (there is a bit more to it, but the concepts are very easy to grasp).\nBoth are very well designed and it felt liberating to be in control of the SQL of the application. I came to realize how powerful SQL really is. All of a sudden it was a world with window functions, views, schemas, users, and more, that was open to us.\nI feel like I was never really in control of my schema when using JPA or other ORM tools in the past.\nTypescript and React For most of my career, I\u0026rsquo;ve worked with type-safe languages. I am 100% certain that they make me more productive. I can refactor my code and I have tremendous tools that empower me.\nOn the other hand, I have worked on a JS project without types for 2 years. It was a successful project with a lot of javascript and a great UX, but it was hard to crank up new features or change old code.\nI will never start a new project without typescript or flow. My heart tells me typescript is the right tool for me, but choosing any of the type-safe languages that compile to JS will make you more productive.\nThe only downside with typescript is finding type definitions for some libraries. There are not always of the best quality and you might have to copy, paste, and modify some locally to get the job done.\nI tend to have a bias towards libraries written in typescript: they come with type definitions and often have a better design.\nReact is a good library. I feel at ease with its API surface and I found that teaching React to newer developers is never too much of a burden.\nUsing a PaaS Don\u0026rsquo;t waste time setting up docker containers and a Kubernetes cluster. You just started your project, it does not have to handle billions of requests. You just need to publish your project in one command without headaches.\nWe used Pivotal Cloud Foundry and we would type cf push theApplication.jar and be done with our deployment.\nNo micro-services A good old monolith is all you want for the exact same reasons as the above.\nAlways strive for simplicity. You can create services later on when your startup is widely successful.\nMartin Fowler wrote a wonderful article called Monolith First that I encourage you to read.\nNot using webpack I have a lot of respect for the folks maintaining webpack. It is a good tool with unprecedented possibilities. That being said, when I set up the project, and despite my two years of webpack experience, I always felt like I was struggling with legacy software, and weird edge cases. I had to use or write custom plugins just to overcome the shortcomings of the tool.\nWebpack is getting better day after day but, for the sake of the project, I took a look at the competition.\nFor my JS build, I don\u0026rsquo;t want fancy configuration and a ton of plugins. I want something like spring-boot, with good defaults, and I want it to be fast out-of-the-box.\nWe used fuse-box, a very good bundler with an efficient cache. It is written in typescript and readily supports this language. Two decisive reasons for me.\nI never regretted trusting the fuse-box team, they\u0026rsquo;re doing a awesome job and they really listen to their community.\nThe other tool I am following closely is parcel. It auto-detects the features you need and provide an all-around pleasurable developer experience with no configuration. And it\u0026rsquo;s faster than Webpack.\nParcel is still in its infancy and I expect a few rough edges in the next months but I would probably give it a shot for my next project.\nIt felt weird (would probably not do again) Mobx and mobx-state-tree I love mobx. That\u0026rsquo;s why I\u0026rsquo;m a reluctant to list it in this category.\nIt feels simple and powerful and it is written in typescript.\nThose were compelling reasons for the choice of this library when I started the project.\nI have been working with Redux intensively on a past project and I found it required a lot of design and tools (boilerplate) to get the simplest features working.\nOn the other side of the spectrum, we have Mobx. You feel really strong when you design your first stores, because it just works.\nOn the other hand, edge cases are rough. Some libraries like react-table would just not behave.\nAfter using it for a year, I can probably list a few rules of thumbs:\n Create wrappers using \u0026lt;Observer /\u0026gt; for libraries using shouldComponentUpdate aggressively, because they will mess up with the expectations of the developers Come up with strategies for serialization and deserialization early in the project with libraries like seriliazr.  But all of this has a cost in terms of code and mobx has a somewhat hidden learning curve that makes it difficult to grasp for junior developers.\nWe also tried mobx-state-tree and I love the ideas behind the library. It comes at a cost, though and this cost, at least for now, is performance.\nAll in all, choosing a state library for React is hard.\nThings to watch for in this area are immer and unstated.\nI still do not have the definitive answer to the question of state management in a React application. Remember there are no silver bullets and be careful when you design your frontend architecture early on.\nREST I have an idea of what a good rest API looks like.\nI think it involves a lot of design and bikeshedding.\nWhen a developer is in charge of a new feature, they always have a lot of choices to make:\n Should I add attributes to an existing REST resource? (overfetching) Should I add a new REST resource? (duplication) Should aggregate resources on the backend or the frontend? (inconsistency)  And I did not even talk about HATEOAS or documentation.\nCreating a good REST API is definitely something you should strive for and take the time to get right, if your business model requires it.\nOtherwise, I would consider GraphQL very seriously.\nIn our case, our model looked like a tree and not like small separated entities. That\u0026rsquo;s also something to consider.\nI feel that thinking your API in terms of a cluster of objects comes more naturally to developers. It favors emergent design and it encourages your developers and your business to get together and figure out the aggregates in your model.\nIf you want to go down this road early on and not even bother writing a fully-fledged backend server, I would review postgraphile and graphcool as great starting points.\nNot using \u0026ldquo;strict: true\u0026rdquo; with typescript Typescript is awesome, but you have to enable strict null checks to make the most of it. We started the project without strict checks and it was a significant endeavor to change it, so we never had the time to do it.\nEvery time we got an \u0026ldquo;X is undefined\u0026rdquo; error in the frontend, I regretted not adding strict: true to the typescript configuration at the start of the project.\nIt felt wrong (would never do again) Using an in memory database for tests and development We used Postgres in production and H2 (an in-memory database) for development and tests.\nWe had too many errors that we could only see after deploying the product to production.\nFortunately, most of them were easy to fix. The errors we saw the most were differences in ordering and grouping between the two DBMS.\nHence the rule: \u0026ldquo;every SQL query shall have an ORDER BY clause\u0026rdquo;.\n As Lukas Eder pointed out in the comments: If you don\u0026rsquo;t need ordering, you should always avoid it, as ordering mostly incurs an O(N log N) operations (apart from those rather rare cases where you can pull the data directly from an index).\n You can probably overcome those inconsistencies by setting up a CI build where your tests run against Postgres.\nBut more importantly, we were not able to take full advantage of features like window functions or JSON data types, to name a few.\nThe next time I\u0026rsquo;m starting a project I will use the same DBMS in development and in production.\nI feel that having a little docker-compose.yml at the root of your project, loose a little time (1 second) at the start of the day to boot it, and having a slightly worst developer experience is well worth the investment.\nServer-Side Rendering When building a Single Page App, you depend on your javascript to create the HTML that the user will interact with.\nWith Server-Side Rendering (SSR), you ask the server to run the javascript for the page that was requested, and then, send the resulting HTML directly to your clients.\nIt is believed that your users will have a better experience with this technique, especially on lower-end devices that will struggle to process big javascript files.\nIt is also believed that this wields better SEO results, as it easier for crawlers to parse HTML than to execute javascript.\nAt the beginning of the project, I was sure I could take advantage of SSR. I had set up a few projects in JS that leveraged SSR in the past, and studied libraries like nextjs carefully.\nOn the JVM, it is a bit less common, but I managed to pull something off using J2V8.\nThe truth is SSR is a trade-off and I think most web applications don\u0026rsquo;t need to invest time in server-rendered javascript.\nBesides, measuring the benefits of SSR is really tricky. You have to consider different metrics than the \u0026ldquo;time to render\u0026rdquo;. Take, for example, time to interactive.\nIt is a fascinating subject but it was foolish to spend time on this matter as SEO and slow processors were clearly not a priority for the business.\nRemoving Server-Side Rendering was a good call, and reduced the overall complexity of the server code.\nThat being said, there is room for a tool that would simplify SSR on the JVM. It would be a amazing side project if you\u0026rsquo;re interested in the challenge.\nService layer Not spending enough time on the simplest aspect of the architecture was something I came to regret a few months into the project with multiple people working on the code.\nMake sure that every layer has clear boundaries and do not hesitate to split your project in small modules early on.\nFor example, these modules can be a good starting point:\n model: mapping with your database and helpers services: fetching and updating your database, only exports higher-level functions like Graphql endpoints web-backend: things that depend on HTTP libraries web-frontend: JS stuff  Modules are a great way to enforce architectural decisions. Moreover, you can only use the internal keyword in Kotlin by splitting your code into modules.\nLater in the project, when you figure out cluster of domain objects that work together well, you should also split the service layer into smaller modules.\nGood examples might be the \u0026ldquo;order module\u0026rdquo;, the \u0026ldquo;transaction module\u0026rdquo;, or the \u0026ldquo;security module\u0026rdquo; depending on your domain.\nI like to thing of this approach as a stepping stone towards \u0026ldquo;micro-services\u0026rdquo;, without the complexity of deploying them as separate network entities.\nThere is actually a continuum between an integrated system and a distributed system and you probably won\u0026rsquo;t have to cross the line.\nFor more insights, I recommend watching the Majestic Modular Monoliths talk by Axel Fontaine.\nConclusion Your time is precious, you don\u0026rsquo;t want to be spending it unwisely or come to regret too many engineering decisions later.\nOnly experience can make you aware of the tradeoffs you will make in the early stages of a product. I hope that mine will help you avoid some traps and make better choices when designing a greenfield project.\n","permalink":"https://geowarin.com/what-i-did-wrong-as-a-cto/","summary":"I spent one year as a CTO for a startup. Here are some technical decisions I came to regret and those that I would make again in my next project","title":"What I Did Wrong as a CTO"},{"content":"Since React is just the view layer of your front-end stack, the community had to provide the solutions for the remaining problems.\nRight now there seems to be a consensus for the best libraries. The recommended stack is:\n Building: Webpack and babel Managing your UI state: Redux and ImmutableJS Routing: React-router (ahem!)  But there seems to be lots of options to connect Redux to your API.\nYesterday, I decided to see what Shasta had in store for us. I have to say that I was more than happy with what I saw!\nCheck out my demo project on github.\nWhat is Shasta? Shasta is the latest project of @Contra (Eric Schoffstall), previously known for Gulp.\nThe idea is to take the best practices and libraries used by the React community and stitch them together with nice helpers.\nIt is an opinionated library.\nWithout surprise, you will find support for all the libraries cited above.\nShasta is very ambitious and aims to help you solve Server Side Rendering, manage security and user sessions, etc.\nCheck the shasta-boilerplate for a more comprehensive example.\nIn this article I will focus on Tahoe and the shasta data view.\nI think that those two things alone are well worth a blog post!\nA word of warning! Shasta is under development. All the dependencies in the demo are pointing to the github repositories. There are no npm releases yet!\nLikewise, the documentation is very sparse at the moment.\nThinks are likely to change or break. You have been warned.\nYour best bet to learn more about Shasta right now is to listen to the Javascript Jabber podcast episode on Shasta.\nThe store The central element in Shasta is the store. It is very similar to Redux\u0026rsquo;s store but it adds the notion of plugins.\nTo get started quickly, I created a project [using nwb]({% post_url 2016-02-18-react-freshness %}).\nHere is what the project looks like after setting up Shasta with the router and Tahoe:\nLet\u0026rsquo;s dig into the core package.\nstore.js allows you to reference the store as a singleton. This is also where you register the plugins you use:\nimport { createStore, createReducer } from \u0026#39;shasta\u0026#39;; import localReducers from \u0026#39;../reducers/.lookup\u0026#39;; import plugins from \u0026#39;./plugins\u0026#39;; export default createStore({ plugins: plugins, reducers: [ createReducer(localReducers) ] }) plugins.js is very simple. In this example, I use two plugins, Tahoe and shasta-router:\nimport * as router from \u0026#39;shasta-router\u0026#39; import * as api from \u0026#39;tahoe\u0026#39;; export default [ api, router ] Finally, the actions.js:\nimport { actions as routeActions } from \u0026#39;shasta-router\u0026#39; import { createActions, createReducerActions } from \u0026#39;shasta\u0026#39;; import store from \u0026#39;./store\u0026#39; import localActions from \u0026#39;../api/.lookup\u0026#39; import localReducers from \u0026#39;../reducers/.lookup\u0026#39;; export default createActions({ ...localActions, ...routeActions, ...createReducerActions(localReducers) }, store.dispatch) Plugins What are those plugins? Shasta defines multiple extension points. Most notably, your plugin can export reducers and middlewares which dramatically help reducing the boilerplate.\nWith the example above, you will get your routes stored in Redux with react-router-redux, and the setup for the Redux Devtools chrome extension.\nTahoe also adds a bunch of reducers to handle our API calls.\nWhat about the .lookup file? The .lookup files are a really nice idea, they use the glob-loader to re-export all the js files according to a glob expression.\nThis avoids writing repetitive and error prone-code like:\nimport * as reducer1 from \u0026#39;./reducer1\u0026#39; import * as reducer2 from \u0026#39;./reducer2\u0026#39; export default { reducer1, reducer2 } The Root Component Here is a quick glance at the Root component of our application. It is very classic, just note that Shasta adds support for additional PropTypes like routes or immutable types.\nimport React from \u0026#34;react\u0026#34;; import {Provider, Component, PropTypes} from \u0026#34;shasta\u0026#34;; import {Router} from \u0026#34;shasta-router\u0026#34;; export default class RootView extends Component { static displayName = \u0026#39;RootView\u0026#39;; static propTypes = { history: PropTypes.object.isRequired, store: PropTypes.object.isRequired, routes: PropTypes.node.isRequired }; render () { const {store, history, routes} = this.props; return ( \u0026lt;Provider store={store}\u0026gt; \u0026lt;Router history={history}\u0026gt; {routes} \u0026lt;/Router\u0026gt; \u0026lt;/Provider\u0026gt; ) } } Our first reducer: the counter! This one took you by surprise, didn\u0026rsquo;t it? Just to show the ideas behind Shasta, here is the reducer for our sacred counter example.\nreducers/counter.js:\nimport { Map } from \u0026#39;immutable\u0026#39;; export const initialState = Map({ count: 1 }); export const increment = (state, { payload = 1 }) =\u0026gt; state.update(\u0026#39;count\u0026#39;, c =\u0026gt; c + payload); export const decrement = (state, { payload = 1 }) =\u0026gt; state.update(\u0026#39;count\u0026#39;, c =\u0026gt; c - payload); export const reset = () =\u0026gt; initialState; Here, you see a very straightforward implementation of a reducer. It leverages the ImmutableJS API to create those nice little one-liners.\nHere is how to use those reducers as actions in your views:\nimport React from \u0026#34;react\u0026#34;; import {connect, Component} from \u0026#34;shasta\u0026#34;; import actions from \u0026#34;../core/actions\u0026#34;; @connect({ count: \u0026#39;counter.count\u0026#39; }) export default class Counter extends Component { render() { return \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Counter\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;{this.props.count}\u0026lt;/p\u0026gt; \u0026lt;button onClick={() =\u0026gt; actions.counter.increment()}\u0026gt; Increment \u0026lt;/button\u0026gt; \u0026lt;button onClick={() =\u0026gt; actions.counter.decrement()}\u0026gt; Decrement \u0026lt;/button\u0026gt; \u0026lt;button onClick={() =\u0026gt; actions.counter.reset()}\u0026gt; Reset \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; } } With our lookup file setup, there is nothing else to do. Your actions will automatically be available from all components in the actions.counter namespace.\nThe @connect annotation reminds a lot of Redux but it has been modified to work seamlessly with immutable data types.\nLet\u0026rsquo;s fetch data! Let\u0026rsquo;s create a api/chuck.js file:\nimport { createAction } from \u0026#39;tahoe\u0026#39;; import { Schema } from \u0026#39;normalizr\u0026#39;; const response = new Schema(\u0026#39;some-response\u0026#39;); export const getRandomFact = createAction({ endpoint: () =\u0026gt; `http://api.icndb.com/jokes/random`, method: \u0026#39;GET\u0026#39;, model: response }); Shasta has direct support for normalizr. This will help us store our entities in a normalized way as we will see in a moment.\nUnder the hood, Tahoe uses superagent to make HTTP requests.\nLet\u0026rsquo;s see how to use this in a view:\nimport React from \u0026#34;react\u0026#34;; import {connect} from \u0026#34;shasta\u0026#34;; import actions from \u0026#34;../core/actions\u0026#34;; import DataComponent from \u0026#34;shasta-data-view\u0026#34;; @connect({ joke: \u0026#39;api.subsets.joke\u0026#39; }) export default class ChuckFact extends DataComponent { resolveData () { actions.chuck.getRandomFact({ subset: \u0026#39;joke\u0026#39; }); } renderLoader () { return ( \u0026lt;div\u0026gt; Loading... \u0026lt;/div\u0026gt; ) } renderData ({joke}) { return \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Chuck Norris Fact\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;{joke.getIn([\u0026#39;value\u0026#39;, \u0026#39;joke\u0026#39;])}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; } renderErrors (errors) { console.error(errors); return ( \u0026lt;div\u0026gt; There was an error fetching Chuck Norris facts \u0026lt;/div\u0026gt; ) } } Shasta has a DataComponent class that will help you manage the pattern of displaying a loading message while fetching the data, and errors if the request fail.\nWhen using a Tahoe action, you can optionally specify a subset in which the fetched data will end up.\nThis is great to scope your fetch requests to a component. Here, our request ends up in the api.subsets.joke namespace.\nHere is the JSON returned by our API so you can understand the getIn call:\n{ \u0026#34;type\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;id\u0026#34;: 459, \u0026#34;joke\u0026#34;: \u0026#34;Chuck Norris can solve the Towers of Hanoi in one move.\u0026#34;, \u0026#34;categories\u0026#34;: [ \u0026#34;nerdy\u0026#34; ] } } Using normalizr If we use a schema for the API call, we can tell normalizr that the value field in our JSON response is actually an entity.\nAs such, we will be able to retrieve it in the api.entities namespace.\nconst response = new Schema(\u0026#39;response\u0026#39;); const joke = new Schema(\u0026#39;jokes\u0026#39;); response.define({ value: joke }); Normalizr will create a map of jokes indexed by ids and store it in api.entities.jokes. Which allows us to write something like so:\n@connect({ jokes: \u0026#39;api.entities.jokes\u0026#39; }) export default class ChuckFact extends DataComponent { renderData ({jokes}) { const jokesEl = jokes.valueSeq().map((joke, id) =\u0026gt; { return \u0026lt;div key={id}\u0026gt;{joke.get(\u0026#39;joke\u0026#39;)}\u0026lt;/div\u0026gt; }); return \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Chuck Norris Fact\u0026lt;/h2\u0026gt; {jokesEl} \u0026lt;/div\u0026gt; } } We can also use a function in the @connect decorator:\nconst getFirstJoke = (store) =\u0026gt; { return store.getIn([\u0026#39;api\u0026#39;, \u0026#39;subsets\u0026#39;, \u0026#39;myJoke\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;value\u0026#39;, \u0026#39;joke\u0026#39;]); }; @connect({ joke: getFirstJoke }) export default class ChuckFact extends DataComponent { resolveData () { actions.chuck.getRandomFact({ subset: \u0026#39;myJoke\u0026#39; }); } renderData ({joke}) { return \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Chuck Norris Fact\u0026lt;/h2\u0026gt; {joke} \u0026lt;/div\u0026gt; } } Conclusion I\u0026rsquo;m very excited about Shasta. It solves a long-standing problem in the React community with an unmatched elegance.\nIt might be that piece that a lot of us have been missing in our projects.\n","permalink":"https://geowarin.com/consuming-apis-with-redux-the-shasta-way/","summary":"Connecting your REST API to Redux used to be hard\u0026hellip; But that was before Shasta!","title":"Consuming APIs with Redux, the Shasta way"},{"content":"In the javascript community, some people have experienced javascript fatigue.\nFor me, this fatigue is two folds. First, you need to keep up to date with the frantic pace of redux, react-router and friends and make sure you will be able to migrate your code to the new major versions.\nSecond, Webpack configuration is not always straight-forward. I know a lot of people that really want to see what React is like and play with it without having to cope with a lot of configuration up-front.\nI have no solution for the first problem. To me, innovation in this community feels like a fantastic thing. Watching the github repos and following people on Twitter is my way to keep up-to-date with the latest improvements.\nI also want to point out that the most used tools seem to be more and more stable. It is unlikely that redux or react-router will go through a full rewrite now. So relax. If you\u0026rsquo;re not able to update your dependencies every two days, it\u0026rsquo;s probably not the end of the world.\nIn this article, I will show you two ways to get started with React with zero configuration. So you can start hacking right away when you\u0026rsquo;re still fresh!\nQuick prototyping with babel browser transform So you need to get some React code out of the door now. You don\u0026rsquo;t care about hot reloading and want to write some React and ES2015 code in a web page.\nJim Sproch has a very cool solution for us.\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script src=\u0026#34;http://www.jimsproch.com/react/future/react.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://www.jimsproch.com/react/future/react-dom.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://www.jimsproch.com/react/babel-browser.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;container\u0026#34; /\u0026gt; \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; ReactDOM.render(\u0026lt;div\u0026gt;Hello World!\u0026lt;/div\u0026gt;, document.getElementById(\u0026#39;container\u0026#39;)); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; And that\u0026rsquo;s about it! Every script tag with the text/babel type will be transformed with babel. So you have support for destructuring, arrow functions and, of course, JSX. It will even work for external scripts so you don\u0026rsquo;t need to write all your code in the page.\nNow, Jim likes to work with the bleeding edge beta of React but you can easily switch react.js and react-dom with production versions:\n\u0026lt;script src=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/react/0.14.7/react.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/react/0.14.7/react-dom.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; The downside of this solution, of course, is that the transform is executed by the client\u0026rsquo;s browser so this will be slower than pre-compiled Babel. And you don\u0026rsquo;t have hot reloading available.\nBut still, zero config.\nA true React project with nwb Nwb will take care of all the webpack and babel configuration.\nInstall it as a global package:\nnpm i -g nwb You can then create a React project:\nnwb new react-app my-nwb-app And it will scaffold the application for you. It is a very simple application:\nnwb: new react-app create .gitignore create .travis.yml create README.md create nwb.config.js create package.json create public/index.html create src/App.js create src/index.js create tests/.eslintrc create tests/App-test.js No configuration outside of nwb.config.js, which is almost empty.\nLet\u0026rsquo;s start the application:\nnpm start Try to modify App.js\u0026hellip; Hot reloading works!\nNow let\u0026rsquo;s run the tests:\n$ nwb test START: 18 02 2016 10:14:54.838:INFO [karma]: Karma v0.13.18 server started at http://localhost:9876/ 18 02 2016 10:14:54.846:INFO [launcher]: Starting browser PhantomJS 18 02 2016 10:14:56.139:INFO [PhantomJS 1.9.8 (Mac OS X 0.0.0)]: Connected on socket IHNx80uh9I6VW9fjAAAA with id 617985 App component ‚úî displays a welcome message Finished in 0.01 secs / 0.004 secs SUMMARY: ‚úî 1 test completed We\u0026rsquo;ve got karma and coverage pre-configured so we can start TDDing right away.\nAnd the best part, we can build the application and get an optimized version of the scripts.\n$ npm run build \u0026gt; my-nwb-app@1.0.0 build /Users/geowarin/dev/react/my-nwb-app \u0026gt; nwb build nwb: clean-app nwb: build-react-app Hash: 81e127933ddb73bbdfb4 Version: webpack 1.12.11 Time: 3234ms Asset Size Chunks Chunk Names vendor.js 131 kB 0 [emitted] vendor app.js 971 bytes 1 [emitted] app vendor.js.map 1.54 MB 0 [emitted] vendor app.js.map 4.13 kB 1 [emitted] app Nwb also has support for sass, stylus and less.\nNwb gotchas Nwb has opinions. And that\u0026rsquo;s a good thing. For example, you write tests with Karma and that\u0026rsquo;s it.\nSupport for Babel 6 is not there yet so it\u0026rsquo;s not completely bleeding edge.\nBut you can serenely leave the hard part of configuring your app to Nwb. It\u0026rsquo;s got a very impressive test suite and coverage. You\u0026rsquo;re in good hands!\nConclusion Feeling fatigued? I do not! I wish there were more projects like Nwb with strong opinions and one easy way to do things.\nBut it is possible to get started with React and even have a production-ready application with zero configuration.\nNever used React? You have no more excuses. Get to work and help this community improve!\n","permalink":"https://geowarin.com/react-freshness/","summary":"We\u0026rsquo;ve heard a lot about Javascript fatigue but what if I showed you two simple ways to get started with React without a single line of configuration? Refresh!","title":"React freshness"},{"content":"Nowadays, it\u0026rsquo;s getting rare and even a bit annoying when a service rolls up its own authentication mechanism instead of relying on a OAuth sign-on with our social networks.\nLogin via social networks means fewer passwords to remember, and stronger guarantees in terms of security because you can check and control the authorizations of the applications you use.\nIn this article, I will show you how to allow users to log into your application via Twitter from a rich Javascript client (React).\nWe will also persist our users connections in database.\nThe code is available on github.\nSetting up your app on Twitter Before coding anything, you will need to create a new Twitter application in your twitter apps page.\nThen go to the \u0026ldquo;Keys and access tokens\u0026rdquo; tab and note your API key and API secret ids.\nCreating a Spring Boot app Use the Spring initializer to create a new Spring Boot application. You will need the following dependencies:\ndependencies { compile(\u0026#39;org.springframework.boot:spring-boot-devtools\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-starter-security\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-starter-social-twitter\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-starter-jdbc\u0026#39;) compile(\u0026#39;com.h2database:h2\u0026#39;) } Please copy your appId and appSecret in the application.properties file and configure a few things:\nspring.social.twitter.appId= \u0026lt;Consumer Key\u0026gt; spring.social.twitter.appSecret= \u0026lt;Consumer Secret\u0026gt; # Disable auto views, we are making an API spring.social.auto-connection-views=false # Disable basic security security.basic.enabled=false # Persist H2 data to disk to keep connection info between reboots spring.datasource.url=jdbc:h2:~/social-test Sign-in and Sign-up flows Spring social has two different flows when someone logs into your app via social networks.\nThe first time someone logs into your application, they will go through the sign-up flow. If their ID already registered in Spring Social, they will go through the sign-in flow instead.\nYou job is to create a SignInAdapter that will handle the sign-in process and a controller that will decide what to do during the sign-up process (you will receive a request on the /signup URL by default).\nHere is an overview of the authentication flow in Spring Social:\n Your application produces a POST request to /signin/{providerId} The ProviderSigninController then redirects the user to the identification provider\u0026rsquo;s sign-in screen The user logs in The identification provider will send the OAuth token with GET request to /signin/{providerId} If the user is not found in the UsersConnectionRepository, the controller will use a SessionStrategy to store the pending login request and will then redirect to the signupUrl page If the user is found, your SignInAdapter interface is called  If you want to know more details about Spring social inner workings, check the SocialWebAutoConfiguration class of Spring Boot and the ProviderSignInController class of Spring Social.\nIn the above diagram, we can see that we have two more extensions points:\n The SessionStrategy. By default, it stores temporary information about the connection in the HTTP session The UsersConnectionRepository. By default, Spring boot provides an InMemoryUsersConnectionRepository. Connections will be lost when your application reboots.  Spring Security Config We need to enable security in our application. Let\u0026rsquo;s create a classic security configuration. It will make sure that users using our REST api are authenticated but will let calls to /api/session, our authentication end point, go through:\n@Configuration @Order(SecurityProperties.ACCESS_OVERRIDE_ORDER) public class SecurityConfiguration extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(\u0026#34;/api/session\u0026#34;).permitAll() .antMatchers(\u0026#34;/h2-console/**\u0026#34;).permitAll() .antMatchers(\u0026#34;/api/**\u0026#34;).authenticated() .and() .headers().frameOptions().disable() // for h2  .and() .requestCache() .requestCache(new NullRequestCache()) .and() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.IF_REQUIRED) .and().csrf().disable(); } } The authentication end point is very simple. It provides a way to GET the current session and to DELETE it to logout:\n@RestController @RequestMapping(\u0026#34;/api/session\u0026#34;) public class AuthenticationResource { @Autowired AuthenticationManager authenticationManager; @RequestMapping(method = RequestMethod.GET) public User session(Principal user) { String name = user == null ? null : user.getName(); return new User(name); } @RequestMapping(method = RequestMethod.DELETE) public void logout(HttpSession session) { session.invalidate(); } } Sign-in To handle the sign-in, we need to provide a SignInAdapter:\n@Configuration public class SocialConfiguration { @Bean public SignInAdapter authSignInAdapter() { return (userId, connection, request) -\u0026gt; { AuthUtil.authenticate(connection); return null; }; } } We can create a simple authenticate method that will take a Social Connection and authenticate the user using Spring Security\u0026rsquo;s context:\npublic class AuthUtil { protected static final Logger log = LoggerFactory.getLogger(AuthUtil.class); public static void authenticate(Connection\u0026lt;?\u0026gt; connection) { UserProfile userProfile = connection.fetchUserProfile(); String username = userProfile.getUsername(); UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(username, null, null); SecurityContextHolder.getContext().setAuthentication(authentication); log.info(\u0026#34;User {} {} connected.\u0026#34;, userProfile.getFirstName(), userProfile.getLastName()); } } Note that we have access to our user\u0026rsquo;s profile with the Connection object.\nSign-up Here is a simple implementation of a signup controller:\n@Controller public class SignupController { private final ProviderSignInUtils signInUtils; @Autowired public SignupController(ConnectionFactoryLocator connectionFactoryLocator, UsersConnectionRepository connectionRepository) { signInUtils = new ProviderSignInUtils(connectionFactoryLocator, connectionRepository); } @RequestMapping(value = \u0026#34;/signup\u0026#34;) public String signup(WebRequest request) { Connection\u0026lt;?\u0026gt; connection = signInUtils.getConnectionFromSession(request); if (connection != null) { AuthUtil.authenticate(connection); signInUtils.doPostSignUp(connection.getDisplayName(), request); } return \u0026#34;redirect:/\u0026#34;; } } There are two things to note here:\n Spring lets us decide what to do the first time we register a user. In this example, we just authenticate him The SignInUtils class is very handy to handle this scenario. Its constructor optionally takes a SessionStrategy that will be used to retrieve the connection info. You can customize the strategy here.  A this point, your authentication process should work. It will use the HTTP Session to store connection data and an in-memory user repository.\nThe client We can create a very simple client with any web framework. It will need to:\n Issue a GET /api/session request to check if the user is logged Display a login form that will POST to /login/twitter if not Display a logout button if the user is connected. The logout button will send a DELETE /api/session request.  I chose to use React because of its very simple and declarative API.\nWe can use ES2015 features and JSX without a pre-compilation step thanks to this script.\nIt is a bit slower because it lets the browser do the compilation but it\u0026rsquo;s perfect for prototyping.\nHere is the client code:\nconst LoginForm = () =\u0026gt; ( \u0026lt;form action=\u0026#34;/signin/twitter\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Please login\u0026lt;/h1\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Login\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ); const LogoutComponent = (props) =\u0026gt; ( \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Your name is {props.name}\u0026lt;/h2\u0026gt; \u0026lt;button onClick={props.logout}\u0026gt;Logout\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); class Main extends React.Component { constructor(...args) { super(...args); this.state = {name: null}; } componentDidMount() { fetch(\u0026#39;/api/session\u0026#39;, {credentials: \u0026#39;same-origin\u0026#39;}) .then(res =\u0026gt; res.json()) .then(session =\u0026gt; this.setState({name: session.name})); } logout() { console.log(\u0026#34;logout\u0026#34;); fetch(\u0026#39;/api/session\u0026#39;, {method: \u0026#39;delete\u0026#39;, credentials: \u0026#39;same-origin\u0026#39;}) .then(res =\u0026gt; this.setState({name: null})); } render() { const profile = this.state.name ? \u0026lt;LogoutComponent name={this.state.name} logout={() =\u0026gt; this.logout()}/\u0026gt; : \u0026lt;LoginForm /\u0026gt;; return ( \u0026lt;div\u0026gt; {profile} \u0026lt;/div\u0026gt; ) } } ReactDOM.render(\u0026lt;Main /\u0026gt;, document.getElementById(\u0026#39;container\u0026#39;)); We use the fetch API (a modern replacement for XMLHttpRequest) to make ajax calls so the code will only work in FF and Chrome. You can find various polyfills in the wild.\nNotice that we need to pass {credentials: 'same-origin'} to the REST API calls to make sure we send the cookies along with the requests.\nStore the connections in database In most applications, we want to store the user already known in a database.\nSpring social provides a default JDBC implementation of the UsersConnectionRepository. Take a look the documentation to know more about the table in which it will store users' connections data.\nWe can initialize the database with the script contained in Spring Social\u0026rsquo;s jar:\n@Component public class DbInitializer implements InitializingBean { private final DataSource dataSource; @Autowired public DbInitializer(DataSource dataSource) { this.dataSource = dataSource; } @Override public void afterPropertiesSet() throws Exception { ClassPathResource resource = new ClassPathResource(\u0026#34;org/springframework/social/connect/jdbc/JdbcUsersConnectionRepository.sql\u0026#34;); runScript(resource); } private void runScript(Resource resource) { ResourceDatabasePopulator populator = new ResourceDatabasePopulator(); populator.setContinueOnError(true); populator.addScript(resource); DatabasePopulatorUtils.execute(populator, dataSource); } } Now, we need to create a SocialAdapater that will use Spring Social\u0026rsquo;s JdbcUsersConnectionRepository as a user repository:\nclass DatabaseSocialConfigurer extends SocialConfigurerAdapter { private final DataSource dataSource; public DatabaseSocialConfigurer(DataSource dataSource) { this.dataSource = dataSource; } @Override public UsersConnectionRepository getUsersConnectionRepository(ConnectionFactoryLocator connectionFactoryLocator) { TextEncryptor textEncryptor = Encryptors.noOpText(); return new JdbcUsersConnectionRepository(dataSource, connectionFactoryLocator, textEncryptor); } } Don\u0026rsquo;t forget to declare our DatabaseSocialConfigurer as a Spring bean and we are good to go!\nPlease note that this works because of an \u0026ldquo;interesting\u0026rdquo; design decision of Spring Social. You can see here that Spring Social will take the first SocialConfigurer that declares a non-null UsersConnectionRepository.\nOurs come before the SocialConfigurers auto-configured by Spring Boot but if you are wary of this implementation, consider disabling Spring boot auto-configuration.\nWe can use the h2 console Spring boot auto-configured for us to check the database.\nConclusion Social login with Spring is a bit tricky but definitely worth the investment!\nSpring Boot provides default configuration for LinkedIn and Facebook as well but there are many more connectors like Github and Tripit that you can include by replicating Spring Boot\u0026rsquo;s configuration.\nDon\u0026rsquo;t forget to check out the code and give your opinion in the comments.\n","permalink":"https://geowarin.com/social-login-with-spring/","summary":"Log-in with your social account in your Spring application","title":"Social login with Spring"},{"content":"If you are starting a Spring Boot project today, chances are that you want to use Java 8.\nOne of the most awesome features in Java 8 is the Date and Time API, also known as JSR-310.\nBy default, Jackson will treat the new dates as normal objects and serialize all the fields that they contain, which will probably not suit your needs.\nI will show you how to fix the problem with the jackson-datatype-jsr310 library, within a Spring Boot project, but the concepts here are applicable to any application using Jackson.\nThe code is available on github if you want to take a look.\nThe problem Let\u0026rsquo;s write a simple controller:\n@RestController public class DateController { @RequestMapping(\u0026#34;/localDate\u0026#34;) public LocalDate todayLocalDate() { return LocalDate.now(); } @RequestMapping(\u0026#34;/offsetDateTime\u0026#34;) public OffsetDateTime todayOffsetDateTime() { return OffsetDateTime.now(); } } Simple, right? What could possibly go wrong?\nWell, it\u0026rsquo;s probably not what you expected. This output is not going to be easy to use in your client application.\nMore importantly, are you going to send this kind of format to your server when you are targeting a Java date?\nThe output of offsetDateTime is pretty similar in terms of unusualness.\nThe solution Turns out that the solution is pretty straight-forward. Just add the following dependency to your project:\ncompile \u0026#39;com.fasterxml.jackson.datatype:jackson-datatype-jsr310\u0026#39; And the result, for LocalDateTime:\n[ 2016, 2, 2 ] And for OffsetDateTime:\n1454451664.708000000 Happy? No? Let\u0026rsquo;s try improve the solution.\nTweaking the output If you look at how the library works internally, you will see that the output depends on some features being activated or not.\nTo have a better default, we can override the default ObjectMapper and give it a different config:\n@Configuration public class JacksonConfig { @Bean @Primary public ObjectMapper objectMapper(Jackson2ObjectMapperBuilder builder) { ObjectMapper objectMapper = builder.createXmlMapper(false).build(); objectMapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false); // objectMapper.configure(SerializationFeature.WRITE_DATE_TIMESTAMPS_AS_NANOSECONDS, false);  return objectMapper; } } This time the result is, for LocalDateTime:\n2016-02-02 And for OffsetDateTime:\n2016-02-02T23:24:08.255+01:00 If you uncomment the second line, dates will be written as timestamps without the nanoseconds but unfortunately, it is mutually exclusive with the first option.\nNevertheless, those formats are a lot more sensible and understandable by client libraries like momentjs.\nConclusion It takes just a little configuration to make JSR-310 dates behave correctly with Jackson and Spring Boot.\nAs always, check out the project on github and tell if this helped!\n","permalink":"https://geowarin.com/correctly-handle-jsr-310-java-8-dates-with-jackson/","summary":"The DateTime API in Java 8 is awesome but default Jackson serialization is not. Let\u0026rsquo;s fix that!","title":"Correctly handle JSR-310 (java 8) dates with Jackson"},{"content":"The Java driver for RethinkDB has recently been released in beta.\nI created a little chat application with Spring Boot, you can see the result on github.\nThere is a docker-compose file at the root of the project that you can use to run a RethinkDB instance instead of installing it directly on your machine.\nWhy RethinkDB? I already gave RethinkDB a try a few months ago and I was very impressed with its beautiful admin GUI, its clustering capabilities and its clever and intuitive API.\nBut there is more! RethinkDB is a DB engine designed to push updates to the clients in real time.\nIn the CAP theorem, rethinkDB focuses on being Consistent in case of difficulties in the cluster.\nRelevant quote from the FAQ:\nOnce RethinkDB is started, you can connect on the beautiful admin GUI on port 8080:\nSetting up the project I created a Gradle project with the web and websocket Spring boot starters. I also added a couple of webjars: jquery for ajax requests, sockjs and stomp to connect to Spring\u0026rsquo;s websockets:\ndependencies { compile(\u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-starter-websocket\u0026#39;) compile(\u0026#39;org.springframework.boot:spring-boot-devtools\u0026#39;) compile \u0026#39;org.webjars:jquery:3.0.0-alpha1\u0026#39; compile \u0026#39;org.webjars:sockjs-client:1.0.0\u0026#39; compile \u0026#39;org.webjars:stomp-websocket:2.3.3\u0026#39; compile \u0026#39;com.rethinkdb:rethinkdb-driver:2.2-b1-SNAPSHOT\u0026#39; } Getting a connection Every action we will perform on the database will require a Connection. We can create a small factory that we will later use in the code:\npublic class RethinkDBConnectionFactory { private String host; public RethinkDBConnectionFactory(String host) { this.host = host; } public Connection\u0026lt;ConnectionInstance\u0026gt; createConnection() { try { return RethinkDB.r.connection().hostname(host).connect(); } catch (TimeoutException e) { throw new RuntimeException(e); } } } Initializing the DB For this little chat, we will need a database called chat and a table called messages.\nTo avoid creating them by hand, we can create a Spring bean that will get called when the application starts:\npublic class DbInitializer implements InitializingBean { @Autowired private RethinkDBConnectionFactory connectionFactory; @Autowired private ChatChangesListener chatChangesListener; private static final RethinkDB r = RethinkDB.r; @Override public void afterPropertiesSet() throws Exception { createDb(); // we will see that later on  chatChangesListener.pushChangesToWebSocket(); } private void createDb() { Connection\u0026lt;ConnectionInstance\u0026gt; connection = connectionFactory.createConnection(); List\u0026lt;String\u0026gt; dbList = r.dbList().run(connection); if (!dbList.contains(\u0026#34;chat\u0026#34;)) { r.dbCreate(\u0026#34;chat\u0026#34;).run(connection); } List\u0026lt;String\u0026gt; tables = r.db(\u0026#34;chat\u0026#34;).tableList().run(connection); if (!tables.contains(\u0026#34;messages\u0026#34;)) { r.db(\u0026#34;chat\u0026#34;).tableCreate(\u0026#34;messages\u0026#34;).run(connection); r.db(\u0026#34;chat\u0026#34;).table(\u0026#34;messages\u0026#34;).indexCreate(\u0026#34;time\u0026#34;).run(connection); } } } Ignore the pushChangesToWebSocket() method call for now, we will see this in a minute.\nWe can already get a feel for the RethinkDB API. It was originally designed for dynamically typed language so some things might be a little awkward for hardcore Java developers.\nFor instance, the result of the operations can be of any type. RethinkDB will try to coerce the result according to the return type chosen, if possible.\nThis is both good, because of the additional flexibility, and bad, because you cannot rely on autocomplete to know the return type of an operation.\nThe ChatController The chat controller will react to two things:\n GETting the last 20 messages from the DB POSTing a new message  Here is the code, which is kind of straight-forward:\n@RestController @RequestMapping(\u0026#34;/chat\u0026#34;) public class ChatController { protected final Logger log = LoggerFactory.getLogger(ChatController.class); private static final RethinkDB r = RethinkDB.r; @Autowired private RethinkDBConnectionFactory connectionFactory; @RequestMapping(method = RequestMethod.POST) public ChatMessage postMessage(@RequestBody ChatMessage chatMessage) { chatMessage.setTime(OffsetDateTime.now()); Object run = r.db(\u0026#34;chat\u0026#34;).table(\u0026#34;messages\u0026#34;).insert(chatMessage) .run(connectionFactory.createConnection()); log.info(\u0026#34;Insert {}\u0026#34;, run); return chatMessage; } @RequestMapping(method = RequestMethod.GET) public List\u0026lt;ChatMessage\u0026gt; getMessages() { List\u0026lt;ChatMessage\u0026gt; messages = r.db(\u0026#34;chat\u0026#34;).table(\u0026#34;messages\u0026#34;) .orderBy().optArg(\u0026#34;index\u0026#34;, r.desc(\u0026#34;time\u0026#34;)) .limit(20) .orderBy(\u0026#34;time\u0026#34;) .run(connectionFactory.createConnection(), ChatMessage.class); return messages; } } The cool thing is that the API clean and simple to understand.\nSome things are still a bit funny:\n The optArg after the orderBy is a bit cryptic I spent some time figuring out that your POJO class must not contain any id attribute for the auto-generation to work  Setting up websockets Now that we can read and write from the DB, we need to push the updates to the client in real time.\nWe will use websockets over SockJS for that. The configuration for websockets is pretty classic:\n@Configuration @EnableWebSocketMessageBroker public class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer { @Override public void configureMessageBroker(MessageBrokerRegistry config) { config.enableSimpleBroker(\u0026#34;/topic\u0026#34;); } @Override public void registerStompEndpoints(StompEndpointRegistry registry) { registry.addEndpoint(\u0026#34;/chatWS\u0026#34;).withSockJS(); } } How to read that:\n Our clients will be able to connect to the /chatWS endpoint The clients will then have the possibility to listen to any topic whose url begins with /topic (i.e, /topic/messages) and get notified in real time  Listening to the updates We will now listen to database updates in a thread and broadcast the changes to all the clients listening on the web socket.\nWe use the @Async annotation, so Spring will take care of running the code in a thread for us:\n@Service public class ChatChangesListener { protected final Logger log = LoggerFactory.getLogger(ChatChangesListener.class); private static final RethinkDB r = RethinkDB.r; @Autowired private RethinkDBConnectionFactory connectionFactory; @Autowired private SimpMessagingTemplate webSocket; @Async public void pushChangesToWebSocket() { Cursor\u0026lt;ChatMessage\u0026gt; cursor = r.db(\u0026#34;chat\u0026#34;).table(\u0026#34;messages\u0026#34;).changes() .getField(\u0026#34;new_val\u0026#34;) .run(connectionFactory.createConnection(), ChatMessage.class); while (cursor.hasNext()) { ChatMessage chatMessage = cursor.next(); log.info(\u0026#34;New message: {}\u0026#34;, chatMessage.message); webSocket.convertAndSend(\u0026#34;/topic/messages\u0026#34;, chatMessage); } } } So what happens here? Each time a change happens in the database, we will get an update. This update will contain two fields: old_val and new_val. See the documentation.\nSince we are only interested in the new things, we will only retrieve the new_val field.\nNote that the second (optional) argument to the run method is a class. If present, RethinkDB will try to convert the data to this target class, just like we did in the ChatController above.\nThen, we simply broadcast the message to all the clients listening on /topic/messages.\nThe client If you never used webjars before, they are simply jar packages containing frontend dependencies. With Spring Boot we can use them in our web pages directly. Below the index.html file of our application:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;webjars/jquery/3.0.0-alpha1/jquery.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;webjars/sockjs-client/1.0.0/sockjs.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;webjars/stomp-websocket/2.3.3/stomp.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;js/main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;chat\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;messages\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;form onsubmit=\u0026#34;sendMessage(); return false;\u0026#34;\u0026gt; \u0026lt;label\u0026gt; Message: \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;messageInput\u0026#34; /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; And the javascript:\nvar userName = window.prompt(\u0026#34;Enter your name\u0026#34;, \u0026#34;some user\u0026#34;); function appendMessage(message) { $(\u0026#39;#messages\u0026#39;).append($(\u0026#39;\u0026lt;div /\u0026gt;\u0026#39;).text(message.from + \u0026#34;: \u0026#34; + message.message)) } function getPreviousMessages() { $.get(\u0026#39;/chat\u0026#39;).done(messages =\u0026gt; messages.forEach(appendMessage)); } function sendMessage() { var $messageInput = $(\u0026#39;#messageInput\u0026#39;); var message = {message: $messageInput.val(), from: userName}; $messageInput.val(\u0026#39;\u0026#39;); post(\u0026#39;/chat\u0026#39;, message); } function onNewMessage(result) { var message = JSON.parse(result.body); appendMessage(message); } function connectWebSocket() { var socket = new SockJS(\u0026#39;/chatWS\u0026#39;); stompClient = Stomp.over(socket); //stompClient.debug = null;  stompClient.connect({}, (frame) =\u0026gt; { console.log(\u0026#39;Connected: \u0026#39; + frame); stompClient.subscribe(\u0026#39;/topic/messages\u0026#39;, onNewMessage); }); } getPreviousMessages(); connectWebSocket(); Conclusion RethinkDB is an awesome database, especially because it lets you decouple the code that updates the database and the code that listens to the changes.\nThe driver is brand new and still in beta but we can already salute the efforts of the developers for such an amazing work!\nAs always, check out the project on github and tell me what you think!\n","permalink":"https://geowarin.com/a-simple-chat-with-spring-boot-and-rethinkdb/","summary":"RethinkDB is a great database engine allowing you to receive live updates on your data. Let\u0026rsquo;s create a Spring Boot App and give it a try!","title":"A simple chat with Spring Boot and RethinkDB"},{"content":"I think Groovy is a wonderful language. However, I would not advise a complete rewrite of your project in Groovy!\nWe can however use Groovy to test our Java code.\nI\u0026rsquo;m actually a big fan of this approach. I\u0026rsquo;ve been using it to test legacy applications written in Java.\nI still couldn\u0026rsquo;t use lambdas or fancy Java 8 features but all of a sudden, my test code was more expressive. I could take advantage closures, power asserts, the Spock DSL and Groovy simple syntax. The best part: every library I used was a test dependency and never impacted the actual code.\nIn this article I will show you how to add Groovy tests to an existing Java application built either with Maven or Gradle.\nThe code source of a demo application using Maven and Spock is available on github.\nWhy groovy? Groovy is a dynamic language with optional typing. It means that you can have the guarantees of a type system when it matters and the versatility of duck typing when you know what your are doing.\nGroovy removes all the verbosity from the Java syntax. Some small examples:\n// map literals Map\u0026lt;String, String\u0026gt; things = [\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;] // Write to a file new File(\u0026#34;hello.txt\u0026#34;) \u0026lt;\u0026lt; \u0026#39;Hello world!\u0026#39; // Add some numbers BigInteger a = 18 BigDecimal b = 24 int sum = a + b println \u0026#34;$sum ${sum.class}\u0026#34; // 42 class java.lang.Integer  // List literals List\u0026lt;Number\u0026gt; numbers = [-2, 12, 6, 3] // Closures def result = numbers .findAll { it \u0026gt; 0 } // filter  .collect { it * 2 } // map  .sum() // reduce  // template strings println \u0026#34;This answer to life, universe and everything: ${result}\u0026#34; If you want a good introduction to groovy check out the groovy style guide.\nYou can also watch the amazing Groovy for Java developers presentation by Peter Ledbrook.\nAnother thing. Groovy let you access private class members. Although this completely violates encapsulation, you will get away with just a warning.\nIt is nice to have this kind of ability when you add tests to a legacy application before refactoring it.\nWhy Spock? Spock is a wonderful test framework.\nIt combines the best features of other frameworks like JUnit, jMock, and RSpec and let you write specifications with a nice BDD DSL.\nIt is fully compatible with JUnit so you can use all the stuff you like (rules for instance) and much more!\nIt will also completely remove the need for a mocking framework like Mockito.\nIf you want to learn more about Spock read: why spock and spock primer.\nI also found the next level spock repo interesting to look at.\nHow? You are now ready to add Spock to your tool-belt. But how?\nWith Maven Add dependencies to Groovy and Spock:\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.groovy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;groovy-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4.4\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.spockframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spock-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-groovy-2.4\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Now, you need to tell maven to compile the code contained in src/test/groovy. We will use the gmavenplus plugin for that.\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.gmavenplus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gmavenplus-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;generateStubs\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;testGenerateStubs\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.groovy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;groovy-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4.4\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- Optional --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.16\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;parallel\u0026gt;methods\u0026lt;/parallel\u0026gt; \u0026lt;threadCount\u0026gt;5\u0026lt;/threadCount\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*Test.*\u0026lt;/include\u0026gt; \u0026lt;include\u0026gt;**/*Spec.*\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;**/Abstract*.java\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; By convention, Spock tests are called specifications and spec files end with *Spec.groovy. This is optional but we can enable that behavior by overriding the surefire default configuration.\nWith gradle Just apply the groovy plugin, included by default in Gradle.\napply plugin: \u0026#39;groovy\u0026#39; Since the plugin extends the Java convention, it will automatically compile the Java code contained in src/main/java and src/test/java as well as the Groovy code contained in src/main/groovy and src/test/groovy.\nYour first Spock specification Place this little specification in src/test/groovy:\nimport spock.lang.Specification import spock.lang.Unroll class MySpec extends Specification { @Unroll def \u0026#34;max(#a,#b) == #c\u0026#34;() { expect: // This class is in our Java code  MyClass.max(a, b) == c where: a | b | c 1 | 2 | 2 42 | -12 | 42 42 | -12 | -42 } } Here is what it looks like in IntelliJ:\nYou can also verify that it works with maven by typing:\nmvn test You would get this result:\n------------------------------------------------------- T E S T S ------------------------------------------------------- Running MySpec Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.146 sec \u0026lt;\u0026lt;\u0026lt; FAILURE! - in MySpec max(42,-12) == -42(MySpec) Time elapsed: 0.105 sec \u0026lt;\u0026lt;\u0026lt; FAILURE! org.spockframework.runtime.SpockComparisonFailure: Condition not satisfied: Math.max(a, b) == c | | | | | 42 42 -12| -42 false at MySpec.max(#a,#b) == #c(MySpec.groovy:9) Results : Failed tests: MySpec.max(#a,#b) == #c:9 Condition not satisfied: Math.max(a, b) == c | | | | | 42 42 -12| -42 false Tests run: 3, Failures: 1, Errors: 0, Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1.887 s [INFO] Finished at: 2016-01-20T15:52:40+01:00 [INFO] Final Memory: 11M/309M [INFO] ------------------------------------------------------------------------ IDE integration Install the gmavnen intelliJ plugin and the spock plugin for a better integration with your IDE.\nConclusion Even if your whole codebase is in Java, your fellow developers will thank you for bringing a breath of fresh air to your project with Groovy tests.\nThe configuration to get this working is very simple. You have no excuse for not giving it a try!\nAs always, checkout the github repo and tell me your thoughts.\n","permalink":"https://geowarin.com/test-your-java-application-with-groovy/","summary":"You can write better tests for your Java application with Spock, Groovy and very little configuration","title":"Test your Java application with Groovy"},{"content":"If you like having a self-contained application as a deliverable for your project, the idea of putting your Spring applications in a Docker container might be appealing to you.\nIn this article, I will show you a simple way to make a docker image the output of your Gradle build, thanks to the gradle-docker plugin.\nThe code that we will build is a simple console application powered by Spring Boot that will periodically write Chuck Norris facts to the standard output. It is available on github and on Docker Hub.\nTake a Spring boot application You can easily generate a starter project with start.spring.io or with IntelliJ. We will create a gradle/groovy application with no Spring Boot starter to keep the code very simple.\nClick on this link to generate the project!\nUnzip it and open it in your favorite IDE. Since the application is going to loop forever, you can remove the generated test, which would loop forever too.\nAdd the following dependency to your build.gradle:\ncompile \u0026#39;org.codehaus.groovy.modules.http-builder:http-builder:0.7.1\u0026#39; Since we will use the JSONSlurper, the idiomatic way to parse JSON in groovy, we will need to the change the groovy dependency to groovy-all:\ncompile \u0026#39;org.codehaus.groovy:groovy-all\u0026#39; The code The code is really simple:\npackage com.github.geowarin import groovy.util.logging.Log4j import groovyx.net.http.RESTClient import org.apache.log4j.Level import org.springframework.boot.CommandLineRunner import org.springframework.stereotype.Component @Component @Log4j class MainRunner implements CommandLineRunner { private static Random random = new Random(); @Override void run(String... args) throws Exception { while (true) { log.log(randomLevel(), randomMessage()) sleep 3000 } } private Level randomLevel() { switch (random.nextInt(3)) { case 0: return Level.DEBUG; case 1: return Level.INFO; case 2: return Level.ERROR; default: return Level.INFO; } } private String randomMessage() { def client = new RESTClient(\u0026#39;http://api.icndb.com/jokes/\u0026#39;) def response = client.get(path: \u0026#39;random\u0026#39;) response.data.value.joke } } Build the docker image Add the plugin repository to find the Docker plugin:\nbuildscript { ext { springBootVersion = \u0026#39;1.3.0.RELEASE\u0026#39; } repositories { mavenCentral() } dependencies { classpath(\u0026#34;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\u0026#34;) classpath \u0026#34;se.transmode.gradle:gradle-docker:1.2\u0026#34; // \u0026lt;- Here  } } Apply the Docker plugin:\napply plugin: \u0026#39;docker\u0026#39; Finally, add the buildDocker task:\ntask buildDocker(type: Docker) { baseImage = \u0026#39;develar/java:latest\u0026#39; push = project.hasProperty(\u0026#39;push\u0026#39;) tag = \u0026#39;geowarin/sout-chuck-norris\u0026#39; addFile { from jar rename {\u0026#39;app.jar\u0026#39;} } entryPoint([\u0026#39;java\u0026#39;, \u0026#39;-Djava.security.egd=file:/dev/./urandom\u0026#39;, \u0026#39;-jar\u0026#39;, \u0026#39;/app.jar\u0026#39;]) // exposePort(8080) } buildDocker.dependsOn(build) With this Docker plugin, every Docker instruction is available in the Gradle build so you don\u0026rsquo;t even have to write a Dockerfile.\nIn this task, we create an image called geowarin/sout-chuck-norris (change geowarin to your user name). It will contain only the jar produced by our build, which will be renamed to app.jar. Then, the entry point of the container is simply java -jar app.jar.\nThe advantage of using an entry point instead of a CMD is that we can append command line arguments to the docker run ... command and those will be passed to our application.\nThe downside is you cannot use docker exec ... bash to attach to the container.\nWe use Develar\u0026rsquo;s java 8 image. It is built on top of Alpine and weights less than 120MB.\nYou can now run ./gradlew buildDocker to create the docker image containing our project.\nREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE geowarin/sout-chuck-norris latest 85ff1a728670 4 seconds ago 135.9 MB Publish the image to the Docker hub Create an account on the docker hub then use docker login to authenticate your client.\nYou can now run ./gradlew buildDocker -Ppush to publish your image to docker hub.\nOnce it is published, anyone can run you application. If the image is not available on their machine, it will be pulled from the docker hub.\n$\u0026gt; docker run geowarin/sout-chuck-norris Unable to find image 'geowarin/sout-chuck-norris:latest' locally latest: Pulling from geowarin/sout-chuck-norris 09ef480f93cc: Verifying Checksum a6fb0a3c9260: Download complete Pulling repository docker.io/geowarin/sout-chuck-norris 914b85281644: Pulling dependent layers 914b85281644: Download complete Status: Downloaded newer image for geowarin/sout-chuck-norris:latest . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.3.1.RELEASE) 2016-01-12 16:54:19.089 INFO 1 --- [ main] c.g.geowarin.SoutChuckNorrisApplication : Starting SoutChuckNorrisApplication on 05d1fedaba4d with PID 1 (/app.jar started by root in /) 2016-01-12 16:54:19.093 INFO 1 --- [ main] c.g.geowarin.SoutChuckNorrisApplication : No active profile set, falling back to default profiles: default 2016-01-12 16:54:19.205 INFO 1 --- [ main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@4533542a: startup date [Tue Jan 12 16:54:19 GMT 2016]; root of context hierarchy 2016-01-12 16:54:20.609 INFO 1 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2016-01-12 16:54:21.456 INFO 1 --- [ main] com.github.geowarin.MainRunner : Chuck Norris can download emails with his pick-up. Conclusion Spring boot producing runnable jars, it is fairly easy to embed them inside of a container.\nAs usual, do not hesitate to give me your feedback and to checkout the code on github!\n","permalink":"https://geowarin.com/build-a-docker-image-of-your-spring-boot-app/","summary":"With Gradle, you can create a Docker image of your Spring Boot application and ship it instead of shipping a jar","title":"Build a Docker image of your Spring Boot app"},{"content":"If you deploy a lot of micro-services with Spring Boot (or any other technology), you will have a hard time collecting and making sense of the all logs of your different applications.\nIn this article, I will show you a simple way to redirect your logs to Elastic Search with a Logback appender.\nThe demo project is available on github.\nWhile this approach requires very little configuration, the 12 factors app manifesto actually recommends logging to stdout.\nWe will see how we can leverage docker to do that in the conclusion.\nThe EFK stack A lot of people refer to the triptych Elastic Search + Logstash + Kibana as the ELK stack.\nIn this stack, Logstash is the log collector. Its role will be to redirect our logs to Elastic Search. Your app can either send its logs directly to Logstash/Fluentd as we will see in this example, or write them to a file that Logstash will regularly process.\nElastic Search is used to store and process a large amount of logs.\nWe can then use Kibana as a dashboard to analyze them:\nInstead of Logstash, we will use Fluentd, an alternative log collector which is really easy to set up.\nDocker compose to run your EFK With docker-compose, setting up the EFK stack is really straightforward:\nes: image: elasticsearch:2 # The following will store es data in your boot2docker vm volumes: - /srv/docker/es:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 kibana: image: kibana ports: - 5601:5601 links: - es:elasticsearch fluentd: build: fluent-es/ ports: - 24224:24224 links: - es:es If you are running docker inside a VM, like me on my Mac, you cannot easily use volumes to persist Elastic Search data because the owner of the directory must be elasticsearch. So above is a little trick to easily overcome this.\nTo delete this directory, connect to your boot2docker vm with docker-machine ssh default.\nThe fluentd part points to a custom docker image in which I installed the Elastic Search plugin as well as redefined the fluentd config to look like this:\n\u0026lt;source\u0026gt; type forward port 24224 bind 0.0.0.0 \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; type elasticsearch logstash_format true host \u0026quot;#{ENV['ES_PORT_9200_TCP_ADDR']}\u0026quot; # dynamically configured to use Docker's link feature port 9200 flush_interval 5s \u0026lt;/match\u0026gt; In this config, we use the environment variable that docker-compose automatically sets when we use links to find the Elastic Search host.\nConfigure logback to send logs to fluentd Add the following dependencies to you build configuration:\ncompile \u0026#39;org.fluentd:fluent-logger:0.3.2\u0026#39; compile \u0026#39;com.sndyuk:logback-more-appenders:1.1.1\u0026#39; We use logback-more-appenders, which includes a fluentd appender. It\u0026rsquo;s not available on central so you will have to add the follwing maven repo:\nrepositories { mavenCentral() maven { url 'http://sndyuk.github.com/maven' } } Here is the logback configuration:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;include resource=\u0026#34;org/springframework/boot/logging/logback/base.xml\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;FLUENTD_HOST\u0026#34; value=\u0026#34;${FLUENTD_HOST:-${DOCKER_HOST:-localhost}}\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;FLUENTD_PORT\u0026#34; value=\u0026#34;${FLUENTD_PORT:-24224}\u0026#34;/\u0026gt; \u0026lt;appender name=\u0026#34;FLUENT\u0026#34; class=\u0026#34;ch.qos.logback.more.appenders.DataFluentAppender\u0026#34;\u0026gt; \u0026lt;tag\u0026gt;dab\u0026lt;/tag\u0026gt; \u0026lt;label\u0026gt;normal\u0026lt;/label\u0026gt; \u0026lt;remoteHost\u0026gt;${FLUENTD_HOST}\u0026lt;/remoteHost\u0026gt; \u0026lt;port\u0026gt;${FLUENTD_PORT}\u0026lt;/port\u0026gt; \u0026lt;maxQueueSize\u0026gt;20\u0026lt;/maxQueueSize\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;logger name=\u0026#34;fluentd\u0026#34; level=\u0026#34;debug\u0026#34; additivity=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;CONSOLE\u0026#34; /\u0026gt; \u0026lt;appender-ref ref=\u0026#34;FILE\u0026#34; /\u0026gt; \u0026lt;appender-ref ref=\u0026#34;FLUENT\u0026#34; /\u0026gt; \u0026lt;/logger\u0026gt; \u0026lt;/configuration\u0026gt; Note that we use the FLUENTD_HOST and FLUENTD_PORT environment variables to connect to Fluentd so this can be overridden in production.\nUse docker to natively redirect logs to Fluentd Redirecting to fluentd directly is kind of cool but, the 12 factors app manifesto says we should write our logs to stdout instead.\nIf you use docker to deploy your services, you can use a native docker feature called log drivers to redirect your standard output to fluentd!\ndocker run --log-driver=fluentd --log-opt fluentd-address=192.168.2.4:24225 ubuntu echo \u0026quot;Hello world\u0026quot; See the manual for more information.\nConclusion In a cloud environment, redirecting your app\u0026rsquo;s logs to a file is not practical. Sometimes, it is not even an option (no persistent filesystem available on your host).\nElastic Search tends to become the de-facto standard logging solution for the cloud era.\nDon\u0026rsquo;t forget to checkout the project on github and tell me what you think!\n","permalink":"https://geowarin.com/spring-boot-logs-in-elastic-search-with-fluentd/","summary":"Redirect your Spring boot logs to Elastic Search with a simple logback appender","title":"Spring boot logs in Elastic Search with fluentd"},{"content":"When writing integration tests, you might have to run a third party server or middleware. Your tests should remain fast to run and you should be able to run them from your IDE.\nDocker seems a good choice for this task!\nI just published a small library that contains a JUnit rule allowing you to start Docker containers before your unit tests.\nIf that sounds of interest to you, you should give it a try and tell me what you think!\nJUnit rules JUnit rules allow us to do some sort of AOP applied to JUnit test. Within a rule you are given the handle of the test to run.\nYou can decide what to do with it. Should we skip it? Should we run it? Should we wrap it in a try catch? Should we add some behavior before or after the test?\nYou can use the @Rule annotation to run the rule before each test or the @ClassRule annotation to run it once in your test class.\nYou can have has many rules as you need in your any of your tests.\nIt is much more powerful than creating an abstract test class from which test will inherit. This is the application of the composition over inheritance principle.\nHere is an example of a JUnit rule:\nimport com.rabbitmq.client.ConnectionFactory; import org.junit.ClassRule; import org.junit.Test; import rules.RabbitContainerRule; public class RabbitIntegrationTest { @ClassRule public static RabbitContainerRule rabbitContainerRule = new RabbitContainerRule(); @Test public void testConnectsToDocker() throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(rabbitContainerRule.getDockerHost()); factory.setPort(rabbitContainerRule.getRabbitServicePort()); factory.newConnection(); } } Behind the scene Did you know that the Docker daemon is accessible via a REST API? In fact when you use the docker client, it sends HTTP requests to the daemon.\nThat also means that we can create other docker clients in any programming language. In Java, Spotify has open-sourced a great docker client.\nWe will use this library to create our JUnit rule. Here is a simplified version of what we will be doing:\npublic class DockerContainerRule extends ExternalResource { public DockerContainerRule(String imageName, String[] ports, String cmd) { dockerClient = createDockerClient(); ContainerConfig containerConfig = createContainerConfig(imageName, ports, cmd); dockerClient.pull(imageName); container = dockerClient.createContainer(containerConfig); } @Override protected void before() throws Throwable { super.before(); dockerClient.startContainer(container.id()); } @Override protected void after() { super.after(); dockerClient.killContainer(container.id()); dockerClient.removeContainer(container.id(), true); dockerClient.close(); } } Simple, isn\u0026rsquo;t it? You can check out the full code here\nThis class will allow users to create their own rules, extending this one:\npublic class RabbitContainerRule extends DockerContainerRule { public static final String RABBIT_CONTAINER_IMAGE_NAME = \u0026#34;rabbitmq:management\u0026#34;; public RabbitContainerRule() { // List the ports to open on the container.  // They will automatically be bound to random unused ports on your host  super(RABBIT_CONTAINER_IMAGE_NAME, new String[]{\u0026#34;5672\u0026#34;, \u0026#34;15672\u0026#34;}); } @Override protected void before() throws Throwable { super.before(); // wait for container to boot  waitForPort(getRabbitServicePort()); } public int getRabbitServicePort() { return getHostPort(\u0026#34;5672/tcp\u0026#34;); } public int getRabbitManagementPort() { return getHostPort(\u0026#34;15672/tcp\u0026#34;); } } Bonus There is an annoying thing with docker containers: you cannot tell if the process running inside is in a ready state and waiting for your to use it or if it is still booting.\nMost people use netcat on a specific port to wait for a container.\nIn Java, we can do the same thing with good old sockets!\npublic void waitForPort(int port, long timeoutInMillis) { SocketAddress address = new InetSocketAddress(getDockerHost(), port); long totalWait = 0; while (true) { try { SocketChannel.open(address); return; } catch (IOException e) { try { Thread.sleep(100); totalWait += 100; if (totalWait \u0026gt; timeoutInMillis) { throw new IllegalStateException(\u0026#34;Timeout while waiting for port \u0026#34; + port); } } catch (InterruptedException ie) { throw new IllegalStateException(ie); } } } } Conclusion JUnit rules are a very cool way to improve the readability and the expressiveness of our tests. Check out the system rules for a good example.\nDon\u0026rsquo;t forget to give a try to the project, which is available on github and give me your feedback.\n","permalink":"https://geowarin.com/integration-tests-with-a-docker-junit-rule/","summary":"Docker can help you write better tests with a simple JUnit rule","title":"Integration tests with a Docker JUnit Rule"},{"content":"My dear colleague Fruzenshtein asked me what I think a modern Java developer should know or learn in 2016.\nThis is a mix of techniques and tools I\u0026rsquo;ve learned and found useful the past year and the one that I wish to learn in the coming year.\nPractices Kent Beck once said he was \u0026ldquo;just a good programmer with great habits\u0026rdquo;. As professional developers, we should all strive to cultivate better habits.\nTDD Java developers are lucky to work in a very rich ecosystem with extremely good testing libraries. If you do not have the healthy habit of writing tests first, you should at least make an effort to learn the tools that will help you craft quality software.\nIn no particular order, here is a curated list of the frameworks and tools I use to write better tests:\n Spock Geb / Fluentlenium Fake Mongo AssertJ DbUnit / NoSqlUnit Junit lambda  I find that writing tests in groovy is particularly awesome, if you do not know groovy yet, give it a try this year!\nModern build and CI Maven is great, fast and well integrated with IDEs. However, I have definitely switched to gradle for all my projects.\nGradle is synonym of less boilerplate, custom tasks, polyglot and truly incremental builds.\nYou should also look at a modern Continuous integration setup. I have been using travis for my open source projects.\nAt work, I\u0026rsquo;ve been very happy to use gitlab. It has a very cool CI environment called gitlab CI, which borrows a lot of concepts from travis.\nBetter yet, deploying Gitlab with docker should take you 5 minutes!.\nIn the Continuous deployment world, Spinnaker looks very promising.\nGood tools If your are stuck with svn do yourself a favor and learn git as soon as possible.\nHere are some resources I found useful while learning git:\n Learn git in 15 minutes Learn git branching  And of course, a great IDE will change your life.\nInfrastructure: Docker and cloud The past two years have been all about docker. For a good reason in my opinion!\nI\u0026rsquo;ve had tons of fun and success using docker-compose to set up development and QA environments.\nIf you never deployed one of your pet projects to the cloud, it has never been easier. Give a try at pivotal Web Services, Heroku or Digital Ocean if you are running docker containers.\nIf you are looking to deploy Spring applications at scale, be sure to take a look at netflix OSS. With great projects like Ribbon, Eureka and Hystrix, you\u0026rsquo;ve got everything to run your own cloud!\nWeb and JS Javascript has evolved a lot this year and you should definitely look at React or follow angular 2\u0026rsquo;s progress\nThe main problem with those libraries is the setup of a good development environment. Webpack, JSPM or Browserify? How to properly setup hot reloading? How to design your build pipeline? Those question are still difficult to answer but I\u0026rsquo;m confident 2016 will solve those problems.\nIf you want to get started with Spring boot and react, have a look at my boot-react project.\nFunctional programming With lambdas in Java 8, I think the java community has begun to look more and more at the functional programming paradigms.\nPersonally, my gateway to FP has been javascript. With react and redux borrowing a lot of concepts from the elm architecture to the excellent course I am taking called \u0026ldquo;Hardcore functional programming in javascript\u0026rdquo;, it seems that this community has a lot of interest for FP.\nI will also have a look at livescript, an Haskell-like language that compiles to javascript.\nNaturally, I will also try to use those functional paradigms on the backend. I\u0026rsquo;m not a big fan of Scala yet so I think I will look more closely at kotlin, which is now in beta with top tier IDE support and compatible with spring\nConclusion What do you think of this list? Would you add something? Do not forget to check out the profile of successful Java developer in 2016  on Fruzenshtein\u0026rsquo;s notes!\n","permalink":"https://geowarin.com/the-modern-java-developer/","summary":"What to learn in 2016 to be a top notch Java dev?","title":"The modern java developer"},{"content":"When I develop web applications, I love using React. I\u0026rsquo;m also a Spring and groovy addict.\nThose two stacks make me more productive. Can we have the best of both worlds?\nI will show you step by step how I created this project. Feel free to fiddle with it and give me your feedback.\nGoal My perfect stack on the backend is to use Spring boot and groovy. With the latest version of Spring boot, there is a new tool called dev-tools that will automatically reload the embedded server when you recompile your project.\nOn the frontend, most React developers use webpack. React has awesome support for hot reloading with react-hot-loader. It will magically update your views without requiring you to refresh your browser. Because React encourages your to have a unidirectional data flow, your whole application can use hot reloading every time you save. For this to work, we have to launch a webpack dev server.\nThe problem when you launch your Spring boot server on the port 8080 and the dev server on the port 3000 is that you will get cross origin requests preventing the two servers from interacting.\nWe also want to isolate the two projects and make separate gradle modules.\nThis blog post will show a solution to this problem and will provide an enjoyable dev environment.\nThis might not be the perfect solution and I\u0026rsquo;d love any feedback from both communities to help me improve it.\nThe backend We will generate the backend. To do that, you can go on http://start.spring.io/ and create a gradle project using groovy, java 8 and the latest Spring boot (1.3.0 M2 at the time of writing).\nFor the dependencies tick DevTools and Web.\nIf you want to do it command line style just type the following in your console:\ncurl https://start.spring.io/starter.tgz \\ -d name=boot-react \\ -d bootVersion=1.3.0.M2 \\ -d dependencies=devtools,web \\ -d language=groovy \\ -d JavaVersion=1.8 \\ -d type=gradle-project \\ -d packageName=react \\ -d packaging=jar \\ -d artifactId=boot-react \\ -d baseDir=boot-react | tar -xzvf - This will create a base project with the latest spring boot, the devtools, groovy and gradle.\nDon\u0026rsquo;t forget to generate the gradle wrapper:\ngradle wrapper See the commit\nGreat so now we have tomcat embedded, hot reloading and supernatural groovy strength. The usual.\nWe will create a simple REST resource that we would like our frontend to consume:\n@RestController class SimpleResource { @RequestMapping(\u0026#39;/api/simple\u0026#39;) Map resource() { [simple: \u0026#39;resource\u0026#39;] } } The frontend As mentioned before, we want the frontend to be a separated project. We will create a gradle module for that.\nAt the root of your project add a settings.gradle file with the following content:\ninclude 'frontend' Now, create a frontend directory under the project root and add a build.gradle file in it:\nplugins { id \u0026#34;com.moowork.node\u0026#34; version \u0026#34;0.10\u0026#34; } version \u0026#39;0.0.1\u0026#39; task bundle(type: NpmTask) { args = [\u0026#39;run\u0026#39;, \u0026#39;bundle\u0026#39;] } task start(type: NpmTask) { args = [\u0026#39;start\u0026#39;] } start.dependsOn(npm_install) bundle.dependsOn(npm_install) See the commit\nWe will use the gradle node plugin to call the two main tasks in our application:\n npm run bundle will create the minified app in the dist directory npm start will start our dev server  We can call them from the gradle build with ./gradlew frontend:start and ./gradlew frontend:bundle\nThe content of the project will basically be the same as react-hot-boilerplate\nLet\u0026rsquo;s get the sources of this project as a zip file from github and unzip them into the frontend directory. With bash, type the following command at the root of your project:\nwget -qO- -O tmp.zip https://github.com/gaearon/react-hot-boilerplate/archive/master.zip \u0026amp;\u0026amp; unzip tmp.zip \u0026amp;\u0026amp; mv react-hot-boilerplate-master/* frontend \u0026amp;\u0026amp; rm -rf react-hot-boilerplate-master \u0026amp;\u0026amp; rm tmp.zip See the commit\nIf everything goes well, typing ./gradlew fronted:start, will start the react application at http://localhost:3000.\nThe first problem arises when you ctrl+c out of the gradle build, the server will still hang. You can kill it with killall node. This is a problem I\u0026rsquo;d like help solving, if you have a solution, please tell me.\nIn the rest of the article I will use npm start directly, which presupposes that you have npm available on your development machine. The whole build will only require Java.\nWe will use the webpack-html-plugin to automatically generate the index.html page.\nnpm install --save-dev html-webpack-plugin Since using the document body as a root for our application is a bad practice, we need to tweak the default html template.\nI created a file called index-template.html in a newly created assets directory. It will serve as a template to generate our index.html file:\nAs you can see, it contains a div with the id root.\nLet\u0026rsquo;s tweak the dev server a little bit to combine it with another server.\nLet\u0026rsquo;s change webpack.config.js:\nvar path = require(\u0026#39;path\u0026#39;); var webpack = require(\u0026#39;webpack\u0026#39;); var HtmlWebpackPlugin = require(\u0026#39;html-webpack-plugin\u0026#39;); module.exports = { devtool: \u0026#39;eval\u0026#39;, entry: [ \u0026#39;webpack-dev-server/client?http://localhost:3000\u0026#39;, \u0026#39;webpack/hot/only-dev-server\u0026#39;, \u0026#39;./src/index\u0026#39; ], output: { path: path.join(__dirname, \u0026#39;dist\u0026#39;), filename: \u0026#39;bundle.js\u0026#39;, publicPath: \u0026#39;http://localhost:3000/\u0026#39; }, plugins: [ new webpack.HotModuleReplacementPlugin(), new webpack.NoErrorsPlugin(), new HtmlWebpackPlugin({ title: \u0026#39;Boot React\u0026#39;, template: path.join(__dirname, \u0026#39;assets/index-template.html\u0026#39;) }) ], resolve: { extensions: [\u0026#39;\u0026#39;, \u0026#39;.js\u0026#39;] }, module: { loaders: [{ test: /\\.js$/, loaders: [\u0026#39;react-hot\u0026#39;, \u0026#39;babel\u0026#39;], include: path.join(__dirname, \u0026#39;src\u0026#39;) }] } }; We changed the publicPath to point directly at our dev server and included the HtmlWebpackPlugin.\nNow we can get rid of the old index.html and start our dev server with npm start. The index will be automatically generated for us.\nSee the commit\nInclude the frontend in the boot jar We have to create the npm bundle task, which will generate an optimized web application in the dist directory.\nIn the package.json file, update the scripts:\n\u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;node server.js\u0026quot;, \u0026quot;bundle\u0026quot;: \u0026quot;webpack --optimize-minimize --optimize-dedupe --output-public-path ''\u0026quot; } Now if you launch ./gradlew frontend:bundle, it will generate an optimized bundle.js file and the index.html in the dist directory.\nThe last step is to include this dist directory in our application\u0026rsquo;s jar as static assets. Add the following task to our main gradle build:\njar { from(\u0026#39;frontend/dist\u0026#39;) { into \u0026#39;static\u0026#39; } } processResources.dependsOn(\u0026#39;frontend:bundle\u0026#39;) If you generate your jar with ./gradlew assemble, you will see that the built jar includes the frontend resources.\nIf you run the jar (java -jar build/libs/boot-react-0.0.1-SNAPSHOT.jar), you should see the React hello world on localhost:8080\nSee the commit\nLaunch it in dev When working on our application, it would be nice if:\n Launching the spring boot server in dev launched the webpack dev server Our dev-server proxied the request to localhost:8080 so we can access the application on localhost:3000 and not get cross-origin requests  Add the following WebpackLauncher to the project:\n@Configuration @Profile(\u0026#39;dev\u0026#39;) class WebpackLauncher { @Bean WebpackRunner frontRunner() { new WebpackRunner() } class WebpackRunner implements InitializingBean { static final String WEBPACK_SERVER_PROPERTY = \u0026#39;webpack-server-loaded\u0026#39; static boolean isWindows() { System.getProperty(\u0026#39;os.name\u0026#39;).toLowerCase().contains(\u0026#39;windows\u0026#39;) } @Override void afterPropertiesSet() throws Exception { if (!System.getProperty(WEBPACK_SERVER_PROPERTY)) { startWebpackDevServer() } } private void startWebpackDevServer() { String cmd = isWindows() ? \u0026#39;cmd /c npm start\u0026#39; : \u0026#39;npm start\u0026#39; cmd.execute(null, new File(\u0026#39;frontend\u0026#39;)).consumeProcessOutput(System.out, System.err) System.setProperty(WEBPACK_SERVER_PROPERTY, \u0026#39;true\u0026#39;) } } } This will take care of the first task by launching npm start when our server starts. I used a system property to make sure the dev-tools will not reload the frontend when we make a change in the backend code. This class will be available when we start the application with the dev profile\nWe can make a simple proxy with webpack-dev-server. Change the server.js file:\nvar webpack = require(\u0026#39;webpack\u0026#39;); var WebpackDevServer = require(\u0026#39;webpack-dev-server\u0026#39;); var config = require(\u0026#39;./webpack.dev.config\u0026#39;); new WebpackDevServer(webpack(config), { publicPath: config.output.publicPath, hot: true, historyApiFallback: true, proxy: { \u0026#34;*\u0026#34;: \u0026#34;http://localhost:8080\u0026#34; } }).listen(3000, \u0026#39;localhost\u0026#39;, function (err, result) { if (err) { console.log(err); } console.log(\u0026#39;Listening at localhost:3000\u0026#39;); }); Launch your application with the --spring.profiles.active=dev flag.\nYou should be able see the react hello world on http://localhost:3000. If you make some changes to it, it will automatically reload.\nSee the old commit commit\nAnd the new commit\nFetch the resource We can check that we do not get cross-origin errors using axios, a simple library to do http requests. It supports promises and automatically handles json.\nnpm i -S axios Let\u0026rsquo;s amend our App.js:\nimport React, { Component } from \u0026#39;react\u0026#39;; import axios from \u0026#39;axios\u0026#39;; export default class App extends Component { componentDidMount() { axios.get(\u0026#39;/api/simple\u0026#39;) .then(res =\u0026gt; console.log(res.data)) .catch(err =\u0026gt; console.error(err)) } render() { return ( \u0026lt;h1\u0026gt;Hello, guys.\u0026lt;/h1\u0026gt; ); } } See the commit\nBetter optimization of the javascript assets We can further improve the compression of the javascript assets by separating our dev webpack configuration from our production configuration.\nIn the production configuration, we can use the DefinePlugin to set the NODE_ENV variable to production. This will allow webpack to automatically remove all the code intended for development purposes in our libraries:\nnew webpack.DefinePlugin({ \u0026#34;process.env\u0026#34;: { NODE_ENV: JSON.stringify(\u0026#34;production\u0026#34;) } }) See the commit\nFeedback needed Well, this works pretty well!\nWhat do you think? Care to comment and help me make something better? Your feedback is welcome!\nThe project is available on github. Pull requests and issues are gladly accepted.\n","permalink":"https://geowarin.com/spring-boot-and-react-hot-loader/","summary":"The perfect setup for Spring boot and React hot loader","title":"Spring Boot and React hot loader"},{"content":"Since version 8, java has a way better abstraction than java.util.Future called CompletableFuture. This new API along with the lambdas enables new ways of reasoning with futures by composing, listening and joining them.\nFutures are traditionally created by submitting tasks to an Executor. Spring allows declaring one or multiple executors and will submit any method annotated with @Async as tasks for those executors.\nThe big problem is that executors still return Futures and not CompletableFutures.\nWe are going to create our own Executor to solve this problem. Then we will study a solution to handle timeouts with those futures and as a bonus, do a little bit of AOP to debug our threads.\nYou can see the resulting application on my gihtub.\nCreating an Executor for CompletableFutures If you try to return a CompletableFuture from an Async method in Spring, you will get the following error:\nCaused by: java.lang.ClassCastException: java.util.concurrent.FutureTask cannot be cast to java.util.concurrent.CompletableFuture The idea is to use delegation to decorate an existing instance of ExecutorService. We will implement the ExecutorService and use type covariance to return CompletableFutures instead of Future.\nThe following code has been greatly inspired by this blog post. Many thanks to Brian Oxley!\nSo the first thing we need to do is to create a decorator for an executor service and delegate every method to that service:\nstatic class DelegatingExecutorService implements ExecutorService { protected ExecutorService delegate; public DelegatingExecutorService(ExecutorService executorService) { this.delegate = executorService; } @Override public \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task) { return delegate.submit(task); } @Override public \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Runnable task, T result) { return delegate.submit(task, result); } // Override and delegate everything } We can create an interface that will extends ExecutorService and return CompletableFutures instead of Futures:\n/** * DelegatingCompletableExecutorService {@code ExecutorService} to covariantly return {@code * CompletableFuture} in place of {@code Future}. */ public interface CompletableExecutorService extends ExecutorService { /** * @return a completable future representing pending completion of the * task, never missing */ @Override \u0026lt;T\u0026gt; CompletableFuture\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task); /** * @return a completable future representing pending completion of the * task, never missing */ @Override \u0026lt;T\u0026gt; CompletableFuture\u0026lt;T\u0026gt; submit(Runnable task, T result); /** * @return a completable future representing pending completion of the * task, never missing */ @Override CompletableFuture\u0026lt;?\u0026gt; submit(Runnable task); } We can then implement this new interface using our decorator as a base:\nstatic class DelegatingCompletableExecutorService extends DelegatingExecutorService implements CompletableExecutorService { DelegatingCompletableExecutorService(ExecutorService threads) { super(threads); } @Override public \u0026lt;T\u0026gt; CompletableFuture\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task) { final CompletableFuture\u0026lt;T\u0026gt; cf = new CompletableFuture\u0026lt;\u0026gt;(); delegate.submit(() -\u0026gt; { try { cf.complete(task.call()); } catch (CancellationException e) { cf.cancel(true); } catch (Exception e) { cf.completeExceptionally(e); } }); return cf; } @Override public \u0026lt;T\u0026gt; CompletableFuture\u0026lt;T\u0026gt; submit(Runnable task, T result) { return submit(callable(task, result)); } @Override public CompletableFuture\u0026lt;?\u0026gt; submit(Runnable task) { return submit(callable(task)); } } We also need to create an utility method to create a CompletableExecutorService:\npublic static CompletableExecutorService completable(ExecutorService delegate) { return new DelegatingCompletableExecutorService(delegate); } See this gist for the final result.\nCreating an async service To enable asynchronous methods in Spring, you will need this kind of configuration class:\n@Configuration @EnableAsync public class SpringAsyncConfig implements AsyncConfigurer { protected final Log logger = LogFactory.getLog(getClass()); @Override public Executor getAsyncExecutor() { ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(\u0026#34;async-%d\u0026#34;).build(); return CompletableExecutors.completable(Executors.newFixedThreadPool(10, threadFactory)); } @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return (ex, method, params) -\u0026gt; logger.error(\u0026#34;Uncaught async error\u0026#34;, ex); } } As you can see, we can specify which executor will handle our @Async methods.\nWe can now return CompletableFutures from our services!\n@Service public class AsyncService { private static String[] greetings = new String[]{ \u0026#34;hallo\u0026#34;, \u0026#34;hallo\u0026#34;, \u0026#34;hej\u0026#34;, \u0026#34;hej\u0026#34;, \u0026#34;bonjour\u0026#34;, \u0026#34;hola\u0026#34;, \u0026#34;ciao\u0026#34;, \u0026#34;shalom\u0026#34;, \u0026#34;f√°ilte\u0026#34;, \u0026#34;kaixo\u0026#34;, \u0026#34;konnichiwa\u0026#34;, \u0026#34;saluton\u0026#34;, \u0026#34;p√§iv√§√§\u0026#34;, \u0026#34;selamat pagi\u0026#34;, \u0026#34;gut de\u0026#34;, \u0026#34;ol√°\u0026#34; }; @Async public CompletableFuture\u0026lt;String\u0026gt; asyncGreeting() { AsyncUtil.randomSleep(3000, TimeUnit.MILLISECONDS); String result = AsyncUtil.getThreadName() + \u0026#34; - \u0026#34; + random(greetings); return CompletableFuture.completedFuture(result); } @SafeVarargs public final \u0026lt;T\u0026gt; T random(T... elements) { LinkedList\u0026lt;T\u0026gt; greetings = new LinkedList\u0026lt;\u0026gt;(Arrays.asList(elements)); Collections.shuffle(greetings, ThreadLocalRandom.current()); return greetings.getFirst(); } } Here is the AsyncUtil class in case you are wondering what\u0026rsquo;s going on:\npublic class AsyncUtil { public static void randomSleep(int duration, TimeUnit timeUnit) { try { timeUnit.sleep(ThreadLocalRandom.current().nextInt(duration)); } catch (InterruptedException e) { Throwables.propagate(e); } } public static String getThreadName() { return Thread.currentThread().getName(); } } Our service will say hello in a random language within 3 seconds of time. How do we handle the result?\npublic class Runner implements CommandLineRunner { @Autowired private AsyncService asyncService; @Override public void run(String... args) throws Exception { IntStream.rangeClosed(1, 10) .mapToObj(__ -\u0026gt; asyncService.asyncGreeting().exceptionally(Throwable::getMessage)) .forEach(this::printResult); } private void printResult(CompletableFuture\u0026lt;String\u0026gt; future) { future.thenRun(() -\u0026gt; System.out.println(future.join())); } } My what a beauty! In the above class, we create a stream of ten elements to call our async service ten times, make sure that we handle exceptions and print each result on the console.\nPlease, notice that nothing is blocking in the above code. The join() method will wait for a result but since we are calling it in a callback after completion, we get a fully asynchronous code.\nThis code will produce the following output, printing each lines at different timings:\nasync-7 - gut de async-5 - konnichiwa async-4 - hallo async-2 - hallo async-6 - saluton async-1 - f√°ilte async-9 - p√§iv√§√§ async-0 - hej async-8 - hallo async-3 - saluton Handling timeouts An interesting question with future is how to set a timeout and cancel them if they run late.\nMy solution is to create another executor like this:\nstatic class TimeOutExecutorService extends CompletableExecutors.DelegatingCompletableExecutorService { private final Duration timeout; private final ScheduledExecutorService schedulerExecutor; TimeOutExecutorService(ExecutorService delegate, Duration timeout) { super(delegate); this.timeout = timeout; schedulerExecutor = Executors.newScheduledThreadPool(1); } @Override public \u0026lt;T\u0026gt; CompletableFuture\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task) { CompletableFuture\u0026lt;T\u0026gt; cf = new CompletableFuture\u0026lt;\u0026gt;(); Future\u0026lt;?\u0026gt; future = delegate.submit(() -\u0026gt; { try { cf.complete(task.call()); } catch (CancellationException e) { cf.cancel(true); } catch (Throwable ex) { cf.completeExceptionally(ex); } }); schedulerExecutor.schedule(() -\u0026gt; { if (!cf.isDone()) { cf.completeExceptionally(new TimeoutException(\u0026#34;Timeout after \u0026#34; + timeout)); future.cancel(true); } }, timeout.toMillis(), TimeUnit.MILLISECONDS); return cf; } } This implementation was inspired by a discussion on stackoverflow.\nWe can now create a new executor as a Spring bean:\n@Bean(name = \u0026#34;timed\u0026#34;) public Executor timeoutExecutor() { ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(\u0026#34;timed-%d\u0026#34;).build(); return TimedCompletables.timed(Executors.newFixedThreadPool(10, threadFactory), Duration.ofSeconds(2)); } An use it like this:\n@Async(\u0026#34;timed\u0026#34;) public CompletableFuture\u0026lt;String\u0026gt; asyncTimeoutGreeting() { AsyncUtil.randomSleep(3000, TimeUnit.MILLISECONDS); String result = AsyncUtil.getThreadName() + \u0026#34; - \u0026#34; + random(greetings); return CompletableFuture.completedFuture(result); } Now if we run the application again, about one third of the tasks will time out:\ntimed-4 - saluton timed-3 - hallo timed-7 - saluton timed-8 - f√°ilte timed-1 - saluton timed-5 - hallo Timeout after PT2S Timeout after PT2S Timeout after PT2S Timeout after PT2S Profiling threads with AOP Let\u0026rsquo;s add a dependency to spring-boot-starter-aop to automatically profile the execution of our async methods:\n@Aspect @Component public class ServiceProfiler { @Pointcut(\u0026#34;execution(java.util.concurrent.CompletableFuture completable.service.*.*(..))\u0026#34;) public void serviceMethods() { } @Around(\u0026#34;serviceMethods()\u0026#34;) public Object profile(ProceedingJoinPoint pjp) throws Throwable { StopWatch stopWatch = new StopWatch(); stopWatch.start(); Object output = pjp.proceed(); stopWatch.stop(); if (output instanceof CompletableFuture) { CompletableFuture future = (CompletableFuture) output; String debug = String.format(\u0026#34;(%d ms)\u0026#34;, stopWatch.getTotalTimeMillis()); future.thenAccept(o -\u0026gt; System.out.println(o + \u0026#34; - \u0026#34; + debug)); } return output; } } This is a bit unnecessary but I used one of the callbacks of CompletableFuture to display the profiling message :)\nConclusion Java 8 CompletableFutures provide an awesome API to deal with async tasks. Too bad that no Executor is able to create them without a bit of code on our part.\nI\u0026rsquo;m not a concurrency expert so please tell me what you think of this solution in the comments.\n","permalink":"https://geowarin.com/completable-futures-with-spring-async/","summary":"Use Java 8 new CompletableFuture with Spring async","title":"Completable futures with Spring async"},{"content":"I must admit it after years of trying to avoid writing script shells: I\u0026rsquo;m not a big fan of bash. Sure you can do amazing things when you become a script guru but for someone who spends his life trying to write readable code, it feels a bit unnatural.\nSo it was with great pleasure and a bit of excitation that I began playing with the new kid in the shell block: fish.\nAfter a few weeks of practice, I can tell you that I love it. Here are a few tips to get you started using fish.\nInstallation The following command will install fish:\nbrew install fish To add it as an available shell, you should sudo vi /etc/shells and add the following line /usr/local/bin/fish. Now to use it as default, type:\nchsh -s /usr/local/bin/fish Configure you shell First thing you can do is to configure fish. Type:\nfish_config You will be brought to a web page where you can configure your prompt and various options of fish. Personally, I use the Classic + Git prompt which is still minimalist but will display useful information when you are inside a git repository.\nYou can see right away one of the big pros of fish: it is very fast, easy to customize and has very good defaults.\nInstall oh my fish There is one small problem with fish: it is not compatible with POSIX. This means that you cannot directly use bash commands or scripts directly in fish.\nOf course, you can invoke bash inside of fish: bash my-command. But there is a simpler solution for a handful of very handy scripts called oh-my-fish.\nOh-my-fish allows you to use plugins (some kind of functions with shell loading hooks) to easily customize your shell.\nFollow the very simple installation instructions. This will create a new fish configuration. The main configuration file in fish is located in ~/.config/fish/config.fish and it will be replaced by oh-my-fish (it will be backed up don\u0026rsquo;t worry).\nIn a nutshell, installing oh-my-fish will add the following line to your config:\n# Load oh-my-fish configuration. source $OMF_PATH/init.fish Navigate with z If you don\u0026rsquo;t know z, try it out immediately, it is guaranteed to change your life. It will allow you to navigate to the most frequent directories with fuzzy commands.\nFor instance, issuing z fun would bring me to /Users/geowarin/.configfish/functions since it is a directory I often visit.\nTo install it:\nbrew install z This will install z\u0026hellip; For bash.\nThis is where oh-my-fish comes into play. Simply install the z plugin with:\nomf install z Backward history search with re-search One of the most useful features of bash is the ability to search a term in your recent history with CTRL + R. This feature is not enabled by default but somebody wrote a little program called re-search.\nFollow the instructions to install it. You will have to git clone it, make, add it to the path, add a function to fish and finally define a keyboard shortcut to call it.\nThose are really interesting steps. To add something to the path, open ~/.config/fish/config.fish and use the set function:\nset -gx PATH $PATH ~/bin This will add ~/bin to the path, you can put re-search in there.\nTo add a function, you simply have to add files to ~/.config/fish/functions. The functions contained in the files of this directory will automatically be loaded by fish.\nFinally you can see it is very easy to bind a function to a shortcut simply by editing ~/.config/fish/functions/fish_user_key_bindings.fish\nbind \\cr re_search Define your own functions The final step to your fish initiation is to define your own functions. I might not be super fluent with bash but I was able to define my own functions when I had something repetitive to do.\nOne thing I like is to directly cd into a directory I created. A simple solution with bash is to define a function that will do something like this:\nfunction mkd() { mkdir -p \u0026quot;$@\u0026quot; \u0026amp;\u0026amp; cd \u0026quot;$@\u0026quot; } With fish, simply create a file in ~/.config/fish/functions and write:\nfunction mkd mkdir -p $argv; and cd $argv end You can see that fish syntax is actually pretty simple.\nAnother thing I like is to define a variable linking to a binary before invoking it, like this:\nfunction office set -l office /Applications/LibreOffice.app/Contents/MacOS/soffice eval $office --headless --convert-to $argv[1] --outdir (pwd) $argv[2] end This will allow you to invoke Libre Office in command line to convert a file from one format to another:\noffice docx myDoc.odt A last one, invoke a web server in the current directory and open it in the browser:\nfunction server python -m SimpleHTTPServer\u0026amp; sleep 1 open http://localhost:8000 end Working around POSIX limitation with bash -c In simple cases, you can get pretty far by calling bash scripts with bash -c.\nA tool I love is sdkman, which will manage JVM-based binaries like groovy or gradle.\nJust add the following function in fish:\nfunction sdk bash -c '. ~/.sdkman/bin/sdkman-init.sh; sdk \u0026quot;$@\u0026quot;' sdk $argv end I also wanted the current versions of the binaries managed by skdman to be in my path so I added the following to my config.fish:\n# sdkman set PATH $PATH (find ~/.sdkman/*/current/bin -maxdepth 0) Working around POSIX limitation with bass In most cases, you will find good plugins compatible with oh-my-fish. If it is not the case, I have found bass to be incredibly useful.\nIt is a simple python wrapper that will call scripts in bash and pass in and out environment variables.\nSimply git clone the project and use make to install it.\nI have used it successfully to make nvm compatible with fish.\nFor nvm, I added the following function:\nfunction nvm bass source (brew --prefix nvm)/nvm.sh ';' nvm $argv end Here you go! I hope this will help you get started with fish.\n","permalink":"https://geowarin.com/the-missing-fish-shell-tutorial/","summary":"Fish is an awesome shell but requires a bit of practice. Here are a few tips I wish people gave me when I started using it.","title":"The missing fish shell tutorial"},{"content":"Spring boot is an opinionated library that allows to create executable Spring applications with a convention over configuration approach.\nThe magic behind this framework lies in the @EnableAutoConfiguration annotation, which will automatically load all the beans the application requires depending on what Spring Boot finds in the classpath.\nThe @Enable* annotations The @Enable... annotations are not new, they were first introduced in Spring 3 when the idea of replacing the XML files with java annotated classes is born.\nA lot of Spring users already know @EnableTransactionManagement, which will enable declarative transaction management, @EnableWebMvc, which enables Spring MVC, or @EnableScheduling, which will initialize a scheduler.\nThese annotations are in fact a simple configuration import with the @Import annotation.\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Import({ EnableAutoConfigurationImportSelector.class, AutoConfigurationPackages.Registrar.class }) public @interface EnableAutoConfiguration { /** * Exclude specific auto-configuration classes such that they will never be applied. */ Class\u0026lt;?\u0026gt;[] exclude() default {}; } The EnableAutoConfigurationImportSelector uses SpringFactoriesLoader#loadFactoryNames of Spring core. SpringFactoriesLoader will look for jars containing a file with the path META-INF/spring.factories.\nWhen it finds such a file, the SpringFactoriesLoader will look for the property named after our configuration file. In our case, org.springframework.boot.autoconfigure.EnableAutoConfiguration.\nLet\u0026rsquo;s take a look at the spring-boot-autoconfigure jar, which indeed contains a spring.factories file copied below:\n# Initializers org.springframework.context.ApplicationContextInitializer=\\ org.springframework.boot.autoconfigure.logging.AutoConfigurationReportLoggingInitializer # Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\ org.springframework.boot.autoconfigure.MessageSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfiguration,\\ org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.JpaRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.MongoRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.redis.RedisAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\ org.springframework.boot.autoconfigure.jms.JmsTemplateAutoConfiguration,\\ org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.mobile.DeviceResolverAutoConfiguration,\\ org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\ org.springframework.boot.autoconfigure.mongo.MongoTemplateAutoConfiguration,\\ org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\ org.springframework.boot.autoconfigure.reactor.ReactorAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.SecurityAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration,\\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.DispatcherServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.MultipartAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.HttpMessageConvertersAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.websocket.WebSocketAutoConfiguration In this file, we can see a list of the Spring Boot auto-configurations. Let\u0026rsquo;s take a closer look at one of those configurations, MongoAutoConfiguration for instance:\n@Configuration @ConditionalOnClass(Mongo.class) @EnableConfigurationProperties(MongoProperties.class) public class MongoAutoConfiguration { @Autowired private MongoProperties properties; private Mongo mongo; @PreDestroy public void close() throws UnknownHostException { if (this.mongo != null) { this.mongo.close(); } } @Bean @ConditionalOnMissingBean public Mongo mongo() throws UnknownHostException { this.mongo = this.properties.createMongoClient(); return this.mongo; } } This simple Spring configuration class declares typical beans needed to use mongoDb.\nThis classes, like a lot of others in Spring Boot relies heavily on Spring annotations:\n @ConditionOnClass activates a configuration only if one or several classes are present on the classpath @EnableConfigurationProperties automatically maps a POJO to a set of properties in the Spring Boot configuration file (by default application.properties) @ConditionalOnMissingBean enables a bean definition only if the bean wasn\u0026rsquo;t previously defined  You can also refine the order in which those configuration classes load with @AutoConfigureBefore et @AutoConfigureAfter.\nProperties Mapping Let\u0026rsquo;s look at MongoProperties, which is a classic example of Spring Boot properties mapping:\n@ConfigurationProperties(prefix = \u0026#34;spring.data.mongodb\u0026#34;) public class MongoProperties { private String host; private int port = DBPort.PORT; private String uri = \u0026#34;mongodb://localhost/test\u0026#34;; private String database; // ... getters/ setters omitted } The @ConfigurationProperties will associate every properties with a particular prefix to the POJO. For instance, the property spring.data.mongodb.port will be mapped to the port attribute of this class.\nIf you\u0026rsquo;re a Spring Boot user, I strongly encourage you to use those capabilities to remove the boiler plate code associated with configuration properties.\nThe @Conditional annotations The power of Spring Boot lies in one of Spring 4 new features: the @Conditional annotations, which will enable some configuration only if a specific condition is met.\nA sneak peek in the org.springframework.boot.autoconfigure.condition package in Spring Boot will give us an overview of what we can do with those annotations:\n @ConditionalOnBean @ConditionalOnClass @ConditionalOnExpression @ConditionalOnMissingBean @ConditionalOnMissingClass @ConditionalOnNotWebApplication @ConditionalOnResource @ConditionalOnWebApplication  Let\u0026rsquo;s take a closer look at @ConditionalOnExpression, which allows you to write a condition in the Spring Expression language.\n@Conditional(OnExpressionCondition.class) @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.TYPE, ElementType.METHOD }) public @interface ConditionalOnExpression { /** * The SpEL expression to evaluate. Expression should return {@code true} if the * condition passes or {@code false} if it fails. */ String value() default \u0026#34;true\u0026#34;; } In this class, we indeed make use of the @Conditional annotation. The condition is defined in the OnExpressionCondition class:\npublic class OnExpressionCondition extends SpringBootCondition { @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) { // ...  // we first get a handle on the EL context via the ConditionContext  boolean result = (Boolean) resolver.evaluate(expression, expressionContext); // ...  // here we create a message the user will see when debugging  return new ConditionOutcome(result, message.toString()); } } In the end, the @Conditional are resolved to simple booleans, via the ConditionOutcome.isMatch method.\nThe ApplicationContextInitializers The second possibility that the spring.factories file offers, is to define application initializers. They allow us to manipulate Spring\u0026rsquo;s applicationContext before the application is loaded.\nIn particular, they can create listeners on the context thanks to the ConfigurableApplicationContext#addApplicationListener method.\nSpring Boot does that in the AutoConfigurationReportLoggingInitializer which listens to system events, like context refresh or the application\u0026rsquo;s failure to start. This will help create the auto-configuration report when you start your application in debug mode.\nYou can start your application in debug mode with either the -Ddebug flag or add the property debug=true to application.properties.\nDebug Spring Boot Auto-Configuration The documentation gives us some advice to understand what happened during the auto-configuration.\nWhen launched in debug mode, Spring Boot will generate a report that looks like this one:\nPositive matches: ----------------- MessageSourceAutoConfiguration - @ConditionalOnMissingBean (types: org.springframework.context.MessageSource; SearchStrategy: all) found no beans (OnBeanCondition) JmxAutoConfiguration - @ConditionalOnClass classes found: org.springframework.jmx.export.MBeanExporter (OnClassCondition) - SpEL expression on org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration: ${spring.jmx.enabled:true} (OnExpressionCondition) - @ConditionalOnMissingBean (types: org.springframework.jmx.export.MBeanExporter; SearchStrategy: all) found no beans (OnBeanCondition) DispatcherServletAutoConfiguration - found web application StandardServletEnvironment (OnWebApplicationCondition) - @ConditionalOnClass classes found: org.springframework.web.servlet.DispatcherServlet (OnClassCondition) Negative matches: ----------------- DataSourceAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType (OnClassCondition) DataSourceTransactionManagerAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.jdbc.core.JdbcTemplate,org.springframework.transaction.PlatformTransactionManager (OnClassCondition) MongoAutoConfiguration - required @ConditionalOnClass classes not found: com.mongodb.Mongo (OnClassCondition) FallbackWebSecurityAutoConfiguration - SpEL expression on org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration: !${security.basic.enabled:true} (OnExpressionCondition) SecurityAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.security.authentication.AuthenticationManager (OnClassCondition) EmbeddedServletContainerAutoConfiguration.EmbeddedJetty - required @ConditionalOnClass classes not found: org.eclipse.jetty.server.Server,org.eclipse.jetty.util.Loader (OnClassCondition) WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#localeResolver - @ConditionalOnMissingBean (types: org.springframework.web.servlet.LocaleResolver; SearchStrategy: all) found no beans (OnBeanCondition) - SpEL expression: '${spring.mvc.locale:}' != '' (OnExpressionCondition) WebSocketAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.web.socket.WebSocketHandler,org.apache.tomcat.websocket.server.WsSci (OnClassCondition) For each auto-configuration, we can see why it was initiated or why it failed.\nConclusion Spring Boot\u0026rsquo;s approach leverages the possibilities of Spring 4 and allows to create an auto-configured executable jar.\nDon\u0026rsquo;t forget that, as the documentation states, you can gradually replace the auto-configuration by declaring your own beans.\nWhat I love about Spring Boot is that it allows you to prototype an application very quickly but also to learn with its source. Auto-configurations are neat pieces of code that can teach you a thing or two about Spring.\nAs Josh Long, developer advocate at Pivotal, said:\n","permalink":"https://geowarin.com/understanding-spring-boot/","summary":"Wonder how spring boot\u0026rsquo;s magic operates behind the scenes? You\u0026rsquo;ve come to the right place!","title":"Understanding Spring Boot"},{"content":"Today, I\u0026rsquo;ll be reviewing the first book ever written on Spring Boot, Learning Spring Boot, by Greg L. Turnquist. Packt Publishing, the editor, contacted me to review it during its writing but I\u0026rsquo;ll remain as unbiased as possible.\nIt\u0026rsquo;s a good book, well worth reading if you want to learn how spring boot works but more globally, it will give you good insights and tips on the capabilities of Spring and its integration with other technologies.\nSummary The book has five chapters:\n Quick Start with Groovy Quick Start with java Debugging and Managing Your App Data Access with Spring boot Securing your App with Spring Boot  Each chapter is one big tutorial that you can follow along by coding. I guess you will be better off with the e-book version for copy-paste even if the entire code is available on github.\nIt will be a better experience for mac users, as the author gives some instructions on how to install the dependencies (like Active MQ) with brew. I guess you can achieve the same results with a small effort on other platforms as well.\nThe topics Quick Start with Groovy In the first chapter you will get started fast, using Spring Boot\u0026rsquo;s CLI and groovy. I dig groovy so it\u0026rsquo;s a nice start.\nYou will get some basic notions of testing with spock, manage javascript dependencies with WebJars and bower and learn how to use CRaSH to consult and monitor your app through a remote shell which is amazing.\nQuick Start with java In this chapter, you will build a classic java application with Spring Boot. But you\u0026rsquo;ll also use the Spring Social Github, consume its API and leverage Spring Mobile and jQuery mobile to develop a simple application working on mobile devices.\nDebugging and Managing Your App This chapter is about JMS integration with either an in memory broker or with ActiveMQ. You will also add health checks, custom CRaSH commands and connect to your app with JMX to monitor your queue.\nData Access with Spring Boot In this chapter, you will use Spring Data and Spring Data Rest with either H2 and MySQL or Mongo and produce a RESTful, discoverable json API. You will use profiles to use different configuration in development and in production.\nSecuring Your App with Spring Boot In this chapter you will get dig into Spring Security with basic http auth, in memory authentication and create a real in database security model. You will also learn how to configure your Tomcat to be more secure using SSL.\nConclusion Whatever are your current skills with Spring, you will undoubtedly learn some new things reading the book. I did and I enjoyed the experience. The topics addressed by Greg L. Turnquist are diverse and interesting and the book is easy to follow.\nIf you work with Spring often, you have to understand how Spring Boot works. It\u0026rsquo;s an amazing tool for fast prototyping and a wonderful way to dig deeper into the framework by small increments.\n","permalink":"https://geowarin.com/review-of-learning-spring-boot/","summary":"Review of the book Learning Spring Boot by Greg L. Turnquist","title":"Review of Learning Spring Boot"},{"content":"Good news everyone!\nAfter reviewing the awesome Learning Spring Boot by Greg Lee Turnquist, Packt Publishing asked me to write a book of my own on Spring MVC 4.\nOf course I said! I just signed the contract and I\u0026rsquo;m getting ready for five months of intense writing with a publishing date somewhere around the last quarter of 2015.\nI\u0026rsquo;m glad to seize this opportunity to answer the questions you guys have been asking me after the publication of [my article on spring MVC]({% post_url 2013-01-23-complete-example-of-a-spring-mvc-3-2-project %}) last year.\nHere is a peek of the topics I will cover in the book:\nMaster Spring MVC 4\n Setting up a Spring web application in no time Mastering the MVC Architecture Handling forms and complex URL mapping Crafting a RESTful application Leaving nothing to luck: Unit tests and Acceptance Tests Securing your application Optimizing your requests Deploying to the cloud Beyond Spring Web  I hope that you like the outline and that the book will help you achieve ultimate mastery of Spring MVC!\nI created a section on the blog where you can follow my progress.\n","permalink":"https://geowarin.com/book/writing-a-book-on-spring-mvc-4.html","summary":"Good news everyone!\nAfter reviewing the awesome Learning Spring Boot by Greg Lee Turnquist, Packt Publishing asked me to write a book of my own on Spring MVC 4.\nOf course I said! I just signed the contract and I\u0026rsquo;m getting ready for five months of intense writing with a publishing date somewhere around the last quarter of 2015.\nI\u0026rsquo;m glad to seize this opportunity to answer the questions you guys have been asking me after the publication of [my article on spring MVC]({% post_url 2013-01-23-complete-example-of-a-spring-mvc-3-2-project %}) last year.","title":"Writing a book on Spring MVC 4"},{"content":"Spring boot RC1 is available.\nUpdate : RC3 released I updated the project.\nIt takes spring development and fast prototyping to a whole new level by taking care of all the dependencies for you, auto-detecting your configuration, providing an executable jar (great for deploying in the cloud), and much more.\nIn this post we\u0026rsquo;ll see how to integrate spring-boot with jersey, including testing of Jersey controllers with jersey-test.\nOf course, the source code is available on my github.\nSetting up spring-boot Spring boot aims towards simplicity and convention over configuration. First step is to include the necessary configuration in your pom.xml :\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0.RC3\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;spring-milestones\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Spring Milestones\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://repo.spring.io/milestone\u0026lt;/url\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; One of the good ideas of spring boot is to provide all the \u0026lsquo;boilerplate\u0026rsquo; configuration for you by letting you inherit their parent configuration.\nThen, you\u0026rsquo;ll select a starter, in this case, we are going to develop a web application, so starter-web is fine.\nNow, we\u0026rsquo;ll create a main function for our application :\n@EnableAutoConfiguration public class Application { public static void main(String[] args) throws Exception { new SpringApplicationBuilder(Application.class) .showBanner(false) .run(args); } } We will just add an index.html file in the webapp directory and we should be ok. With this configuration, you can run the main function and you\u0026rsquo;ll see your index file.\nAwesome.\nRunnable jar Spring boot allows you to package your application as a runnable jar. Include the following in your pom.xml :\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;pluginRepositories\u0026gt; \u0026lt;pluginRepository\u0026gt; \u0026lt;id\u0026gt;spring-milestones\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://repo.spring.io/milestone\u0026lt;/url\u0026gt; \u0026lt;/pluginRepository\u0026gt; \u0026lt;/pluginRepositories\u0026gt; With this, when running mvn package, you will generate the runnable jar. Just java -jar it to launch an embedded Tomcat containing your webapp!\nIntegrating jersey Jersey has a spring support project jersey-spring-3. Despite what its name suggests, the project is (still?) compatible with spring 4.0 so we\u0026rsquo;ll use it.\nIt basically allows you to inject spring beans in your jersey controllers.\nTo complete our configuration we\u0026rsquo;ll add the jersey servlet to our application together with a small class to configure it.\nIn the Application :\n@Bean public ServletRegistrationBean jerseyServlet() { ServletRegistrationBean registration = new ServletRegistrationBean(new ServletContainer(), \u0026#34;/rest/*\u0026#34;); // our rest resources will be available in the path /rest/*  registration.addInitParameter(ServletProperties.JAXRS_APPLICATION_CLASS, JerseyConfig.class.getName()); return registration; } We also need to add the @ComponentScan annotation to find our spring services and components (including jersey)\nNext, we\u0026rsquo;ll create the JerseyConfig class :\npublic class JerseyConfig extends ResourceConfig { public JerseyConfig() { register(RequestContextFilter.class); packages(\u0026#34;com.geowarin.rest\u0026#34;); register(LoggingFilter.class); } } Here we are providing the package(s) in which our rest resources are.\nSpeaking about our rest resources, we\u0026rsquo;ll create a simple one :\n@Path(\u0026#34;/\u0026#34;) @Component public class RestResource { @GET @Produces(MediaType.APPLICATION_JSON) @Path(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;Hello World\u0026#34;; } } There you have it : the dreadful hello world !\nIn the complete example, I show you how to generate JSON from a domain class.\nBasically all you have to do is provide classes with the @XmlRootElement annotation, add the getters and setters for the properties you want serialized and don\u0026rsquo;t forget to provide a default constructor (see here).\nTo show that dependency injection works, we\u0026rsquo;ll add a simple service :\n@Singleton @Service public class MessageService { List\u0026lt;Message\u0026gt; messages = Collections.synchronizedList(new ArrayList\u0026lt;Message\u0026gt;()); @PostConstruct public void init() { messages.add(new Message(\u0026#34;Joe\u0026#34;, \u0026#34;Hello\u0026#34;)); messages.add(new Message(\u0026#34;Jane\u0026#34;, \u0026#34;Spring boot is cool !\u0026#34;)); } public List\u0026lt;Message\u0026gt; getMessages() { return messages; } } We can now autowire it to our Jersey controller!\n@Path(\u0026#34;/\u0026#34;) @Component public class RestResource { @Autowired private MessageService messageService; @GET @Produces(MediaType.APPLICATION_JSON) @Path(\u0026#34;/messages\u0026#34;) public List\u0026lt;Message\u0026gt; message() { return messageService.getMessages(); } } Moxy will automatically convert the returned result to JSON.\nTesting Real programmers do tests. We want to test our controller right? There is a framework for that: jersey-test.\nThe Problem? it does not (yet) support annotated configuration.\nUpdate : I submitted a pull request which has been accepted by Jersey. I updated the project to use the 2.6 snapshot release of jersey which includes the modified SpringComponentProvider.\nNow the test :\npublic class RestResourceTest extends JerseyTest { @Override protected Application configure() { ApplicationContext context = new AnnotationConfigApplicationContext(TestConfig.class); return new JerseyConfig() .property(\u0026#34;contextConfig\u0026#34;, context); } @Test public void testHello() { final String hello = target(\u0026#34;hello\u0026#34;).request().get(String.class); assertThat(hello).isEqualTo(\u0026#34;Hello World\u0026#34;); } @Test public void testMessages() throws JSONException { final String messages = target(\u0026#34;messages\u0026#34;).request().get(String.class); String expected = \u0026#34;[ \u0026#34; + \u0026#34;{ \u0026#39;author\u0026#39;: \u0026#39;Joe\u0026#39;, \u0026#39;contents\u0026#39;: \u0026#39;Hello\u0026#39;},\u0026#34; + \u0026#34;{ \u0026#39;author\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;contents\u0026#39;: \u0026#39;Spring boot is cool !\u0026#39;}\u0026#34; + \u0026#34;]\u0026#34;; JSONAssert.assertEquals(expected, messages, JSONCompareMode.LENIENT); } } Jersey Test will automatically select a provider from your classpath, in the example I\u0026rsquo;m using the in memory provider which I believe to be the fastest but you can also use grizzly and others instead.\nI\u0026rsquo;m using JSONassert to test json results.\nIn the example, we are providing a simple, lighter TestConfig :\n@Configuration @ComponentScan(basePackageClasses = RestResource.class) public class TestConfig { } Conclusion Testing with Jersey Test is fast and intuitive.\nSpring boot is a nice addition to the spring ecosystem. Now that everything should be accessible from the cloud, so should be spring webapps !\n","permalink":"https://geowarin.com/a-simple-spring-boot-and-jersey-application/","summary":"How to setup a simple spring-boot and jersey application","title":"A simple Spring Boot and Jersey Application"},{"content":"I had fun today at work when I had to design a program that allows users to select nodes with xPath but keep the same hierarchical structure as the original xml file.\nThe result with tests is available on github.\nFor instance, the following xPath /persons/person[@age \u0026gt; 18]/project[@language = 'java'], would select the lines highlighted in the file below :\nNormal xPath result would be a list of nodes like that\n\u0026lt;project name=\u0026#39;dom4j\u0026#39; language=\u0026#39;java\u0026#39;/\u0026gt; \u0026lt;project name=\u0026#39;dom4j\u0026#39; language=\u0026#39;java\u0026#39;/\u0026gt; But the desired output should look like this :\n\u0026lt;persons\u0026gt; \u0026lt;person name=\u0026#39;Joe\u0026#39; age=\u0026#39;26\u0026#39;\u0026gt; \u0026lt;project name=\u0026#39;dom4j\u0026#39; language=\u0026#39;java\u0026#39;/\u0026gt; \u0026lt;/person\u0026gt; \u0026lt;person name=\u0026#39;Jane\u0026#39; age=\u0026#39;23\u0026#39;\u0026gt; \u0026lt;project name=\u0026#39;dom4j\u0026#39; language=\u0026#39;java\u0026#39;/\u0026gt; \u0026lt;/person\u0026gt; \u0026lt;/persons\u0026gt; So I decided to write a small class to handle this use case :\npublic class XPathFilter { private final Document xmlDocument; public XPathFilter(String xml) { xmlDocument = readXml(xml); } public String filter(String xPath) { Element root = xmlDocument.getRootElement(); List\u0026lt;Element\u0026gt; resultNodes = root.selectNodes(xPath); if (resultNodes.isEmpty()) { throw new IllegalStateException(\u0026#34;No result found for xpath \u0026#34; + xPath); } deleteNonResultNodes(resultNodes); return write(root); } private void deleteNonResultNodes(List\u0026lt;Element\u0026gt; resultNodes) { Set\u0026lt;Element\u0026gt; nodesToKeep = new HashSet\u0026lt;\u0026gt;(resultNodes); Set\u0026lt;Element\u0026gt; parents; do { parents = getParentNodes(nodesToKeep); for (Element parent : parents) { List\u0026lt;Element\u0026gt; children = parent.elements(); for (Element child : children) { if (!nodesToKeep.contains(child)) { parent.remove(child); } } } nodesToKeep = new HashSet\u0026lt;\u0026gt;(parents); } while (!parents.isEmpty()); } private Set\u0026lt;Element\u0026gt; getParentNodes(Collection\u0026lt;Element\u0026gt; nodes) { Set\u0026lt;Element\u0026gt; parents = new HashSet\u0026lt;\u0026gt;(); for (Element node : nodes) { Element parent = node.getParent(); if (parent != null) { parents.add(parent); } } return parents; } private Document readXml(String xml) { Document document; try (StringReader reader = new StringReader(xml)) { DocumentFactory factory = new DocumentFactory(); SAXReader saxReader = new SAXReader(); saxReader.setDocumentFactory(factory); document = saxReader.read(reader); } catch (DocumentException e) { throw new IllegalArgumentException(e); } return document; } private String write(Element rootElement) { Document documentOut = DocumentHelper.createDocument(); documentOut.add((Element) rootElement.clone()); StringWriter writer = new StringWriter(); XMLWriter xmlWriter = new XMLWriter(writer, OutputFormat.createPrettyPrint()); try { xmlWriter.write(documentOut); } catch (IOException e) { throw new IllegalStateException(e); } finally { try { xmlWriter.close(); } catch (IOException ignored) { } } return writer.toString(); } } The readXml and write methods are just standard dom4j stuff.\nThe real code lies in the deleteNonResultNodes function which traverses XML nodes from the results to the root. On each level, we will look at the parents of the current nodes (the selected ones at first) and delete every child which is not a result.\nThe parents will become the current nodes and will keep on until we reach the root. At this point we would have kept only the xml structure that actually wrap our xPath results.\nThat\u0026rsquo;s all folks ! Check out the result on github, unit tests included.\n","permalink":"https://geowarin.com/fun-with-xpath/","summary":"How to select nodes with xPath and keep the file structure of the original document in java with dom4j","title":"Fun with xpath"},{"content":"You want to get started with Spring MVC 3.2 with a complete XML-less configuration? Have a cool simple project with a lot of the nice-to-have features?\n A templating framework (we will use SiteMesh for this example - I think it is one of the simplest, most powerful frameworks out there) Localized and custom text and validation messages with reloadable bundles in development UTF-8 encoding filter for your user inputs Use the twitter boostrap for a responsive, slick design Unit tests of your controllers using spring-test-mvc Be able to run it with embedded tomcat or jetty maven plugins?  Then you can directly git clone this project : https://github.com/geowarin/spring-mvc-examples/tree/master/mvc-base\nThis article will explain how this can be done with 5 classes and 1 jsp.\nThe configuration with spring 3.2 and servlet 3.0 Since spring 3.1, it is possible to run spring MVC without a web.xml if you are in a servlet 3.0 environment. But spring 3.2 takes things a little bit further by providing a set of abstract classes to enable a very easy configuration. Check this :\npublic class WebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer { @Override protected Class\u0026lt;?\u0026gt;[] getRootConfigClasses() { return null; } @Override protected Class\u0026lt;?\u0026gt;[] getServletConfigClasses() { return new Class\u0026lt;?\u0026gt;[] { WebConfig.class }; } @Override protected String[] getServletMappings() { return new String[] { \u0026#34;/\u0026#34; }; } @Override protected Filter[] getServletFilters() { CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(\u0026#34;UTF-8\u0026#34;); return new Filter[] { characterEncodingFilter, new SiteMeshFilter()}; } } The filters are not mandatory, it just demonstrates how to add them to this configuration (site mesh requires a small xml file to point to a template - it won\u0026rsquo;t be covered by this article but check out the documentation or have a look at this project on github).\nThe UTF-8 filter will prevent encoding problems with your user inputs.\nSpring MVC also requires a WebConfig class. This is the minimal one :\n@Configuration @EnableWebMvc @ComponentScan(basePackages = { \u0026#34;com.geowarin.mvc.base.controller\u0026#34; }) public class WebConfig extends WebMvcConfigurerAdapter { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026#34;/resources/**\u0026#34;).addResourceLocations(\u0026#34;/resources/\u0026#34;); } @Bean public ViewResolver viewResolver() { InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setViewClass(JstlView.class); viewResolver.setPrefix(\u0026#34;/WEB-INF/views\u0026#34;); viewResolver.setSuffix(\u0026#34;.jsp\u0026#34;); return viewResolver; } } The @ComponentScan annotation will indicate the package in which our controllers are found. The ViewResolver bean will indicate both where our views can be found and what their extension is.\nIn this example for instance we will just have a simple view in /WEB-INF/views/home.jsp. The ResourceHandler indicates where our static resources can be found (css, js, images, etc.).\nHere is our controller :\n@Controller public class HomeController { @RequestMapping(value = \u0026#34;/\u0026#34;, method = RequestMethod.GET) public String displayHome(Model model) { return \u0026#34;/home\u0026#34;; } } At this point, you can write \u0026ldquo;hello\u0026rdquo; in your home.jsp, launch a tomcat and enjoy our 3 classes spring MVC hello world. No web.xml, nothing else.\nInterceptors, locales, messages Let me just show you the full configuration for our project :\n@Configuration @EnableWebMvc @ComponentScan(basePackages = { \u0026#34;com.geowarin.mvc.base.controller\u0026#34; }) public class WebConfig extends WebMvcConfigurerAdapter { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026#34;/resources/**\u0026#34;).addResourceLocations(\u0026#34;/resources/\u0026#34;); } @Override public void addInterceptors(InterceptorRegistry registry) { LocaleChangeInterceptor localeChangeInterceptor = new LocaleChangeInterceptor(); localeChangeInterceptor.setParamName(\u0026#34;lang\u0026#34;); registry.addInterceptor(localeChangeInterceptor); } @Bean public LocaleResolver localeResolver() { CookieLocaleResolver cookieLocaleResolver = new CookieLocaleResolver(); cookieLocaleResolver.setDefaultLocale(StringUtils.parseLocaleString(\u0026#34;en\u0026#34;)); return cookieLocaleResolver; } @Bean public ViewResolver viewResolver() { InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setViewClass(JstlView.class); viewResolver.setPrefix(\u0026#34;/WEB-INF/views\u0026#34;); viewResolver.setSuffix(\u0026#34;.jsp\u0026#34;); return viewResolver; } @Bean public MessageSource messageSource() { ReloadableResourceBundleMessageSource messageSource = new ReloadableResourceBundleMessageSource(); messageSource.setBasenames(\u0026#34;classpath:messages/messages\u0026#34;, \u0026#34;classpath:messages/validation\u0026#34;); // if true, the key of the message will be displayed if the key is not \t// found, instead of throwing a NoSuchMessageException \tmessageSource.setUseCodeAsDefaultMessage(true); messageSource.setDefaultEncoding(\u0026#34;UTF-8\u0026#34;); // # -1 : never reload, 0 always reload \tmessageSource.setCacheSeconds(0); return messageSource; } } The localeInterceptor will provide a way to switch the language in any page just by passing the lang=\u0026lsquo;en\u0026rsquo;, lang=\u0026lsquo;fr\u0026rsquo;, and so on to your url. The localeResolver will work with a simple cookie to memorize the user preference (you don\u0026rsquo;t want to pass the lang argument through your whole site, do you?).\nWith the messageSource, you will get access to properties bundle usable in your web pages. Here it is configured to be developper friendly (always reload, no error).\nThese bundled can be localized. You can have as many as you want :\n messages_en.properties for english language text message_fr.properties for french message_cn.properties, etc.  A form, a DTO, some validation Next we will show a very simple usage of a form to demonstrate the localized, custom validation messages.\nIn your home.jsp, write this code :\n\u0026lt;form:form id=\u0026#34;form\u0026#34; method=\u0026#34;post\u0026#34; modelAttribute=\u0026#34;formDTO\u0026#34;\u0026gt; \u0026lt;form:input path=\u0026#34;messageFromUser\u0026#34; /\u0026gt; \u0026lt;form:errors path=\u0026#34;messageFromUser\u0026#34; cssClass=\u0026#34;errorMessage\u0026#34; element=\u0026#34;div\u0026#34; /\u0026gt; \u0026lt;c:if test=\u0026#34;${not empty message}\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;message\u0026#34; class=\u0026#34;alert alert-success\u0026#34;\u0026gt; \u0026lt;spring:message code=\u0026#34;message.youWrote\u0026#34; arguments=\u0026#34;${message}\u0026#34; htmlEscape=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/c:if\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34; class=\u0026#34;btn\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/form:form\u0026gt; This will bind your form to a model attribute called formDTO, we will see it in the controller shortly. You could also provide an action attribute but we will just map the action to \u0026quot;/\u0026quot;, just like our jsp.\nThen the input will be bound to the messageFromUser attribute of the formDTO. You can also see the associated error message if validation fails.\nLastly, if a success message is present in the request (the controller will place it), we will display it with a localized text taking an argument. We also want to escape the text given by the user to prevent XSS injection.\nThe message bundle for this example would look like this :\nmessage.youWrote=You wrote : {0} Our new controller :\n@Controller public class HomeController { @RequestMapping(value = \u0026#34;/\u0026#34;, method = RequestMethod.GET) public String displayHome(Model model) { return \u0026#34;/home\u0026#34;; } @ModelAttribute(\u0026#34;formDTO\u0026#34;) public FormDTO createFormBean() { return new FormDTO(); } @RequestMapping(value = \u0026#34;/\u0026#34;, method=RequestMethod.POST) public String submitMessage(@Valid FormDTO formDTO, BindingResult result, SessionStatus sessionStatus, RedirectAttributes redirectAttrs) { if (result.hasErrors()) { return \u0026#34;/home\u0026#34;; } String message = formDTO.toString(); sessionStatus.setComplete(); redirectAttrs.addFlashAttribute(\u0026#34;message\u0026#34;, message); return \u0026#34;redirect:/\u0026#34;; } } You can see we expose our FormDTO to the Model. Then we process this action of posting on \u0026ldquo;/\u0026rdquo;. With spring MVC request mapping you can inject whatever is relevant to the context of your page. Here we will ask Spring MVC to give us the form posted with indication on its correctness as far as validation rules are concerned (we will see that below).\nWe also want some other small things : access to redirect attributes to display a single time (flash) message, access to the session to dispose our form, etc.\nIf the user input is correct, we will redirect him (yes with spring MVC you have some PRG for free) to the home. You can also use the instruction \u0026lsquo;forward:url\u0026rsquo;.\nI strongly advise you to check out the documentation to learn what can be injected in your controllers.\nOur FormDTO is a simple POJO, annotated with hibernate-validator annotation :\npublic class FormDTO { @NotEmpty private String messageFromUser; // Getters and setters omitted } Hibernate validator provides a lot of useful annotations like @Min, @Max, @Email. You can even stack them or create your own rules.\nThat\u0026rsquo;s it ! To customize validation message, just write properties with the same name as the annotations :\n# This will override validation messages caused by @NotEmpty annotation NotEmpty=This cannot be empty ! # This will override @NotEmpty validation messages with a path of messageFromUser NotEmpty.messageFromUser=Don't you have anything to say? Wait ! How do we test a controller? With spring mvc test ! Have a look :\nimport static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.*; import static org.springframework.test.web.servlet.result.MockMvcResultHandlers.*; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.*; @RunWith(SpringJUnit4ClassRunner.class) @WebAppConfiguration @ContextConfiguration(classes = WebConfig.class) public class HomeControllerTest { @Autowired private WebApplicationContext wac; private MockMvc mockMvc; @Before public void setup() { this.mockMvc = MockMvcBuilders.webAppContextSetup(this.wac).build(); } @Test public void getHome() throws Exception { this.mockMvc.perform(get(\u0026#34;/\u0026#34;)) .andDo(print()) .andExpect(status().isOk()) .andExpect(forwardedUrl(\u0026#34;/WEB-INF/views/home.jsp\u0026#34;)); } @Test public void postEmptyData() throws Exception { this.mockMvc.perform(post(\u0026#34;/\u0026#34;)) .andDo(print()) .andExpect(status().isOk()) .andExpect(model().attributeHasFieldErrors(\u0026#34;formDTO\u0026#34;, \u0026#34;messageFromUser\u0026#34;)) .andExpect(forwardedUrl(\u0026#34;/WEB-INF/views/home.jsp\u0026#34;)); } @Test public void postSomething() throws Exception { this.mockMvc.perform(post(\u0026#34;/\u0026#34;).param(\u0026#34;messageFromUser\u0026#34;, \u0026#34;hello\u0026#34;)) .andDo(print()) .andExpect(status().isMovedTemporarily()) // 302 redirect \t.andExpect(model().hasNoErrors()) .andExpect(flash().attributeExists(\u0026#34;message\u0026#34;)) .andExpect(redirectedUrl(\u0026#34;/\u0026#34;)); } } I think the code is pretty understandable as it uses a syntax which is very close to natural language. But it is quite difficult to get it right the first time. You definitely have to check out the documentation.\nTomcat and jetty To add them to your pom.xml, check out my previous article or download the github project of this article which also provides a very handsome SiteMesh template featuring twitter bootstrap and responsive design.\nConclusion Since version 3.2, Spring MVC is now very easy to configure and use. Spring mvc test, now integrated in the framework, is a really unique feature in terms of controller unit testing and is really worth a try.\nIts action based request mapping \u0026lsquo;a la rest\u0026rsquo; makes it both versatile, easy to integrate with ajax solutions and a very good replacement for the old-timer.\nIt also provides easy XML, RSS, plain text or JSON publishing and can almost be used as a replacement for a rest framework !\nOf course, be sure to check the spring-mvc-showcase, a real goldmine.\n","permalink":"https://geowarin.com/complete-example-of-a-spring-mvc-3.2-project/","summary":"Awesome spring mvc 3.2 starter with sitemesh, localization, UTF-8, boostrap and spring test mvc","title":"Complete example of a Spring MVC 3.2 project"},{"content":"This is a quick memento on how to set up tomcat7-maven-plugin and jetty-maven plugin (current version is 8).\nIt is interesting to note both of these servers are servlet 3.0 compatible.\nSetting up Jetty :\nNow your can run mvn jetty:run\nTo set up tomcat-7 plugin, you will have to add either the plugin to the pluginManagement section of your pom (more portable) - see below - or adding the pluginGroup to your settings.xml.\nNow your can run mvn tomcat7:run\npluginGroup :\n\u0026lt;pluginGroups\u0026gt; .... \u0026lt;pluginGroup\u0026gt;org.apache.tomcat.maven\u0026lt;/pluginGroup\u0026gt; .... \u0026lt;/pluginGroups\u0026gt; ","permalink":"https://geowarin.com/basic-configuration-for-jetty-and-tomcat-maven-plugins/","summary":"Cheatsheet for jetty and tomcat maven plugins","title":"Basic configuration for Jetty and tomcat maven plugins"},{"content":"I don\u0026rsquo;t know if it\u0026rsquo;s a well known feature but there is a bunch of methods on hibernate Configuration class which will produce DDL for your database according to the dialect in use.\nIn this blog post we will see how to use those methods to generate a DDL script to set up our database and get rid of those annoying DBAs (just kidding, always review these scripts, they\u0026rsquo;re not production ready).\nThe source code of this article is available on github : https://github.com/geowarin/hibernate-examples/tree/master/generate-ddl-hibernate\nSo the goal of this article is, given a dialect and a package to scan containing our entities, generate a DDL.\nWith hibernate in the classpath, you can create a new configuration like this :\nhibernateConfiguration = new Configuration(); hibernateConfiguration.addAnnotatedClass(myEntity.class); hibernateConfiguration.addAnnotatedClass(mySecondEntity.class); hibernateConfiguration.setProperty(AvailableSettings.DIALECT, dialect); That\u0026rsquo;s cool but pretty boring. Cooler is to use the reflections project to provide some package scanning.\nprivate Configuration createHibernateConfig() { hibernateConfiguration = new Configuration(); final Reflections reflections = new Reflections(entityPackage); for (Class\u0026lt;?\u0026gt; cl : reflections.getTypesAnnotatedWith(MappedSuperclass.class)) { hibernateConfiguration.addAnnotatedClass(cl); } for (Class\u0026lt;?\u0026gt; cl : reflections.getTypesAnnotatedWith(Entity.class)) { hibernateConfiguration.addAnnotatedClass(cl); } hibernateConfiguration.setProperty(AvailableSettings.DIALECT, dialect); return hibernateConfiguration; } Then you can get your creation scripts lines like that :\nString[] createSQL = hibernateConfiguration.generateSchemaCreationScript(hibDialect); String[] dropSQL = hibernateConfiguration.generateDropSchemaScript(hibDialect); For the create script, each line will contain either a database creation or a constraint.\nSo here is the final result :\n/** * This class will create an hibernate {@link Configuration} with the given dialect and will scan provided * package for {@link MappedSuperclass} and {@link Entity}. * You can then use the export methods to generate your schema DDL. * * @author Geoffroy Warin (https://github.com/geowarin) * */ public class HibernateExporter { private static Logger log = LoggerFactory.getLogger(HibernateExporter.class); private String dialect; private String entityPackage; private boolean generateCreateQueries = true; private boolean generateDropQueries = false; private Configuration hibernateConfiguration; public HibernateExporter(String dialect, String entityPackage) { this.dialect = dialect; this.entityPackage = entityPackage; hibernateConfiguration = createHibernateConfig(); } public void export(OutputStream out, boolean generateCreateQueries, boolean generateDropQueries) { Dialect hibDialect = Dialect.getDialect(hibernateConfiguration.getProperties()); try (PrintWriter writer = new PrintWriter(out)) { if (generateCreateQueries) { String[] createSQL = hibernateConfiguration.generateSchemaCreationScript(hibDialect); write(writer, createSQL, FormatStyle.DDL.getFormatter()); } if (generateDropQueries) { String[] dropSQL = hibernateConfiguration.generateDropSchemaScript(hibDialect); write(writer, dropSQL, FormatStyle.DDL.getFormatter()); } } } public void export(File exportFile) throws FileNotFoundException { export(new FileOutputStream(exportFile), generateCreateQueries, generateDropQueries); } public void exportToConsole() { export(System.out, generateCreateQueries, generateDropQueries); } private void write(PrintWriter writer, String[] lines, Formatter formatter) { for (String string : lines) writer.println(formatter.format(string) + \u0026#34;;\u0026#34;); } private Configuration createHibernateConfig() { hibernateConfiguration = new Configuration(); final Reflections reflections = new Reflections(entityPackage); for (Class\u0026lt;?\u0026gt; cl : reflections.getTypesAnnotatedWith(MappedSuperclass.class)) { hibernateConfiguration.addAnnotatedClass(cl); log.info(\u0026#34;Mapped = \u0026#34; + cl.getName()); } for (Class\u0026lt;?\u0026gt; cl : reflections.getTypesAnnotatedWith(Entity.class)) { hibernateConfiguration.addAnnotatedClass(cl); log.info(\u0026#34;Mapped = \u0026#34; + cl.getName()); } hibernateConfiguration.setProperty(AvailableSettings.DIALECT, dialect); return hibernateConfiguration; } public boolean isGenerateDropQueries() { return generateDropQueries; } public void setGenerateDropQueries(boolean generateDropQueries) { this.generateDropQueries = generateDropQueries; } public Configuration getHibernateConfiguration() { return hibernateConfiguration; } public void setHibernateConfiguration(Configuration hibernateConfiguration) { this.hibernateConfiguration = hibernateConfiguration; } } And its usage :\npublic static void main(String[] args) { //\tHibernateExporter exporter = new HibernateExporter(\u0026#34;org.hibernate.dialect.HSQLDialect\u0026#34;, \u0026#34;com.geowarin.model\u0026#34;); \tHibernateExporter exporter = new HibernateExporter(\u0026#34;org.hibernate.dialect.MySQL5Dialect\u0026#34;, \u0026#34;com.geowarin.model\u0026#34;); exporter.exportToConsole(); } This will produce this kind of output :\ncreate table groups ( id bigint not null auto_increment, createdOn datetime, modifiedOn datetime, version bigint not null, name varchar(50) not null, user_id bigint, primary key (id) ); create table users ( id bigint not null auto_increment, createdOn datetime, modifiedOn datetime, version bigint not null, email varchar(255) not null, password varchar(80) not null, user_name varchar(50) not null unique, primary key (id) ); alter table groups add index FKB63DD9D4CA46C100 (user_id), add constraint FKB63DD9D4CA46C100 foreign key (user_id) references users (id); As I said, it is not suitable for your production environment but it provides some starter DDL if you are the code first kind (and don\u0026rsquo;t want to use hibernate-tools).\n","permalink":"https://geowarin.com/generate-ddl-with-hibernate/","summary":"How to generate SQL schemas with hibernate built-in classes","title":"Generate DDL with hibernate"},{"content":"This blog post follows my previous articles on using hibernate as a standalone JPA provider and how to use spring in a Java SE environment.\nIn this post, I will show you how to use Spring Data JPA, a great project which improves your productivity by generating all CRUD operations for you. Then we will use springtestdbunit to run some very clean tests on our database with spring and dbUnit.\nThe code source of this example is available on github : https://github.com/geowarin/hibernate-examples/tree/master/standalone-data-jpa\nWhat is Spring Data JPA? Are you tired of always implementing the findOne(long id), findAll(), save()\u0026hellip; methods on your repositories? Having to come up with clever tricks to generate a generic DAO?\nThen give Spring Data JPA a try ! This project lets you implement a very simple interface for your repositories and takes care of all the rest, allowing you to focus on your real queries.\nThe configuration The configuration we will set up here is pretty similar to the one we used in the spring standalone article. We will just add a bunch of classes to make use of spring data jpa :\n@Configuration @EnableJpaRepositories(\u0026#34;com.geowarin.standalonedatajpa.repository\u0026#34;) @EnableTransactionManagement public class StandaloneDataJpaConfig { @Bean public DataSource dataSource() { return new EmbeddedDatabaseBuilder().setType(EmbeddedDatabaseType.HSQL) .addScript(\u0026#34;classpath:sql/schema.sql\u0026#34;) .addScript(\u0026#34;classpath:sql/import-users.sql\u0026#34;) .build(); } @Bean public PlatformTransactionManager transactionManager() { JpaTransactionManager txManager = new JpaTransactionManager(); txManager.setEntityManagerFactory(entityManagerFactory()); return txManager; } @Bean public HibernateExceptionTranslator hibernateExceptionTranslator() { return new HibernateExceptionTranslator(); } @Bean public EntityManagerFactory entityManagerFactory() { // will set the provider to \u0026#39;org.hibernate.ejb.HibernatePersistence\u0026#39; \tHibernateJpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter(); // will set hibernate.show_sql to \u0026#39;true\u0026#39; \tvendorAdapter.setShowSql(true); // if set to true, will set hibernate.hbm2ddl.auto to \u0026#39;update\u0026#39; \tvendorAdapter.setGenerateDdl(false); LocalContainerEntityManagerFactoryBean factory = new LocalContainerEntityManagerFactoryBean(); factory.setJpaVendorAdapter(vendorAdapter); factory.setPackagesToScan(\u0026#34;com.geowarin.standalonedatajpa.model\u0026#34;); factory.setDataSource(dataSource()); // This will trigger the creation of the entity manager factory \tfactory.afterPropertiesSet(); return factory.getObject(); } @Bean public MainBean mainBean() { return new MainBean(); } } With this configuration, you won\u0026rsquo;t even need a persistence.xml file ! Compared to our previous example however, one cannot make use of hibernate.hbm2ddl.import_files property to import SQL scripts with hibernate because hibernate.hbm2ddl.auto must be set to either create or create-drop.\nThat\u0026rsquo;s ok we will generate some very tiny scripts to set up the schema and the data in our database (and make our DBA happy, a thing that is never to be disregarded :))\n Hint : If this is a real blocker for you, you could setGenrateDdl to false and maintain a simple hibernate.properties file like this one\nhibernate.hbm2ddl.auto=create hibernate.hbm2ddl.import_files=sql/import-users.sql hibernate.format_sql=true That\u0026rsquo;s because hibernate will always look for a hibernate.properties file in the classpath to override you persistence properties.\n Note the use of @EnableJpaRepositories that will tell spring data in which packages our repositories can be found.\n@EnableTransactionManagement is a replacement of the tag \u0026lt;tx:annotation-driven /\u0026gt;.\nWe can set up the¬†LocalContainerEntityManagerFactoryBean to use a package to scan our entities for us, no need to list them all.\nThe rest is pretty straight forward I believe.\nNote that by default, spring data JPA will try to locate your persistence.xml so the two approaches are completely compatible, for this example however we will go for a full xml-less configuration.\nOne entity, one interface and we are ready We have one very simple entity :\n@Table(name = \u0026#34;users\u0026#34;) @Entity public class User implements Serializable { private static final long serialVersionUID = 1L; @Id @GeneratedValue(strategy=GenerationType.AUTO) private long id; @Column(name = \u0026#34;name\u0026#34;, nullable = false, unique=true, length=50) private String name; // getters and setters omitted } Now let\u0026rsquo;s use spring data to generate a repository for us :\npublic interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { } Tadaa! Is that it? Yes, you can now use your repository in our MainBean :\npublic class MainBean { @Autowired private UserRepository userRepository; private static Logger log = LoggerFactory.getLogger(MainBean.class); public void start() { // Spring Data JPA CRUD operations are transactionnal by default ! \t// http://static.springsource.org/spring-data/data-jpa/docs/current/reference/html/#transactions \tUser newUser = new User(); newUser.setName(\u0026#34;inserted\u0026#34;); userRepository.save(newUser); List all = userRepository.findAll(); log.info(\u0026#34;users=\u0026#34; + all); } } Pretty sweet. But that\u0026rsquo;s not all. You now have three different ways of writing new queries with Spring data :\n  Use named queries\n  Use the @Query annotation to write your own JPQL queries\n  Use the awesome query creation by method name\n  Let\u0026rsquo;s review the last two options (I don\u0026rsquo;t really like named queries but have a look a the documentation if you want)\npublic interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { // Demonstrate query creation by method name \t// http://static.springsource.org/spring-data/data-jpa/docs/current/reference/html/#jpa.query-methods.query-creation \tUser findByName(String name); // Demonstrate the use of a simple JPQL query \t@Query(\u0026#34;from User u where upper(u.name) = upper(:name)\u0026#34;) User findByNameIgnoreCase(@Param(\u0026#34;name\u0026#34;) String name); } Ok let\u0026rsquo;s test it Have a look at the unit test for our repository :\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = { StandaloneDataJpaConfig.class }) @TestExecutionListeners({ DependencyInjectionTestExecutionListener.class, DbUnitTestExecutionListener.class }) public class UserRepositoryTest { @Autowired private UserRepository userRepository; @Test @DatabaseSetup(\u0026#34;userAdminData.xml\u0026#34;) public void testFindAdmin() { User admin = userRepository.findOne(1L); Assert.assertNotNull(admin); Assert.assertEquals(\u0026#34;admin\u0026#34;, admin.getName()); } @Test @DatabaseSetup(\u0026#34;userAdminData.xml\u0026#34;) public void testFindByName() { User admin = userRepository.findByName(\u0026#34;admin\u0026#34;); Assert.assertNotNull(admin); Assert.assertEquals(\u0026#34;admin\u0026#34;, admin.getName()); } @Test @DatabaseSetup(\u0026#34;userAdminData.xml\u0026#34;) public void testFindByNameIgnoreCase() { User admin = userRepository.findByNameIgnoreCase(\u0026#34;AdMIn\u0026#34;); Assert.assertNotNull(admin); Assert.assertEquals(\u0026#34;admin\u0026#34;, admin.getName()); } @Test @DatabaseSetup(\u0026#34;userAdminData.xml\u0026#34;) @ExpectedDatabase(\u0026#34;afterInsert.xml\u0026#34;) public void testInsertUser() { User newUser = new User(); newUser.setName(\u0026#34;inserted\u0026#34;); userRepository.save(newUser); } } We use springtestdbunit to be able to use annotations to set up and verify the database state after each test. Here are our two datasets.\nuserAdminData.xml :\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;dataset\u0026gt; \u0026lt;users id=\u0026#34;1\u0026#34; name=\u0026#34;admin\u0026#34; /\u0026gt; \u0026lt;/dataset\u0026gt; afterInsert.xml :\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;dataset\u0026gt; \u0026lt;users id=\u0026#34;1\u0026#34; name=\u0026#34;admin\u0026#34; /\u0026gt; \u0026lt;users id=\u0026#34;2\u0026#34; name=\u0026#34;inserted\u0026#34; /\u0026gt; \u0026lt;/dataset\u0026gt; Using the @ExpectedDatabase annotation is really awesome, dbUnit will generate very informative messages if your test failed.\nConclusion Spring Data JPA is really a good piece of software. We did not go into too much details but know that it will handle pagination, type-safe queries, is compatible with query-dsl and much more. It is suitable for a Java SE environment which can come in handy if we want to run some quick tests on our database.\nFinally, springtestdbunit is a very nice-to-have feature which will make your repository unit tests a real breeze to write.\nLastly, I will point out that making use of the transational behaviour of spring data CRUD operation is not really a good practice but we did it all the same for the sake of simplicity. A better approach would be to use @Service spring beans to encapsulate one or several operations.\n","permalink":"https://geowarin.com/using-spring-data-jpa-in-a-java-se-environment-and-run-tests-with-dbunit/","summary":"Demonstration of how to set up spring data JPA in java SE and test with dbUnit","title":"Using Spring Data JPA in a Java SE environment and run tests with dbUnit"},{"content":"In this post I will show you how to use spring 3.0 Java based configuration in a Java SE environment, taking advantage of spring autowiring to implement the chain of responsibility design pattern with components and services in a simple project. We will also use spring\u0026rsquo;s PropertySourcesPlaceholderConfigurer to inject custom properties into our beans with the @Value annotation and test our application with spring-test.\nSome fun in perspective ! Tag along.\nThe source code of this application is available on my github : https://github.com/geowarin/spring-examples/tree/master/spring-standalone-chain\nJava configuration Starting from spring 3.0, it is possible to get rid of any XML configuration by providing a pure Java configuration. This is done by annotating your configuration classes with the @Configuration annotation and annotating your beans with @Bean:\n@Configuration @ComponentScan(basePackages = {\u0026#34;com.geowarin.spring.service\u0026#34;, \u0026#34;com.geowarin.spring.component\u0026#34;}) @PropertySource(value = \u0026#34;classpath:chain.properties\u0026#34;) public class SpringStandalonChainConfig { @Bean public static PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer() { PropertySourcesPlaceholderConfigurer pspc = new PropertySourcesPlaceholderConfigurer(); pspc.setPlaceholderPrefix(\u0026#34;#{\u0026#34;); return pspc; } @Bean public MainBean mainBean() { return new MainBean(); } } Additional annotation for java configuration include @ComponentScan to specify packages in which your @Component, @Service, etc. beans are included and @PropertySource to include property files in your configuration.\nTwo thing here :\n I declare a MainBean which will act as an entry point for our application. This bean will benefit of spring autowiring I declare a custom PropertySourcesPlaceholderConfigurer to enable the injection of properties annotated with @Value. I am customizing the prefix for usage of spring Expression Language to be able to use #{} expressions instead of default ${}  Injection with @Value property is a very interesting alternative to the use of spring\u0026rsquo;s Environment as it provides natural type inference.\nFor some people, the use of java configuration can be confusing because one cannot see at first glance where the config is located. What I usually do is keeping my configuration in a separate source folder. This can be achieved with maven and its build-helper plugin :\nThis approach is compatible with eclipse if you have m2e installed. In that case when you import a project using this plugin, eclipse will prompt you for the install of the build-helper connector.\nOur application entry point : the MainBean Here is the code of our MainBean :\npublic class MainBean { @Autowired @Qualifier(\u0026#34;doChain\u0026#34;) private ChainService service; @Value(\u0026#34;#{chain.compatibleWithFirst}\u0026#34;) boolean compatibleWithFirst; @Value(\u0026#34;#{chain.compatibleWithSecond}\u0026#34;) boolean compatibleWithSecond; private static Logger log = LoggerFactory.getLogger(MainBean.class); public void start() { log.info(\u0026#34;property compatibleWithFirst=\u0026#34; + compatibleWithFirst); log.info(\u0026#34;property compatibleWithSecond=\u0026#34; + compatibleWithSecond); ChainContext chainContext = new ChainContext(compatibleWithFirst, compatibleWithSecond); service.executeChain(chainContext); } } We are injecting our service into the bean with a custom qualifier which will enable us to provide several implementations of our service if we need it.\nNote that the @Value annotation, our properties will automatically be casted to booleans, which is pretty cool. Here is our chain.properties file :\nchain.compatibleWithFirst=false chain.compatibleWithSecond=true The Chain : Two components and a service The ChainContext class is a simple pojo we pass to our service to be handled by the chain of responsibility and enable us to test if our service successfully handled our case :\npublic class ChainContext { private final boolean compatibleWithFirstElement; private final boolean compatibleWithSecondElement; private boolean handledByFirst; private boolean handledBySecond; public ChainContext(boolean compatibleWithFirstElement, boolean compatibleWithSecondElement) { this.compatibleWithFirstElement = compatibleWithFirstElement; this.compatibleWithSecondElement = compatibleWithSecondElement; } // Getters and setters ommited } ChainElement is a simple interface which will be implemented by two components : FirstChainElement and SecondChainElement.\npublic interface ChainElement { public boolean doChain(ChainContext context); } Here is the first element, the second one is essentially the same thing :\n@Component @Order(1) public class FirstChainElement implements ChainElement { private static Logger log = LoggerFactory.getLogger(FirstChainElement.class); @Override public boolean doChain(ChainContext context) { if (context.isCompatibleWithFirstElement()) { log.info(\u0026#34;Handled by first\u0026#34;); context.setHandledByFirst(true); return true; } return false; } } The thing to note here is the use of the spring annotation @Order which will enable us to sort our list using spring\u0026rsquo;s AnnotationAwareOrderComparator. Neat :)\nAnd now the service :\n@Service @Qualifier(\u0026#34;doChain\u0026#34;) public class DoChainService implements ChainService { @Autowired private List\u0026lt;ChainElement\u0026gt; chain; @PostConstruct public void init() { Collections.sort(chain, AnnotationAwareOrderComparator.INSTANCE); } @Override public void executeChain(ChainContext context) { for (ChainElement chainElement : chain) { if (chainElement.doChain(context)) break; } } } Note that we use the same qualifier as our MainBean here. The main trick in this article is the usage of @Autowired to inject all the components implementing the ChainElement interface into a List.\nThe application main and tests That\u0026rsquo;s it ! Now you can run your project with this main class :\npublic class SpringStandaloneChainApp { private static final String CONFIG_PACKAGE = \u0026#34;com.geowarin.spring.config\u0026#34;; public static void main(String[] args) { try (AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext()) { ctx.scan(CONFIG_PACKAGE); ctx.refresh(); MainBean bean = ctx.getBean(MainBean.class); bean.start(); } } } And unit test the service with spring-test like that :\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = { SpringStandalonChainConfig.class }) public class ChainServiceTest extends TestCase { @Autowired private ChainService chainService; @Test public void testHandledByFirst() { ChainContext chainContext = new ChainContext(true, false); chainService.executeChain(chainContext); Assert.assertTrue(chainContext.isHandledByFirst()); Assert.assertFalse(chainContext.isHandledBySecond()); } @Test public void testHandledBySecond() { ChainContext chainContext = new ChainContext(false, true); chainService.executeChain(chainContext); Assert.assertFalse(chainContext.isHandledByFirst()); Assert.assertTrue(chainContext.isHandledBySecond()); } } Conclusion Spring is perfectly suitable for a Java SE Environment. Its new java configuration is very handy and powerful as long as you keep things tidy and make sure the configuration is not spread across the whole project. Finally, let\u0026rsquo;s note it is possible to inject all components or services implementing a given interface into a list with the `@Autowired annotation.\n","permalink":"https://geowarin.com/using-spring-in-a-java-se-environment-and-implementing-the-chain-of-responsibility-pattern/","summary":"Demonstration of how to set up spring in java SE and inject several implementations of the same interface","title":"Using spring in a Java SE environment and implementing the chain of responsibility pattern"},{"content":"Sometimes it can be useful to use JPA in a minimal environment to test your model or just if you\u0026rsquo;re curious of how things work in your DAO layer behind the scenes. In this blog post, we will create a simple project then see how you can use dbUnit to run some unit tests on your database.\nThe source code of this blog post is available on my github : https://github.com/geowarin/hibernate-examples/tree/master/hibernate-jpa-standalone-dbunit\nFirst thing is to create a persistence.xml file in your resources. Default location is under META-INF/persistence.xml. Here is a snippet using an embedded hsqldb.\njavax.persistence.jdbc.driver, url, user and password are standard JPA properties. The driver tells jdbc how to connect to your databse. Hsqldb must be in your classpath to find the corresponding driver. You could also init hsqdb to write to a file or use a different protocol, see the documentation for more information.\nThe properties prefixed with hibernate are vendor-specific properties :\n hibernate.dialect will tell hibernate how to issue SQL queries for your database hibernate.hbm2ddl.auto can be set to validate, update, create, create-drop. Since we will start a new database every time we launch our application, we will just create the tables on each run hbm2ddl.import_files is a comma-separated list of paths to some custom SQL scripts that hibernate will¬†execute after the database is created, we will explain this a little bit further hibernate.show_sql and hibernate.format_sql will display the SQL queries generated by hibernate. You can tell hibernate to format them if you want a more human readable output  More information is available in the hibernate documentation.\nThen, we will create a simple entity :\n@Table(name = \u0026#34;users\u0026#34;) @Entity public class User implements Serializable { @Id @GeneratedValue(strategy=GenerationType.AUTO) private long id; @Column(name = \u0026#34;name\u0026#34;, nullable = false, unique=true, length=50) private String name; // Getters and setters omitted  } Notice the @Table annotation that we use to specify the name of the table we are going to store our users in. This is useful information for our SQL scripts for example. For table naming, you should consider having simple conventions. Here, I just use lower case name of the entity and add an s to my table.\nSame thing can be said about the @Column annotation.\nNow let\u0026rsquo;s review our init script, import-users.sql :\nINSERT INTO users(id, name) VALUES(1, \u0026#39;admin\u0026#39;); Pretty straight forward. You just have to figure out the correct syntax from your annotations.\nLast is the instantiation and use of the entity manager :\npublic class App { private static Logger log = LoggerFactory.getLogger(App.class); public static void main(String[] args) { EntityManagerFactory entityManagerFactory = Persistence.createEntityManagerFactory(\u0026#34;persistence\u0026#34;); EntityManager entityManager = entityManagerFactory.createEntityManager(); User found = entityManager.find(User.class, 1L); log.info(\u0026#34;found=\u0026#34; + found); } } Here you go! Since JPA 2 you can create the entity manager factory using the Persistence class. The string passed as parameter is the name of your persistence unit declared in your persistence.xml file. Spring and others will bootstrap the entity manager factory a little bit differently but the concept is essentially the same.\nUsing dbUnit DbUnit is a database testing framework which allows you to load data and verify the correctness of your DAO layer using datasets.\nDatasets are simple xml files which represent a database state. Here is an sample dataset :\nWe can use dbUnit in our simple project, we just have to deal with a little bit of boilerplate code. Here is the abstract class that my test will extend.\n/** * Abstract unit test case class. * This will load the test-data.xml dataset before each test case and will clean the database before each test * * @author Geoffroy Warin (https://github.com/geowarin) * */ public abstract class AbstractDbUnitJpaTest { private static EntityManagerFactory entityManagerFactory; private static IDatabaseConnection connection; private static IDataSet dataset; protected static EntityManager entityManager; /** * Will load test-dataset.xml before each test case * @throws DatabaseUnitException * @throws HibernateException */ @BeforeClass public static void initEntityManager() throws HibernateException, DatabaseUnitException { entityManagerFactory = Persistence.createEntityManagerFactory(\u0026#34;persistence-test\u0026#34;); entityManager = entityManagerFactory.createEntityManager(); connection = new DatabaseConnection(((SessionImpl) (entityManager.getDelegate())).connection()); connection.getConfig().setProperty(DatabaseConfig.PROPERTY_DATATYPE_FACTORY, new HsqldbDataTypeFactory()); FlatXmlDataSetBuilder flatXmlDataSetBuilder = new FlatXmlDataSetBuilder(); flatXmlDataSetBuilder.setColumnSensing(true); InputStream dataSet = Thread.currentThread().getContextClassLoader().getResourceAsStream(\u0026#34;test-data.xml\u0026#34;); dataset = flatXmlDataSetBuilder.build(dataSet); } @AfterClass public static void closeEntityManager() { entityManager.close(); entityManagerFactory.close(); } /** * Will clean the dataBase before each test * * @throws SQLException * @throws DatabaseUnitException */ @Before public void cleanDB() throws DatabaseUnitException, SQLException { DatabaseOperation.CLEAN_INSERT.execute(connection, dataset); } } And an example of usage :\npublic class AppTest extends AbstractDbUnitJpaTest { @Test public void testFind() { User user = entityManager.find(User.class, 1L); Assert.assertNotNull(user); Assert.assertEquals(\u0026#34;userTest\u0026#34;, user.getName()); } @Test public void testInsert() { User newUser = new User(); newUser.setName(\u0026#34;insert\u0026#34;); entityManager.getTransaction().begin(); entityManager.persist(newUser); long id = newUser.getId(); entityManager.getTransaction().commit(); User user = entityManager.find(User.class, id); Assert.assertNotNull(user); Assert.assertEquals(\u0026#34;insert\u0026#34;, user.getName()); } @Test public void testFindAll() { List\u0026lt;User\u0026gt; allUsers = entityManager.createQuery(\u0026#34;from User\u0026#34;).getResultList(); Assert.assertEquals(2, allUsers.size()); } } Notice we have to deal with the transactions ourselves which can be pretty tiresome. Recommended way to do this is to create a service layer which will be responsible for opening and committing transactions.\nThe findAll test uses a standard JPQL query to find all users in the database. The select clause is facultative. The \u0026ldquo;User\u0026rdquo; keyword is the name of our entity.\nConclusion Since JPA 2, we can use JPA in a simple SE environment. Downside is we have to handle transactions and write some boilerplate code to use JPQL queries and a framework like dbUnit.\nIssues that can be addressed beautifully by project like Spring data JPA, check out my other article.\n","permalink":"https://geowarin.com/using-hibernate-as-a-jpa-provider-in-a-java-se-environment-and-run-tests-with-dbunit/","summary":"Demonstration of how to set up hibernate in a java SE application and test with dbUnit","title":"Using Hibernate as a JPA provider in a Java SE environment and run tests with dbUnit"},{"content":"This is a compilation of resources I use to configure eclipse.\nThe Jvm options Always a big headache, there is a good resource on stack-overflow.\nFor JVM noobs, I recommend the following reading.\nIf you are a JVM tuning maniac, you might also like this post and to read the man.\nIf you\u0026rsquo;re interested, I am maintaining a gist with my up-to-date eclipse flags :¬†https://gist.github.com/4562291\nMust-have plugins If you don\u0026rsquo;t know them already, you should check out these plugins :\n EasyShell, which allows to open windows, terminal and copy path on any eclipse resource (and configure shortcuts for each of these actions - pretty awesome) m2e¬†and m2e-wtp¬†are¬†must-haves if you use maven to build JEE apps. I often install the build-helper connector too. GrepConsole, which lets you use simple regexp to color your console, fancy !  Copy workspace settings If you want to have multiple workspaces sharing the same preferences check out this article.\nFile encoding in UTF-8 please First thing is to go to General \u0026gt; Workspace and select UTF-8 in \u0026ldquo;Text File Encoding\u0026rdquo; Second is General \u0026gt; Content Types \u0026gt; Text. Then you can either type \u0026ldquo;UTF-8\u0026rdquo; for everything or at least for \u0026ldquo;Java Properties file\u0026rdquo;\nBonus Et pour nos amis francophones, une superbe pr√©sentation sur la customisation d\u0026rsquo;eclipse !\n","permalink":"https://geowarin.com/tuning-eclipse/","summary":"Tips and tricks, useful plugins and tuning for the eclipse IDE","title":"Tuning eclipse"}]